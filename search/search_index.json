{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>\u26a0\ufe0f End-of-life notice. Effective immediately, HP has ended development, maintenance, and support of this project. The repo will be available in read-only mode until September 1st, 2024, when it will be deleted. You are welcome to create a copy and keep the project going.</p>"},{"location":"#ml-git","title":"ML-Git","text":"<p>ML-Git is a tool which provides a Distributed Version Control system to enable efficient dataset management. Like its name emphasizes, it is inspired in git concepts and workflows, ML-Git enables the following operations:</p> <ul> <li>Manage a repository of different datasets, labels and models.</li> <li>Distribute these ML artifacts between members of a team or across organizations.</li> <li>Apply the right data governance and security models to their artifacts.</li> </ul> <p>If you are seeking to learn more about ML-Git, access ML-Git Page.</p>"},{"location":"#how-to-install","title":"How to install","text":"<p>Prerequisites:</p> <ul> <li>Git</li> <li>Python 3.6.1+</li> <li>Pip 20.1.1+</li> </ul> <p>From repository: <pre><code>pip install ml-git\n</code></pre></p> <p>From source code:</p> <p>Download ML-Git from repository and execute commands below:</p> <pre><code>cd ml-git/\npip install .\n</code></pre>"},{"location":"#how-to-uninstall","title":"How to uninstall","text":"<pre><code>pip uninstall ml-git\n</code></pre>"},{"location":"#how-to-configure","title":"How to configure","text":"<p>1 - As ML-Git leverages git to manage ML entities metadata, it is necessary to configure user name and email address:</p> <pre><code>git config --global user.name \"Your User\"\ngit config --global user.email \"your_email@example.com\"\n</code></pre> <p>2 - OPTIONAL CONFIGURATIONS  - 2.1 - Some ML-Git commands have a wizard to help you during their execution. Those commands have the <code>--wizard</code> option available to enable this wizard. However, you can configure the wizard to be enabled by default on all supported commands by running the following command:</p> <pre><code>```\nml-git repository config --set-wizard=enabled\n```\n</code></pre> <ul> <li>2.2 - You can also allow commands and options to be autocompleted with a  <code>[Tab]</code> key press. For that, take a look at the following link ML-Git Shell Completion Support.</li> </ul> <p>3 - Storage:</p> <p>ML-Git needs a configured storage to store data from managed artifacts. Please take a look at the ML-Git architecture and internals documentation to better understand how ML-Git works internally with data.</p> <ul> <li>To configure the storage see documentation about supported storages and how to configure each one.</li> </ul> <p>4 - ML-Git project:</p> <ul> <li>An ML-Git project is an initialized directory that will contain a configuration file to be used by ML-Git in managing entities.  To configure it you can use the basic steps to configure the project described in first project documentation.</li> </ul>"},{"location":"#usage","title":"Usage","text":"<pre><code>ml-git --help\nUsage: ml-git [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --version  Show the version and exit.\n\nCommands:\n  clone       Clone an ml-git repository ML_GIT_REPOSITORY_URL\n  datasets    Management of datasets within this ml-git repository.\n  labels      Management of labels sets within this ml-git repository.\n  models      Management of models within this ml-git repository.\n  repository  Management of this ml-git repository.\n</code></pre>"},{"location":"#basic-commands","title":"Basic commands","text":"<code>ml-git clone &lt;repository-url&gt;</code> <pre><code>ml-git clone https://github.com/user/ml_git_configuration_file_example.git\n</code></pre> <p>If you prefer to create a new directory to clone into:</p> <pre><code>ml-git clone https://github.com/user/ml_git_configuration_file_example.git my-project-dir\n</code></pre> <p>If you prefer keep git tracking files in the project:</p> <pre><code>ml-git clone https://github.com/user/ml_git_configuration_file_example.git --track\n</code></pre> <code>ml-git &lt;ml-entity&gt; create</code> <p>This command will help you to start a new project, it creates your project artifact metadata:</p> <pre><code>ml-git datasets create --categories=\"computer-vision, images\" --bucket-name=your_bucket --import=../import-path --mutability=strict dataset-ex \n</code></pre> <p>Demonstration video:</p> <p></p> <code>ml-git &lt;ml-entity&gt; status</code> <p>Show changes in project workspace:</p> <pre><code>ml-git datasets status dataset-ex\n</code></pre> <p>Demonstration video:</p> <p></p> <code>ml-git &lt;ml-entity&gt; add</code> <p>Add new files to index:</p> <pre><code>ml-git datasets add dataset-ex\n</code></pre> <p>To increment version:</p> <pre><code>ml-git datasets add dataset-ex --bumpversion\n</code></pre> <p>Add an specific file:</p> <pre><code>ml-git datasets add dataset-ex data/file_name.ex\n</code></pre> <p>Demonstration video:</p> <p></p> <code>ml-git &lt;ml-entity&gt; commit</code> <p>Consolidate added files in the index to repository:</p> <pre><code>ml-git datasets commit dataset-ex\n</code></pre> <p>Demonstration video:</p> <p></p> <code>ml-git &lt;ml-entity&gt; push</code> <p>Upload metadata to remote repository and send chunks to storage:</p> <pre><code>ml-git datasets push dataset-ex\n</code></pre> <p>Demonstration video:</p> <p></p> <code>ml-git &lt;ml-entity&gt; checkout</code> <p>Change workspace and metadata to versioned ml-entity tag:</p> <pre><code>ml-git datasets checkout computer-vision__images__dataset-ex__1\n</code></pre> <p>Demonstration video:</p> <p></p> <p>More about commands in documentation</p>"},{"location":"#how-to-contribute","title":"How to contribute","text":"<p>Your contributions are always welcome!</p> <ol> <li>Fork the repository into your own GitHub</li> <li>Clone the repository to your local machine</li> <li>Create a new branch for your changes using the following pattern <code>(feature | bugfix | hotfix)/branch_name</code>. Example: <code>feature/sftp_storage_implementation</code></li> <li>Make changes and test</li> <li>Push the changes to your repository</li> <li>Create a Pull Request from your forked repository to the ML-Git repository with comprehensive description of changes</li> </ol> <p>Another way to contribute with the community is creating an issue to track your ideas, doubts, enhancements, tasks, or bugs found.  If an issue with the same topic already exists, discuss on the issue.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>ML-Git API documentation - Find the commands that are available in our api, usage examples and more.</li> <li>Working with tabular data - Find suggestions on how to use ml-git with tabular data.</li> <li>ml-git data specialization plugins - Dynamically link third-party packages to add specialized behaviors for the data type.</li> </ul>"},{"location":"advanced_scenarios/","title":"Additional use cases","text":"<p>As you get familiar with ML-Git, you might feel the necessity of use advanced ML-Git features to solve your problems. Thus, this section aims to provide advanced scenarios and additional use cases.</p>"},{"location":"advanced_scenarios/#keeping-track-of-a-dataset","title":"Keeping track of a dataset","text":"<p>Often, users can share the same dataset. As the dataset improve, you will need to keep track of the changes. It is very simple to keep check what is new in a shared repository. You just need to navigate to the root of your project. Then, you can execute the command <code>update</code>, it will update the metadata repository, allowing visibility of what has been changed since the last update. For example, new ML entity and/or new versions.</p> <pre><code>ml-git repository update\n</code></pre> <p>In case something new exists in this repository, you will see a output like: <pre><code>INFO - Metadata Manager: Pull [/home/Documents/my-mlgit-project-config/.ml-git/datasets/metadata]\nINFO - Metadata Manager: Pull [/home/Documents/my-mlgit-project-config/.ml-git/labels/metadata]\n</code></pre></p> <p>Then, you can checkout the new available data.</p>"},{"location":"advanced_scenarios/#linking-labels-to-a-dataset","title":"Linking labels to a dataset","text":"<p>ML-Git provides support for users link an entitity to another. In this example, we show how to link labels to a dataset. To accomplish this use case, you will need to have a dataset versioned by ML-Git in your repository.</p> <p>First, you need to configure your remote repository. Then, you can configure your storage. It is a similarly process as you did to configure your repository and storage for your dataset.</p> <pre><code>ml-git repository remote labels add git@github.com:example/your-mlgit-labels.git\nml-git repository storage add mlgit-labels --endpoint-url=&lt;minio-endpoint-url&gt;\nml-git labels init\n</code></pre> <p>Even, we are using a different bucket to store the labels data. It would be possible to store both datasets and labels into the same bucket.</p> <p>If you look at your config file using the command: <pre><code>ml-git repository config\n</code></pre></p> <p>You should see something similar to the following config file:</p> <pre><code>config:\n{'batch_size': 20,\n 'cache_path': '',\n 'datasets': {'git': 'git@github.com:example/your-mlgit-datasets.git'},\n 'index_path': '',\n 'labels': {'git': 'git@github.com:example/your-mlgit-labels.git'},\n 'metadata_path': '',\n 'mlgit_conf': 'config.yaml',\n 'mlgit_path': '.ml-git',\n 'models': {'git': ''},\n 'object_path': '',\n 'refs_path': '',\n 'storages': {'s3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'},\n                                      'endpoint-url': &lt;minio-endpoint-url&gt;,\n                                      'region': 'us-east-1'}}},\n              's3h': {'mlgit-labels': {'aws-credentials': {'profile': 'default'},\n                                      'endpoint-url': &lt;minio-endpoint-url&gt;,\n                                      'region': 'us-east-1'}}},\n 'verbose': 'info'}\n</code></pre> <p>Then, you can create your first set of labels. As an example, we will use your-labels. ML-Git expects any set of labels to be specified under the labels/ directory of your project. Also, it expects a specification file with the name of the labels.</p> <pre><code>ml-git labels create your-labels --categories=\"computer-vision, labels\" --mutability=mutable --storage-type=s3h --bucket-name=mlgit-labels --version=1\n</code></pre> <p>After create the entity, you can create the README.md describing your set of labels. Below, we show an example of caption labels for the your-labels directory and file structure:</p> <pre><code>your-labels/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 annotations\n\u2502   \u251c\u2500\u2500 captions_train2014.json\n\u2502   \u2514\u2500\u2500 captions_val2014.json\n\u2514\u2500\u2500 your-labels.spec\n</code></pre> <p>Now, you are ready to version the new set of labels. For this, do:</p> <pre><code>ml-git labels add your-labels\nml-git labels commit your-labels --dataset=your-datasets\nml-git labels push your-labels\n</code></pre> <p>The commands are very similar to dataset operations. However, you can note one particular change in the commit command. There is an option \"--dataset\" which is used to tell ML-Git that the labels should be linked to the specified dataset. With the following command, it is possible to see what datasets are associated with this labels </p> <pre><code>ml-git labels show your-labels\n</code></pre> <p>The output will looks like: <pre><code>-- labels : your-labels --\ncategories:\n- computer-vision\n- captions\ndataset:\n  sha: 607fa818da716c3313a6855eb3bbd4587e412816\n  tag: computer-vision__images__mscoco__1\nmanifest:\n  files: MANIFEST.yaml\n  storage: s3h://mlgit-datasets\nname: your-captions\nversion: 1\n</code></pre></p> <p>As you can see, there is a new section \"dataset\" that has been added by ML-Git with the sha &amp; tag fields. It can be used to checkout the exact version of the dataset for that set of labels.</p> <p>Uploading labels related to a dataset:</p> <p></p>"},{"location":"advanced_scenarios/#adding-special-credentials-aws","title":"Adding special credentials AWS","text":"<p>Depending the project you are working on, you might need to use special credentials to restrict access to your entities (e.g., datases) stored inside a S3/MinIO bucket. The easiest way to configure and use a different credentials for the AWS storage is installing the AWS command-line interface (awscli). First, install the awscli. Then, run the following command:</p> <pre><code>aws configure --profile=mlgit\n</code></pre> <p>You will need to inform the fields listed below:</p> <pre><code>AWS Access Key ID [None]: your-access-key\nAWS Secret Access Key [None]: your-secret-access-key\nDefault region name [None]: bucket-region\nDefault output format [None]: json\n</code></pre> <p>These commands will create the files ~/.aws/credentials and ~/.aws/config.</p> <p>Below, you can see a short video on how to configure the AWS profile:</p> <p></p> <p>After you have created your special credentials (e.g., mlgit profile)</p> <p>You can use this profile as parameter to add your storages. Following, you can see an exaple of how to attach the profile to the storage mlgit-datasets.</p> <pre><code>ml-git repository storage add mlgit-datasets --credentials=mlgit --endpoint-url=&lt;minio-endpoint-url&gt;\n</code></pre>"},{"location":"advanced_scenarios/#resources-inicialization-using-script","title":"Resources Inicialization using script","text":"<p>You can find the script following the step below. It remotely creates the configurations, and during the execution it will generate a config repository containing the configurations pointing to the metadata repository (GitHub) and storage (AWS S3 or Azure Blob).</p> <p>If you are using Linux, execute on the terminal:</p> <pre><code>cd ml-git\n./scripts/resources_initialization/resources_initialization.sh\n</code></pre> <p>If you are using Windows, execute on the CMD or Powershell:</p> <pre><code>cd ml-git\n.\\scripts\\resources_initialization\\resources_initialization.bat\n</code></pre> <p>At the end of executing this script, you will be able to directly execute a clone command to download your ML-Git project.</p>"},{"location":"advanced_scenarios/#checking-data-integrity","title":"Checking Data Integrity","text":"<p>If at some point you want to check the integrity of the metadata repository (e.g. computer shutdown during a process), simply type the following command:</p> <pre><code>ml-git datasets fsck\n</code></pre> <p>That command will walk through the internal ML-Git directories (index &amp; local repository) and will check the integrity of all blobs under its management. It will return the list of blobs that are corrupted.</p> <p>Checking Data Integrity:</p> <p></p>"},{"location":"aws_s3_configuration/","title":"S3 bucket configuration","text":"<p>This section explains how to configure the settings that the ML-Git uses to interact with your bucket. This requires that you have the following data:</p> <ol> <li>Profile Name</li> <li>Access Key ID</li> <li>Secret Access Key</li> <li>Region Name</li> <li>Output Format</li> </ol> <p>The Access Key ID and Secret Access Key are your credentials. The Region Name identifies the AWS Region whose servers you want to send your requests. The Output Format specifies how the results are formatted.</p> <p>ML-Git allows you to have your bucket directly on AWS infrastructure or through MinIO. This document is divided into two sections wich describe how configure each one of these.</p>"},{"location":"aws_s3_configuration/#aws","title":"AWS","text":"<p>Internally ML-Git uses Boto3 to communicate with AWS services. Boto3 is the Amazon Web Services (AWS) SDK for Python.  It enables Python developers to create, configure, and manage AWS services.</p> <p>Boto3 looks at various configuration locations until it finds configuration values. The following lookup order is used searching through sources for configuration values:</p> <ul> <li>Environment variables</li> <li>The ~/.aws/config file</li> </ul> <p><code>Note:</code>  If, when creating a storage, you define a specific profile to be used, Boto3 will only search for that profile in the ~/.aws/config file.</p> <p>You can configure the AWS in three ways (environment variables, through the console or with the AWS Command Line Interface). These are described in the following sections.</p> <p>1 - Environment Variables</p> <p>Linux or macOS: <pre><code>export AWS_ACCESS_KEY_ID=your-access-key\nexport AWS_SECRET_ACCESS_KEY=your-secret-access-key\nexport AWS_DEFAULT_REGION=us-west-2\n</code></pre></p> <p>Windows: <pre><code>setx AWS_ACCESS_KEY_ID your-access-key\nsetx AWS_SECRET_ACCESS_KEY your-secret-access-key\nsetx AWS_DEFAULT_REGION us-west-2\n</code></pre> 2 -  Console </p> <p>From the home directory (UserProfile) execute:   </p> <pre><code>mkdir .aws\n</code></pre> <p>You need to create two files to store the sensitive credential information (\\~/.aws/credentials) separated from the less sensitive configuration options (\\~/.aws/config). To create these two files type the following commands:</p> <p>For config file:</p> <pre><code>echo \"\n[your-profile-name]\nregion=bucket-region\noutput=json \n\" &gt; .aws/config\n</code></pre> <p>For credentials file: <pre><code>echo \"\n[your-profile-name]\naws_access_key_id = your-access-key\naws_secret_access_key = your-secret-access-key     \n\" &gt; .aws/credentials\n</code></pre></p> <p>3 - AWS CLI</p> <p>For general use, the aws configure command is the fastest way to set up but requires the AWS CLI installed. To install and configure type the following commands:</p> <p><pre><code>pip install awscli\naws configure\n</code></pre> <pre><code>AWS Access Key ID [None]: your-access-key\nAWS Secret Access Key [None]: your-secret-access-key\nDefault region name [None]: bucket-region\nDefault output format [None]: json\n</code></pre></p> <p>These commands will create the files ~/.aws/credentials and ~/.aws/config.</p> <ul> <li>Demonstrating AWS Configure</li> </ul> <p></p>"},{"location":"azure_configurations/","title":"Azure container configuration","text":"<p>ML-Git allows the user to choose to have their data stored in an Azure Blob Storage that provides massively scalable storage for unstructured data like images, videos, or documents.</p> <p>This section explains how to configure the settings that ML-Git uses to interact with your Azure container. </p> <p>To establish the connection between ML-Git and Azure services, you will need a connection string which can be found on the Azure portal. See the image below:</p> <p></p> <p>With this connection string in hand, you can configure your environment in two ways (this order is the one used by ML-Git to get your credentials):</p> <ul> <li>Environment Variable</li> <li>Azure CLI</li> </ul>"},{"location":"azure_configurations/#1-environment-variable","title":"1. Environment Variable","text":"<p>You can add the connection string to your system's set of variables. ML-Git will look for the variable AZURE_STORAGE_CONNECTION_STRING.</p> <p>To add the system variable, run the following command:</p> <p>Windows:</p> <p><code>setx AZURE_STORAGE_CONNECTION_STRING \"&lt;yourconnectionstring&gt;\"</code></p> <p>Linux or macOS:</p> <p><code>export AZURE_STORAGE_CONNECTION_STRING=\"&lt;yourconnectionstring&gt;\"</code></p>"},{"location":"azure_configurations/#2-azure-cli","title":"2. Azure CLI","text":"<p>The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources. The Azure CLI is available across Azure services and is designed to get you working quickly with Azure, with an emphasis on automation.</p> <p>Azure CLI uses a file to store the configurations that are used by its services. To add settings to the file, simply run the following command:</p> <pre><code>az configure\n</code></pre> <p>If tou prefer, you can create a configuration file through the console. The configuration file itself is located at <code>$AZURE_CONFIG_DIR/config</code>. The default value of <code>AZURE_CONFIG_DIR</code> is <code>$HOME/.azure</code> on Linux and macOS, and <code>%USERPROFILE%\\.azure</code> on Windows.</p> <p>From the home directory (UserProfile) execute:   </p> <pre><code>mkdir .azure\n</code></pre> <p>You need to create the config file with the connection string value:</p> <pre><code>echo \"\n [storage]\n connection_string = \"&lt;yourconnectionstring&gt;\"\n\" &gt; .azure/config\n</code></pre>"},{"location":"centralized_cache_and_objects/","title":"Cache and Objects","text":""},{"location":"centralized_cache_and_objects/#centralized-cache","title":"Centralized cache","text":"<p>Centralized cache is a configuration mode that allows cached files to be shared between multiple users on the same machine, reducing the total cost of disk space. Currently, this feature works only in Linux and derivative machines.</p> <p>:warning:Caution:</p> <ul> <li>We encourage the use of centralized cache just with mutability set as strict.</li> <li>It is necessary to deactivate the feature <code>fs.protected_hardlinks</code>, because ML-Git uses hardlink to share cache files. Be aware that changing this setting is a risky operation, as malicious people can exploit this (see the extract below). Do this only if you really need to use the Centralized Cache feature. Remember to revert this change if you will stop to use Centralized Cache.</li> </ul> <p>Please read this extract from kernel.org about protected_hardlinks setting:</p> <p>protected_hardlinks:</p> <p>A long-standing class of security issues is the hardlink-based time-of-check-time-of-use race, most commonly seen in world-writable directories like /tmp. The common method of exploitation of this flaw is to cross privilege boundaries when following a given hardlink (i.e. a root process follows a hardlink created by another user). Additionally, on systems without separated partitions, this stops unauthorized users from \"pinning\" vulnerable setuid/setgid files against being upgraded by the administrator, or linking to special files.</p> <p>When set to \"0\", hardlink creation behavior is unrestricted.</p> <p>When set to \"1\" hardlinks cannot be created by users if they do not already own the source file, or do not have read/write access to it.</p> <p>This protection is based on the restrictions in Openwall and grsecurity.</p> <p>Changing fs.protected_hardlinks:</p> <ol> <li>Execute in terminal: <code>sudo gedit /etc/sysctl.conf</code></li> <li>Search for: <code>#fs.protected_hardlinks = 0</code> and uncomment (remove \u2018#\u2019). If you didn't find it, add a line with <code>#fs.protected_hardlinks = 0</code> to this file</li> <li>Then execute: <code>sudo sysctl -p</code></li> </ol>"},{"location":"centralized_cache_and_objects/#requirements","title":"Requirements","text":"<p>Machine's root user (administrator).</p>"},{"location":"centralized_cache_and_objects/#configuration-steps","title":"Configuration steps","text":"<p>1 - Create a common directory for each entity with read and write permission for all users:</p> <pre><code>sudo mkdir -p /srv/mlgit/cache/dataset\nsudo mkdir -p /srv/mlgit/cache/labels\nsudo mkdir -p /srv/mlgit/cache/model\n</code></pre> <p>Change permissions:</p> <pre><code>sudo chmod -R a+rwX /srv/mlgit/cache/dataset\nsudo chmod -R a+rwX /srv/mlgit/cache/labels\nsudo chmod -R a+rwX /srv/mlgit/cache/model\n</code></pre> <p>2 - With the project ml-git initialized change .ml-git/config.yaml :</p> <pre><code>datasets:\n  git: ''\n  cache_path: 'Cache path directory created on step 1 for dataset entity'\nlabels:\n  git: ''\n  cache_path: 'Path directory created on step 1 for labels entity'\nmodels:\n  git: ''\n  cache_path: 'Path directory created on step 1 for model entity'\nstorages: {}\n</code></pre>"},{"location":"centralized_cache_and_objects/#centralized-objects","title":"Centralized objects","text":"<p>Centralized objects is a configuration that allow to user share ml-git\u2019s data between machine\u2019s users, avoiding downloading times.</p>"},{"location":"centralized_cache_and_objects/#requirements_1","title":"Requirements","text":"<p>Machine's root user (administrator).</p>"},{"location":"centralized_cache_and_objects/#configuration-steps_1","title":"Configuration steps","text":"<p>1 - Create a common directory for each entity with read and write permission for all users:</p> <p>For Windows users</p> <pre><code>mkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\dataset\nmkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\labels\nmkdir \\a %ALLUSERSPROFILE%\\mlgit\\objects\\model\n</code></pre> <p>or</p> <pre><code>mkgit \\a C:\\ProgramData\\mlgit\\objects\\dataset\nmkgit \\a C:\\ProgramData\\mlgit\\objects\\labels\nmkgit \\a C:\\ProgramData\\mlgit\\objects\\model\n</code></pre> <p>For Linux and derivatives users</p> <pre><code>sudo mkdir -p /srv/mlgit/objects/dataset\nsudo mkdir -p /srv/mlgit/objects/labels\nsudo mkdir -p /srv/mlgit/objects/model\n</code></pre> <p>Change permissions</p> <pre><code>sudo chmod -R a+rwX /srv/mlgit/objects/dataset\nsudo chmod -R a+rwX /srv/mlgit/objects/labels\nsudo chmod -R a+rwX /srv/mlgit/objects/model\n</code></pre> <p>2 - With the project ml-git initialized change .ml-git/config.yaml :</p> <pre><code>datasets:\n  git: ''\n  objects_path: 'Path directory created on step 1 for dataset entity'\nlabels:\n  git: ''\n  objects_path: 'Path directory created on step 1 for labels entity'\nmodels:\n  git: ''\n  objects_path: 'Path directory created on step 1 for model entity'\nstorages: {}\n</code></pre>"},{"location":"developer_info/","title":"Contributing to ML-Git","text":"<p>The ML-Git project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways. The main way to contribute is following the next steps:</p> <ol> <li>Fork the repository into your own GitHub</li> <li>Clone the repository to your local machine</li> <li>Create a new branch for your changes using the following pattern (feature | bugfix | hotfix)/branch_name. Example: feature/sftp_storage_implementation</li> <li>Make changes and test</li> <li>Push the changes to your repository</li> <li>Create a Pull Request from your forked repository to the ML-Git repository with comprehensive description of changes</li> </ol> <p>Another way to contribute with the community is creating an issue to track your ideas, doubts, enhancements, tasks, or bugs found. If an issue with the same topic already exists, discuss on the issue.</p>"},{"location":"developer_info/#installing-for-development","title":"Installing for Development","text":"<p>To be able to contribute with our project, you will need to have the following requirements in your machine: </p> <ul> <li>Python 3.6.1+</li> <li>Pipenv</li> <li>Git</li> <li>Docker (required only for Integration Tests execution)</li> </ul>"},{"location":"developer_info/#running-the-tests","title":"Running the Tests","text":"<p>After developing, you must run the unit and integration tests. To be able to do that:</p> <ol> <li> <p>Install Docker:</p> </li> <li> <p>Windows</p> </li> <li>Linux</li> </ol> <p>The Integration Tests script starts a MinIO container on your local machine (port 9000) to be used as storage during tests execution.</p> <ol> <li> <p>[Optional] Install and configure Make to run tests easily:</p> </li> <li> <p>Windows</p> </li> <li> <p>Linux</p> </li> <li> <p>Configure git:</p> </li> </ol> <p><code>git config --global user.name \"First Name and Last Name\"</code> <code>git config --global user.email \"your_name@example.com\"</code> </p>"},{"location":"developer_info/#running-unit-tests","title":"Running Unit Tests","text":"<p>You can run unit tests through: </p>"},{"location":"developer_info/#using-make","title":"Using Make","text":"<p>Execute on terminal:</p> <pre><code>cd ml-git\nmake test.unit\n</code></pre>"},{"location":"developer_info/#without-make","title":"Without Make","text":"<p>Linux</p> <p>Execute on terminal:</p> <pre><code>cd ml-git\nsh ./scripts/run_unit_tests.sh\n</code></pre> <p>Windows</p> <p>Execute on Powershell or CMD:</p> <pre><code>cd ml-git\n.\\scripts\\run_unit_tests.bat\n</code></pre>"},{"location":"developer_info/#running-integration-tests","title":"Running Integration Tests","text":"<p>You can run integration tests through:</p>"},{"location":"developer_info/#using-make_1","title":"Using Make","text":"<p>Execute on terminal:</p> <pre><code>cd ml-git\nmake test.integration </code></pre>"},{"location":"developer_info/#without-make_1","title":"Without Make","text":"<p>Linux</p> <p>Execute on terminal:</p> <pre><code>cd ml-git\nsh ./scripts/run_integration_tests.sh\n</code></pre> <p>Windows</p> <p>Execute on Powershell or CMD:</p> <pre><code>cd ml-git\n.\\scripts\\run_integration_tests.bat\n</code></pre>"},{"location":"developer_info/#google-drive-integration-test","title":"Google Drive Integration Test","text":"<p>To run google drive integration test you need to: 1. Create directory tests/integration/credentials-json</p> <ol> <li> <p>Put your credentials file with name credentials.json in the folder you created in step 1</p> <p>Example of credentials.json: <pre><code>{\"installed\":{\"client_id\":\"fake_client_id     \",\"project_id\":\"project\",\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"fake_client_secret                                       \",\"redirect_uris\":[\"urn:ietf:wg:oauth:2.0:oob\",\"http://localhost\"]}}\n</code></pre></p> </li> <li> <p>Create a folder with name mlgit/test-folder in your GDrive</p> </li> <li> <p>Create files mlgit/B and mlgit/test-folder/A with any content, make sure that files aren't Google Files.</p> <p>You should have the following structure in your drive: <pre><code>YourDrive\n|\n\u251c\u2500\u2500 mlgit\n\u2502   \u251c\u2500\u2500 B\n\u2502   \u2514\u2500\u2500 test-folder\n\u2502       \u2514\u2500\u2500 A\n</code></pre></p> </li> <li> <p>Create tests/integration/gdrive-files-links.json with shared links of mlgit/B and mlgit/test-folder.</p> <p>Example of gdrive-files-links.json: <pre><code>{\n  \"test-folder\": \"https://drive.google.com/drive/folders/1MvWrQtPVDuJ5-XB82dMwRI8XflBZ?usp=sharing\",\n  \"B\": \"https://drive.google.com/file/d/1uy6Kao8byRqTPv-Plw8tuhITyh5N1Uua/view?usp=sharing\"\n}\n</code></pre></p> </li> </ol> <p>The Google Drive Integration Tests are set to not run by default (as they require extra setup, as mentioned earlier).  To include the integration tests for Google Drive storage during an integration tests run, you should execute:</p>"},{"location":"developer_info/#using-make_2","title":"Using Make","text":"<p>Execute on terminal:</p> <pre><code>cd ml-git\nmake test.integration.gdrive\n</code></pre>"},{"location":"developer_info/#without-make_2","title":"Without Make","text":"<p>Linux</p> <p>Execute on terminal:</p> <pre><code>cd ml-git\nsh ./scripts/run_integration_tests.sh --gdrive\n</code></pre> <p>Windows</p> <p>Execute on Powershell or CMD:</p> <pre><code>cd ml-git\n.\\scripts\\run_integration_tests.bat --gdrive\n</code></pre>"},{"location":"developer_info/#executing-a-single-test-file","title":"Executing a Single Test File","text":"<p>To execute a specific integration tests file, execute the <code>run_integration_tests</code> script accordingly with your operating system and pass the test file path relative to integration tests folder (tests/integration/).</p> <p>See the below examples running test_01_init.py located at <code>ml-git/tests/integration/test_01_init.py</code>:</p> <p>Linux: <pre><code>cd ml-git\nsh ./scripts/run_integration_tests.sh test_01_init.py\n</code></pre></p> <p>Windows: <pre><code>cd ml-git\n.\\scripts\\run_integration_tests.bat test_01_init.py\n</code></pre></p>"},{"location":"downloadable_environment/","title":"Downloadable environment","text":""},{"location":"downloadable_environment/#about","title":"About","text":"<p>This image enables new users to get started with ML-Git in a lightweight Linux-based image without worrying about configurations. The image also include a git repository with a predefined dataset and a minio instance populated with the dataset's data.</p>"},{"location":"downloadable_environment/#how-to-use","title":"How to use:","text":"<ol> <li> <p>Ensure that you have Docker installed.</p> </li> <li> <p>Inside root of ML-Git directory build the image locally with the following command:</p> </li> </ol> <p><code>make docker.build</code></p> <p>or</p> <p><code>docker build -t mlgit_docker_env -f docker/Dockerfile .</code></p> <ol> <li>Run the Docker container to launch the built image:</li> </ol> <p><code>make docker.run</code></p> <p>or</p> <p><code>docker run -it -p 8888:8888 --name mlgit_env mlgit_docker_env</code></p> <pre><code>Port 8888 will be used to start the jupyter notebook web service.\n</code></pre>"},{"location":"downloadable_environment/#using-the-ml-git-with-environment-inside-docker-container","title":"Using the ML-Git with environment (inside docker container):","text":"<p>The container has an ML-Git project initialized inside the directory workspace, the content of the versioned tag is an image from mnist database. </p> <p>You can execute the command checkout directly to tag: </p> <pre><code>ml-git datasets checkout handwritten__digits__mnist__1\n</code></pre>"},{"location":"downloadable_environment/#summary-of-files-in-image","title":"Summary of files in image:","text":"<p>local_server.git (local git repository, used to store metadafiles). data (directory used by the bucket to store project data). init.sh (script that run basic command to use ml-git). minio (minio executable). local_ml_git_config_server.git (local git repositoy with configuration files, used by ml-git clone). ml-git (source code of ml-git). workspace (initialized ml-git project).</p>"},{"location":"first_project/","title":"Your 1st ML artefacts under ML-Git management","text":"<p>We will divide this quick howto into 6 main sections:</p> <ol> <li> <p>ML-Git repository configuration / intialization </p> <ul> <li>This section explains how to initialize and configure a repository for ML-Git, considering the scenarios of the storage be an S3 or a MinIO.</li> </ul> </li> <li> <p>Uploading a dataset</p> <ul> <li>Having a repository initialized, this section explains how to create and upload a dataset to the storage.</li> </ul> </li> <li> <p>Adding data to a dataset</p> <ul> <li>This section explains how to add new data to an entity already versioned by ML-Git.</li> </ul> </li> <li> <p>Uploading labels associated to a dataset</p> <ul> <li>This section describes how to upload a set of labels by associating the dataset to which these labels refer.</li> </ul> </li> <li> <p>Uploading models</p> <ul> <li>This section explains how to create and upload your models.</li> </ul> </li> <li> <p>Downloading a dataset</p> <ul> <li>This section describes how to download a versioned data set using ML-Git.</li> </ul> </li> <li> <p>Checking data integrity</p> <ul> <li>This section explains how to check the integrity of the metadata repository.</li> </ul> </li> </ol> <p>At the end of each section there is a video to demonstrate the ML-Git usage.</p>"},{"location":"first_project/#initial-configuration-of-ml-git","title":"Initial configuration of ML-Git","text":"<p>Make sure you have created your own git repository (more information) for dataset metadata and a S3 bucket or a MinIO server for the dataset actual data. If you haven't created it yet, you can use the resources initialization script which aims to facilitate the creation of resources (buckets and repositories).</p> <p>After that, create a ML-Git project. To do this, use the following commands (note that 'mlgit-project' is the project name used as example):</p> <pre><code>mkdir mlgit-project &amp;&amp; cd mlgit-project (or clone an existing repo from Github or Github Enterprise)\nml-git repository init\n</code></pre> <p></p> <p>Now, we need to configure our project with the remote configurations. This section is divided into two parts according to the storage: Setting up a ml-git project with S3 and Setting up a ml-git project with MinIO.</p> <p>After configuring the project with the bucket, the remote ones, the credentials that will be used, and the other configurations that were performed in this section, a good practice is to make the version of the .ml-git folder that was generated in a git repository.</p> <p>That way in future projects or if you want to share with someone  you can use the command <code>ml-git clone</code> to import the project's settings,  without having to configure it for each new project.</p>"},{"location":"first_project/#setting-up-an-ml-git-project-with-minio","title":"Setting up an ML-Git project with MinIO","text":"<p>In addition to creating the MinIO server, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure MinIO for this.</p> <p>For a basic ML-Git repository, you need to add a remote repository for metadata and the MinIO bucket configuration.</p> <pre><code>ml-git repository remote datasets add git@github.com:example/your-mlgit-datasets.git\nml-git repository storage add mlgit-datasets --credentials=mlgit --endpoint-url=&lt;minio-endpoint-url&gt;\n</code></pre> <p>Last but not least, initialize the metadata repository.</p> <pre><code>ml-git datasets init\n</code></pre> <p>Setting up an ML-Git project with MinIO:</p> <p></p>"},{"location":"first_project/#setting-up-an-ml-git-project-with-s3","title":"Setting up an ML-Git project with S3","text":"<p>Similar to the MinIO setup, in addition to creating the bucket in S3, it is necessary to configure the settings that the ML-Git uses to interact with your bucket, see how to configure a S3 bucket for more details.</p> <p>For a basic ML-Git repository, you need to add a remote repository for metadata and a S3 bucket configuration.</p> <pre><code>ml-git repository remote datasets add git@github.com:example/your-mlgit-datasets.git\nml-git repository storage add mlgit-datasets --credentials=mlgit\n</code></pre> <p>After that initialize the metadata repository.</p> <pre><code>ml-git datasets init\n</code></pre>"},{"location":"first_project/#why-ml-git-uses-git","title":"Why ML-Git uses git?","text":"<p>The ML-Git uses git to versioning project's metadata. See below versioned metadata:</p> <ul> <li>.spec, is the specification file that contains informations like version number, artefact name, entity type (dataset, label, model), categories (list of labels to categorize an entity).</li> <li>MANIFEST.yaml, is responsible to map artefact's files. The files are mapped by hashes, that are the references used to perform operations in local files, and download/upload operations in storages (S3, MinIO, Azure, GoogleDrive and SFTP).</li> </ul> <p>You can find more information about metadata here.</p> <p>All configurations are stored in .ml-git/config.yaml and you can look at configuration state at any time with the following command: <pre><code>ml-git repository config show\n</code></pre> Output: <pre><code>config:\n{'batch_size': 20,\n 'cache_path': '',\n 'datasets': {'git': 'git@github.com:example/your-mlgit-datasets.git'},\n 'index_path': '',\n 'labels': {'git': ''},\n 'metadata_path': '',\n 'mlgit_conf': 'config.yaml',\n 'mlgit_path': '.ml-git',\n 'models': {'git': ''},\n 'object_path': '',\n 'push_threads_count': 10,\n 'refs_path': '',\n 'storages': {'s3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'},\n                                      'endpoint-url': &lt;minio-endpoint-url&gt;,\n                                      'region': 'us-east-1'}}},\n 'verbose': 'info'}\n</code></pre></p>"},{"location":"first_project/#uploading-a-dataset","title":"Uploading a dataset","text":"<p>To create and upload a dataset to a storage, you must be in an already initialized project, if necessary read section 1 to initialize and configure a project.</p> <p>ML-Git expects any dataset to be specified under datasets/ directory of your project and it expects a specification file with the name of the dataset. To create this specification file for a new entity you must run the following command:</p> <pre><code>ml-git datasets create imagenet8 --categories=\"computer-vision, images\" --mutability=strict --storage-type=s3h --bucket-name=mlgit-datasets\n</code></pre> <p>This command will create the dataset directory at the root of the project entity. If you want to create a version of your dataset in a different directory, you can use the --entity-dir parameter to inform the relative directory where the entity is to be created. Example:</p> <pre><code>ml-git datasets create imagenet8 --categories=\"computer-vision, images\" --mutability=strict --storage-type=s3h --bucket-name=mlgit-datasets --entity-dir=folderA/folderB\n</code></pre> <p>After that a file must have been created in datasets/folderA/folderB/imagenet8/imagenet8.spec and should look like this:</p> <pre><code>dataset:\n  categories:\n    - computer-vision\n    - images\n  manifest:\n    storage: s3h://mlgit-datasets\n  mutability: strict\n  name: imagenet8\n  version: 1\n</code></pre> <p>There are 5 main items in the spec file:</p> <ol> <li>name: it's the dataset name </li> <li>version: the version should be a positive integer, incremented each time a new version is pushed into ML-Git. You can use the --bumpversion as an argument to do the automatic increment when you add more files to a dataset.</li> <li>categories : labels to categorize the entity. This information is used by ML-Git to create the tag in the git repository managing the metadata.</li> <li>manifest: describes the data storage in which the data is actually stored. In the above example, a S3 bucket named mlgit-datasets. The AWS credential profile name and AWS region should be found in the ML-Git config file.</li> <li>mutability: describes the mutability option that your project has. The mutability options are \"strict\", \"flexible\" and \"mutable\", after selecting one of these options, you cannot change that. If you want to know more about each type of mutability and how it works, please take a look at mutability documentation.</li> </ol> <p>The items listed above are mandatory in the spec. An important point to note is if the user wishes, it is possible to add new items that will be versioned with the spec. The example below presents a spec with the entity's owner information to be versioned. Those information were put under metadata field just for purpose of organization.</p> <pre><code>dataset:\n  categories:\n    - computer-vision\n    - images\n  mutability: strict\n  manifest:\n    storage: s3h://mlgit-datasets\n  name: imagenet8\n  version: 1\n  metadata:\n    owner:\n        name: &lt;your-name-here&gt;\n        email: &lt;your-email-here&gt;\n</code></pre> <p>After creating the dataset spec file, you can create a README.md to create a web page describing your dataset, adding references and any other useful information. Then, you can put the data of that dataset under the directory. Below, you will see the tree of imagenet8 directory and file structure:</p> <pre><code>imagenet8/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 train\n\u2502   \u2502   \u251c\u2500\u2500 train_data_batch_1\n\u2502   \u2502   \u251c\u2500\u2500 train_data_batch_2\n\u2502   \u2502   \u251c\u2500\u2500 train_data_batch_3\n\u2502   \u2502   \u251c\u2500\u2500 train_data_batch_4\n\u2502   \u2502   \u251c\u2500\u2500 train_data_batch_5\n\u2502   \u2502   \u251c\u2500\u2500 train_data_batch_6\n\u2502   \u2502   \u251c\u2500\u2500 train_data_batch_7\n\u2502   \u2502   \u251c\u2500\u2500 train_data_batch_8\n\u2502   \u2502   \u251c\u2500\u2500 train_data_batch_9\n\u2502   \u2502   \u2514\u2500\u2500 train_data_batch_10\n\u2502   \u2514\u2500\u2500 val\n\u2502       \u2514\u2500\u2500 val_data\n\u2514\u2500\u2500 imagenet8.spec\n</code></pre> <p>You can look at the working tree status with the following command:</p> <p><pre><code>ml-git datasets status imagenet8\n</code></pre> Output: <pre><code>INFO - Repository: dataset: status of ml-git index for [imagenet8]\nChanges to be committed\n\nuntracked files\n    imagenet8.spec\n    README.md\n    data\\train\\train_data_batch_1\n    data\\train\\train_data_batch_2\n    data\\train\\train_data_batch_3\n    data\\train\\train_data_batch_4\n    data\\train\\train_data_batch_5\n    data\\train\\train_data_batch_6\n    data\\train\\train_data_batch_7\n    data\\train\\train_data_batch_8\n    data\\train\\train_data_batch_9\n    data\\train\\train_data_batch_10\n    data\\val\\val_data\n\ncorrupted files\n</code></pre></p> <p>That command allows printing the tracked files and the ones in the index/staging area. Now, you are ready to put that new dataset under ML-Git management. For this, do:</p> <pre><code>ml-git datasets add imagenet8\n</code></pre> <p>The command \"ml-git dataset add\" adds the files into a specific dataset, such as imagenet8 in the index/staging area. If you check the working tree status, you can see that now the files appear as tracked but not committed yet:</p> <p><pre><code>ml-git datasets status imagenet8\n</code></pre> Output: <pre><code>INFO - Repository: dataset: status of ml-git index for [imagenet8]\nChanges to be committed\n    new file:   data\\train\\train_data_batch_1\n    new file:   data\\train\\train_data_batch_2\n    new file:   data\\train\\train_data_batch_3\n    new file:   data\\train\\train_data_batch_4\n    new file:   data\\train\\train_data_batch_5\n    new file:   data\\train\\train_data_batch_6\n    new file:   data\\train\\train_data_batch_7\n    new file:   data\\train\\train_data_batch_8\n    new file:   data\\train\\train_data_batch_9\n    new file:   data\\train\\train_data_batch_10\n    new file:   data\\val\\val_data\n\nuntracked files\n\ncorrupted files\n</code></pre></p> <p>Then, you can commit the metadata to the local repository. For this purpose, type the following command:</p> <pre><code>ml-git datasets commit imagenet8\n</code></pre> <p>After that, you can use \"ml-git dataset push\" to update the remote metadata repository just after storing all actual data under management in the specified remote data storage.</p> <pre><code>ml-git datasets push imagenet8\n</code></pre> <p>As you can observe, ML-Git follows very similar workflows as git.</p> <p>Uploading a dataset:</p> <p></p>"},{"location":"first_project/#adding-data-to-a-dataset","title":"Adding data to a dataset","text":"<p>If you want to add data to a dataset, perform the following steps:</p> <ul> <li>In your workspace, copy the new data in under <code>datasets/&lt;your-dataset&gt;/data</code></li> <li> <p>Modify the version number. To do this step you have two ways:</p> <ol> <li>You can put the option <code>--bumpversion</code> on the add command to auto increment the version number, as shown below.</li> <li>Or, you can put the option <code>--version</code> on the commit command to set an specific version number.</li> </ol> </li> <li> <p>After that, like in the previous section, you need to execute the following commands to upload the new data:</p> </li> </ul> <pre><code>ml-git datasets add &lt;your-dataset&gt; --bumpversion\nml-git datasets commit &lt;your-dataset&gt;\nml-git datasets push &lt;your-dataset&gt;\n</code></pre> <p>This will create a new version of your dataset and push the changes to your remote storage (e.g. S3).</p> <p>Adding data to a dataset:</p> <p></p>"},{"location":"first_project/#uploading-labels-associated-to-a-dataset","title":"Uploading labels associated to a dataset","text":"<p>To create and upload labels associated to a dataset, you must be in an already initialized project, if necessary read section 1 to initialize and configure the project. Also, you will need to have a dataset already versioned by ML-Git in your repository, see section 2.</p> <p>The first step is to configure your metadata and data repository/storage.</p> <pre><code>ml-git repository remote labels add git@github.com:example/your-mlgit-labels.git\nml-git repository storage add mlgit-labels --endpoint-url=&lt;minio-endpoint-url&gt;\nml-git labels init\n</code></pre> <p>Even these commands show a different bucket to store the labels data. It would be possible to store both datasets and labels into the same bucket.</p> <p>If you look at your config file, you would see the following information: <pre><code>ml-git repository config show\n</code></pre> Output: <pre><code>config:\n{'batch_size': 20,\n 'cache_path': '',\n 'datasets': {'git': 'git@github.com:example/your-mlgit-datasets.git'},\n 'index_path': '',\n 'labels': {'git': 'git@github.com:example/your-mlgit-labels.git'},\n 'metadata_path': '',\n 'mlgit_conf': 'config.yaml',\n 'mlgit_path': '.ml-git',\n 'models': {'git': ''},\n 'object_path': '',\n 'refs_path': '',\n 'storages': {'s3h': {'mlgit-datasets': {'aws-credentials': {'profile': 'default'},\n                                      'endpoint-url': &lt;minio-endpoint-url&gt;,\n                                      'region': 'us-east-1'}}},\n              's3h': {'mlgit-labels': {'aws-credentials': {'profile': 'default'},\n                                      'endpoint-url': &lt;minio-endpoint-url&gt;,\n                                      'region': 'us-east-1'}}},\n 'verbose': 'info'}\n</code></pre></p> <p>Then, you can create your first set of labels. As an example, we will use mscoco. ML-Git expects any set of labels to be specified under the labels/ directory of your project. Also, it expects a specification file with the name of the labels.</p> <pre><code>ml-git labels create mscoco-captions --categories=\"computer-vision, captions\" --mutability=mutable --storage-type=s3h --bucket-name=mlgit-labels --version=1\n</code></pre> <p>After create the entity, you can create the README.md describing your set of labels. Below is the tree of caption labels for the mscoco directory and file structure:</p> <pre><code>mscoco-captions/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 annotations\n\u2502   \u251c\u2500\u2500 captions_train2014.json\n\u2502   \u2514\u2500\u2500 captions_val2014.json\n\u2514\u2500\u2500 mscoco-captions.spec\n</code></pre> <p>Now, you are ready to put the new set of labels under ML-Git management.  We assume there is an existing mscoco dataset. For this, do:</p> <pre><code>ml-git labels add mscoco-captions\nml-git labels commit mscoco-captions --dataset=mscoco\nml-git labels push mscoco-captions\n</code></pre> <p>The commands are very similar to dataset operations. However, you can note one particular change in the commit command. There is an option \"--dataset\" which is used to tell ML-Git that the labels should be linked to the specified dataset. Internally, ML-Git will look at the checked out dataset in your workspace for that specified dataset. Then, it will include the git tag and sha into the specification file to be committed into the metadata repository. Once done, anyone will be able to retrieve the exact same version of the dataset that has been used for that specific set of labels.</p> <p>One can look at the specific dataset associated with that set of labels by executing the following command: <pre><code>ml-git labels show mscoco-captions\n</code></pre> Output: <pre><code>-- labels : mscoco-captions --\ncategories:\n- computer-vision\n- captions\ndataset:\n  sha: 607fa818da716c3313a6855eb3bbd4587e412816\n  tag: computer-vision__images__mscoco__1\nmanifest:\n  files: MANIFEST.yaml\n  storage: s3h://mlgit-datasets\nname: mscoco-captions\nversion: 1\n</code></pre></p> <p>As you can see, there is a new section \"dataset\" that has been added by ML-Git with the sha &amp; tag fields. It can be used to checkout the exact version of the dataset for that set of labels.</p> <p>Uploading labels related to a dataset:</p> <p></p>"},{"location":"first_project/#uploading-models","title":"Uploading Models","text":"<p>To create and upload your model, you must be in an already initialized project, if necessary read section 1 to initialize and configure a project.</p> <p>The first step is to configure your metadata &amp; data repository/storage.</p> <pre><code>ml-git repository remote models add git@github.com:example/your-mlgit-models.git\nml-git repository storage add mlgit-models --endpoint-url=&lt;minio-endpoint-url&gt;\nml-git models init\n</code></pre> <p>To create a model entity, you can run the following command:</p> <pre><code>ml-git models create imagenet-model --categories=\"computer-vision, images\" --storage-type=s3h --mutability=mutable --bucket-name=mlgit-models\n</code></pre> <p>After creating the model, we add the model file to the data folder. Here below is the directory tree structure:</p> <pre><code>imagenet-model/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 model_file\n\u2514\u2500\u2500 imagenet-model.spec\n</code></pre> <p>Now, you're ready to put that new model set under ML-Git management. We assume there is an existing imagenet8 dataset and mscoco-captions labels. For this, do:</p> <pre><code>ml-git models add imagenet-model\nml-git models commit imagenet-model --dataset=imagenet8 --labels=mscoco-captions\nml-git models push imagenet-model\n</code></pre> <p>There is not much change compared to dataset and labels operation. You can use the options \"-- dataset\" and \"--labels\", which tells to ml-git that the model should be linked to the specified dataset and labels. Internally, ml-git will look in your workspace for the checked out dataset and labels specified in the options. It then will include the reference to the checked out versions into the model's specification file to be committed into the metadata repository. Once done, anyone will then be able to retrieve the exact same version of the dataset and labels that has been used for that specific model.</p> <p>Persisting model's metrics:</p> <p>We can insert metrics to the model in the add command, metrics can be added with the following parameters:</p> <ol> <li>metrics-file: optional metrics file path. It is expected a CSV file containing the metric names in the header and the values in the next line.</li> <li>metric: optional metric keys and values.</li> </ol> <p>An example of adding a model passing a metrics file, would be the following command:</p> <pre><code>ml-git models add imagenet-model --metrics-file='/path/to/your/file.csv'\n</code></pre> <p>An example of adding a model passing metrics through the command line, would be the following command:</p> <pre><code>ml-git models add imagenet-model --metric accuracy 10 --metric precision 20 --metric recall 30\n</code></pre> <p>Obs: The parameters used above were chosen for example purposes, you can name your metrics however you want to, you can also pass as many metrics as you want, as long as you use the command correctly.</p> <p>When inserting the metrics, they will be included in the structure of your model's spec file. An example of what it would look like would be the following structure:</p> <pre><code>model:\n  categories:\n    - computer-vision\n    - images\n  manifest:\n    storage: s3h://mlgit-models\n  metrics:\n    accuracy: 10.0\n    precision: 20.0\n    recall: 30.0\n  name: imagenet-model\n  version: 1\n</code></pre> <p>You can view metrics for all tags for that entity by running the following command:</p> <pre><code>ml-git models metrics imagenet-model\n</code></pre> <p></p>"},{"location":"first_project/#downloading-a-dataset","title":"Downloading a dataset","text":"<p>We assume there is an existing ML-Git repository with a few ML datasets under its management and you'd like to download one of the available datasets. If you don't have a dataset versioned by the ML-Git, see section 2 on how to do this.</p> <p>To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment:</p> <pre><code>ml-git clone git@github.com:example/your-mlgit-repository.git\n</code></pre> <p>Then, go to the project directory: <pre><code>cd your-mlgit-repository\n</code></pre></p> <p>Inside the configured ML-Git project directory, the following command will update the metadata repository, allowing visibility of what has been shared since the last update (new ML entity, new versions).</p> <pre><code>ml-git datasets update\n</code></pre> <p>Or update all metadata repository:</p> <pre><code>ml-git repository update\n</code></pre> <p>To discover which datasets are under ML-Git management, you can execute the following command:</p> <p><pre><code>ml-git datasets list\n</code></pre> Output: <pre><code>ML dataset\n|-- folderA\n|   |-- folderB\n|   |   |-- dataset-ex-minio\n|   |   |-- imagenet8\n|   |   |-- dataset-ex\n</code></pre></p> <p>The ML-Git repository contains 3 different datasets, all falling under the same directories folderA/folderB (These directories were defined when the entity was created and can be modified at any time by the user).</p> <p>In order for ML-Git to manage the different versions of the same dataset. It internally creates a tag based on categories, ML entity name and its version. To show all these tag representing the versions of a dataset, simply type the following:</p> <p><pre><code>ml-git datasets tag list imagenet8\n</code></pre> Output: <pre><code>computer-vision__images__imagenet8__1\ncomputer-vision__images__imagenet8__2\n</code></pre></p> <p>It means there are actually 2 versions under ML-Git management. You can check what version is checked out in the ML-Git workspace with the following command:</p> <p><pre><code>ml-git datasets branch imagenet8\n</code></pre> Output: <pre><code>('vision-computing__images__imagenet8__2', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e')\n</code></pre></p> <p>The output is a tuple:</p> <ol> <li>The tag auto-generated by ML-Git based on the .spec.</li> <li>The sha of the git commit of that version. </li> </ol> <p>It is simple to retrieve a specific version locally to start any experiment by executing one of the following commands:</p> <p><pre><code>ml-git datasets checkout computer-vision__images__imagenet8__1\n</code></pre> or  <pre><code>ml-git datasets checkout imagenet8 --version=1\n</code></pre></p> <p>If you want to get the latest available version of an entity you can just pass its name in the checkout command, as shown below:</p> <pre><code>ml-git datasets checkout imagenet8\n</code></pre> <p>Getting the data will auto-create a directory structure under dataset directory as shown below. That structure folderA/folderB is actually the structure in which the dataset was versioned.</p> <pre><code>folderA\n\u2514\u2500\u2500 folderB\n    \u2514\u2500\u2500 imagenet8\n        \u251c\u2500\u2500 README.md\n        \u251c\u2500\u2500 data\n        \u2502   \u251c\u2500\u2500 train\n        \u2502   \u2502   \u251c\u2500\u2500 train_data_batch_1\n        \u2502   \u2502   \u251c\u2500\u2500 train_data_batch_2\n        \u2502   \u2502   \u251c\u2500\u2500 train_data_batch_3\n        \u2502   \u2502   \u251c\u2500\u2500 train_data_batch_4\n        \u2502   \u2502   \u251c\u2500\u2500 train_data_batch_5\n        \u2502   \u2502   \u251c\u2500\u2500 train_data_batch_6\n        \u2502   \u2502   \u251c\u2500\u2500 train_data_batch_7\n        \u2502   \u2502   \u251c\u2500\u2500 train_data_batch_8\n        \u2502   \u2502   \u251c\u2500\u2500 train_data_batch_9\n        \u2502   \u2502   \u2514\u2500\u2500 train_data_batch_10\n        \u2502   \u2514\u2500\u2500 val\n        \u2502       \u2514\u2500\u2500 val_data\n        \u2514\u2500\u2500 imagenet8.spec\n</code></pre> <p>Downloading a dataset:</p> <p></p>"},{"location":"first_project/#checking-data-integrity","title":"Checking data integrity","text":"<p>If at some point you want to check the integrity of the metadata repository (e.g. computer shuts down during a process), simply type the following command:</p> <pre><code>ml-git datasets fsck\nINFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\objects\\hashfs]\nINFO - HashFS: starting integrity check on [.\\.ml-git\\dataset\\index\\hashfs]\n[1] corrupted file(s) in Local Repository\n[0] corrupted file(s) in Index\nTotal of corrupted files: 1\nINFO - Repository: For more information about the corrupted files you can run the command with the --verbose option.\n</code></pre> <p>That command will walk through the internal ML-Git directories (index &amp; local repository) and will check the integrity of all blobs under its management. It will fix and return the list of blobs that are corrupted or missing.</p> <p>Checking data integrity:</p> <p></p>"},{"location":"first_project/#changing-a-dataset","title":"Changing a Dataset","text":"<p>When adding files to an entity ML-Git locks the files for read only. When the entity's mutability type is flexible or mutable, you can change the data of a file and resubmit it without being considered corrupted.</p> <p>In case of a flexible entity you should perform the following command to unlock the file:</p> <pre><code>ml-git datasets unlock imagenet8 data\\train\\train_data_batch_1\n</code></pre> <p>After that, the unlocked file is subject to modification. If you modify the file without performing this command, it will be considered corrupted.</p> <p>To upload the data, you can execute the following commands:</p> <pre><code>ml-git datasets add &lt;yourdataset&gt; --bumpversion\nml-git datasets commit &lt;yourdataset&gt;\nml-git datasets push &lt;yourdataset&gt;\n</code></pre> <p>This will create a new version of your dataset and push the changes to your remote storage (e.g. S3).</p> <p>Changing a dataset:</p> <p></p>"},{"location":"first_steps/","title":"Getting Started with ML-Git","text":"<p>First steps is a chapter to explore how a user should do a basic setup and first project</p>"},{"location":"first_steps/#installation","title":"Installation","text":"<p>To install ML-Git, run the following command from the command line:</p> <pre><code>pip install git+git://github.com/HPInc/ml-git.git\n</code></pre> <p>For more details, see the Installation Guide.</p>"},{"location":"first_steps/#setting-up","title":"Setting up","text":"<p>As ML-Git leverages git to manage ML entities metadata, it is required that you configure git user and email address:</p> <pre><code>git config --global user.name \"Your User\"\ngit config --global user.email \"your_email@example.com\"\n</code></pre> <p>To fully configure and use ML-Git, you will also need to create a git repository and the storage type you want to use. You can create manually, the git repository and storage, or use a script provided in our advanced scenarios. We recommend that for your first project you stick with this tutorial, so you can exercise many ML-Git commands.</p>"},{"location":"first_steps/#ml-git-first-project","title":"ML-Git First Project","text":"<p>After completing the previous steps, you can create your first project. </p>"},{"location":"first_steps/#configuring-git-repository-and-storage","title":"Configuring Git Repository and Storage","text":"<p>First, create a folder for your ML-Git project (We will use as an example the folder named \"mlgit-project\"):</p> <pre><code>mkdir mlgit-project &amp;&amp; cd mlgit-project\n</code></pre> <p>Then, we will initialize this folder as an ML-Git repository:</p> <pre><code>ml-git repository init\n</code></pre> <p>The next step is configure the remote repositories and buckets that your project will use to store data.</p> <p>To configure the git repository: <pre><code>ml-git repository remote datasets add git@github.com:user/user-mlgit-project\n</code></pre></p> <p>To configure the storage: <pre><code>ml-git repository storage add mlgit-datasets --endpoint-url=&lt;minio-endpoint-url&gt;\n</code></pre></p>"},{"location":"first_steps/#adding-your-first-dataset","title":"Adding Your First Dataset","text":"<p>Now, you have repositories, and storage configurated for your project.  To create and upload your first dataset to a storage, first, run the below command: <pre><code>ml-git datasets init\n</code></pre></p> <p>Then, you can run the below command to create your dataset <pre><code>ml-git datasets create imagenet8 --categories=computer-vision --mutability=strict --bucket-name=mlgit-datasets\n</code></pre> It will generate an output saying that the project was created. Also, it will create a series of folders and files with the specifications of the dataset. You can see the generated files looking into the root folder.</p> <p>After you add your dataset files inside the folder, you can run the following command to see the dataset status: <pre><code>ml-git datasets status imagenet8\n</code></pre> Below, you can see a possible output: <pre><code>INFO - Repository: datasets: status of ml-git index for [imagenet8]\nChanges to be committed:\n\nUntracked files:\n    README.md\n    imagenet8.spec\n    data/   -&gt;  3 FILES\n\nCorrupted files:\n</code></pre></p> <p>Above, the output shows some untracked files. To commit these files, similarly to git, we can run the following sequence of commands:</p> <p>The following command will add all untracked files: <pre><code>ml-git datasets add imagenet8\n</code></pre> The following command will commit the metadata to the local repository: <pre><code>ml-git datasets commit imagenet8\n</code></pre> The following command  will update the remote metadata repository: <pre><code>ml-git datasets push imagenet8\n</code></pre></p>"},{"location":"first_steps/#downloading-a-dataset","title":"Downloading a Dataset","text":"<p>If you already have access to an existing ML-Git project. You can clone the repository and use ML-Git to bring a dataset to your workspace.</p> <p>To clone a repository use the command: <pre><code>ml-git clone git@github.com:example/your-mlgit-repository.git\n</code></pre></p> <p>Then, go to the project directory: <pre><code>cd your-mlgit-repository\n</code></pre></p> <p>Now you can discover which datasets are under ML-Git management by executing the command: <pre><code>ml-git datasets list\n</code></pre></p> <p>It will generate a similar output as you can see below: <pre><code>ML dataset\n|-- folderA\n|   |-- folderB\n|   |   |-- dataset-ex-minio\n|   |   |-- imagenet8\n|   |   |-- dataset-ex\n</code></pre> The example above represets a ML-Git repository containing 3 different datasets, all falling under the same directory folderA/folderB (This hierarchy was defined when the entity was created and can be modified at any time by the user).</p> <p>In order for ML-Git to manage the different versions of the same dataset. It internally creates a tag based on categories, ML entity name and its version. To show all these tag representing the versions of a dataset, simply type the following: <pre><code>ml-git datasets tag list imagenet8\n</code></pre></p> <p>A possible output: <pre><code>computer-vision__images__imagenet8__1\ncomputer-vision__images__imagenet8__2\n</code></pre></p> <p>It means there are 2 versions under ML-Git management. You can check what version is checked out in the ML-Git workspace with the following command:</p> <pre><code>ml-git datasets branch imagenet8\n</code></pre> <p>It is simple to retrieve a specific version locally to start any experiment by executing one of the following commands:</p> <p><pre><code>ml-git datasets checkout computer-vision__images__imagenet8__1\n</code></pre> or  <pre><code>ml-git datasets checkout imagenet8 --version=1\n</code></pre></p> <p>If you want to get the latest available version of an entity you can just pass its name in the checkout command, as shown below: <pre><code>ml-git datasets checkout imagenet8\n</code></pre></p> <p>Downloading a Dataset:</p> <p></p>"},{"location":"gdrive_configurations/","title":"Google Drive API configuration","text":"<p>This section aims to explain how to enable Google Drive API and configure OAuth 2.0 credentials to use with ML-Git.</p>"},{"location":"gdrive_configurations/#enabling-drive-api","title":"Enabling Drive API","text":"<p>You need to create a project in Google developer console to activate Drive API, follow instructions bellow:</p> <p>1. Access console developer and click on create project:</p> <p></p> <p>2. Then type name of your preference and click on \"CREATE\" button:</p> <p></p> <p>3. Go back to dashboard and enable Drive API:</p> <p></p> <p>4. Search for Drive API on search bar:</p> <p></p> <p></p>"},{"location":"gdrive_configurations/#creating-credentials","title":"Creating credentials","text":"<p>When you finish Enabling API step, you need to create your credentials and configure authentication consent screen.</p> <p>1. Click on create credentials:</p> <p></p> <p></p> <p>2. Select user type of your consent:</p> <p></p> <p>3. Add application name to authentication consent screen:</p> <p></p> <p>4. Change application's scope if you prefer and save:</p> <p></p> <p>5. Go back to dashboard and click on create credentials and generate your API KEY:</p> <p></p> <p></p> <p>6. Generate OAuth client id:</p> <p></p> <p>7. Add client name and select application type:</p> <p></p> <p>8. Finally you can download your credentials file:</p> <p></p>"},{"location":"gdrive_configurations/#setting-up-a-ml-git-project-with-google-drive","title":"Setting up a ML-Git project with Google Drive","text":"<p>Create directory with name  of your preference and copy your credentials  file with name credentials.json inside the directory.</p> <p>Add storage configurations example:</p> <pre><code>ml-git repository storage add path-in-your-drive --type=gdriveh --credentials=/home/profile/.gdrive\n</code></pre> <p>After that initialize the metadata repository:</p> <pre><code>ml-git datasets init\n</code></pre> <p>We strongly recommend that you add <code>push_threads_count: 10</code> option in your .ml-git/config.yaml, because of Google Drive API request limit of 10/s. This option change the number of workers used in multithreading push process, by default the number of workers is cpu numbers multiplied by 5. </p> <p>The push command was tested with 10 workers and the request limit was not exceeded.</p> <p>Configuration example:</p> <pre><code>batch_size: 20\npush_threads_count: 10\ndatasets:\n  git: ''\nlabels:\n  git: ''\nmodels:\n  git: ''\nstorages:\n  gdriveh:\n    dataset-gdrive:\n      credentials-path: /home/profile/.gdrive\n</code></pre>"},{"location":"installation_guide/","title":"ML-Git Installation - A detailed guide.","text":""},{"location":"installation_guide/#requirements","title":"Requirements","text":"<p>ML-Git requires a Python version 3.7 or superior, and the Python package manager, pip, to be installed on your system.  Also, git is required as ML-Git uses git to manage ML entities metadata. </p> <p>You can check if you already have these installed from the command line:</p> <p><pre><code>python --version &amp;&amp; pip --version &amp;&amp; git --version\n</code></pre> Output: <pre><code>Pyhon 3.8.2\npip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.8)\ngit version 2.25.1\n</code></pre></p> <p>If you already have those packages installed, you may skip down to Installing ML-Git.</p>"},{"location":"installation_guide/#installing-python","title":"Installing Python","text":"<p>Install Python using your package manager of choice, or by downloading an installer appropriate for your system from python.org and running it.</p>"},{"location":"installation_guide/#installing-pip","title":"Installing Pip","text":"<p>If you're using a recent version of Python, the Python package manager, pip, is most likely installed by default. However, you may need to upgrade pip to the lasted version:</p> <pre><code>pip install --upgrade pip\n</code></pre> <p>If you need to install pip for the first time, download get-pip.py. Then run the following command to install it:</p> <pre><code>python get-pip.py\n</code></pre>"},{"location":"installation_guide/#installing-git","title":"Installing git","text":"<p>You can install git using the following command:</p> <pre><code>sudo apt install git\n</code></pre>"},{"location":"installation_guide/#installing-ml-git","title":"Installing ML-Git","text":"<p>Install the ML-Git package using pip:</p> <pre><code>pip install git+git://github.com/HPInc/ml-git.git\n</code></pre> <p>You should now have the ML-Git installed on your system. Run <code>ml-git --version</code> to check that everything worked okay.</p> <p><pre><code>ml-git --version\n</code></pre> ml-git 2.0.1</p> <p>Install the ML-Git package using the Source Code:</p> <p>Also, you can download ML-Git from the repository and execute commands below:</p> <p>Windows:</p> <pre><code>cd ml-git/\npython3.7 setup.py install\n</code></pre> <p>Linux:</p> <pre><code>cd ml-git/\nsudo python3.7 setup.py install\n</code></pre> <p>The output should be the same as using pip.</p>"},{"location":"minio_s3_configuration/","title":"MinIO","text":""},{"location":"minio_s3_configuration/#minio","title":"MinIO","text":"<p>MinIO is a cloud storage server compatible with Amazon S3. The following sections will explain how to properly set up so ML-Git can work by using the Access Key and Secret Access Key of your MinIO user.</p>"},{"location":"minio_s3_configuration/#running-minio-locally","title":"Running MinIO locally","text":"<p>In case you want to run MinIO locally for testing purposes, it's possible to run a docker container using the following command: <pre><code>docker run -v /path/to/your/dir:/data --name=minio --network=host minio/minio server --console-address \":9001\" /data\n</code></pre> The command will start the MinIO API and Console servers in ports 9000 and 9001, respectively.</p> <p>Once you have successfully started the container, you can access the Console URL (usually http://127.0.0.1:9001) using the default user (username: minioadmin, password: minioadmin) to create a new user (setting up its Access Key and Secret Access Key) or create new buckets.</p> <p>After you finish creating a new user, remember that you'll need the proper local URL for the API to be used in the --endpoint-url parameter of the storage creation command, it'll usually be <code>http://127.0.0.1:9000</code>.</p> <p><code>Note:</code> In case you decide to work with a deployed MinIO server instead, just remember to use the proper URL when creating new storage and to have your MinIO user's Access Key and Secret Access Key in hand for the following setup.</p>"},{"location":"minio_s3_configuration/#credentials-configuration","title":"Credentials configuration","text":"<p>This section explains how to configure the settings that the ML-Git uses to interact with your bucket. This requires that you have the following data:</p> <ol> <li>Profile Name</li> <li>Access Key ID</li> <li>Secret Access Key</li> <li>Output Format</li> </ol> <p>The Access Key ID and Secret Access Key are the credentials for your MinIO user. The Output Format specifies how the results are formatted.</p> <p>Internally ML-Git uses Boto3 to communicate with the MinIO API. Even though Boto3 is the Amazon Web Services (AWS) SDK for Python, we can still use it to communicate with the MinIO services.</p> <p>Boto3 looks at various configuration locations until it finds configuration values. The following lookup order is used searching through sources for configuration values:</p> <ul> <li>Environment variables</li> <li>The ~/.aws/config file</li> </ul> <p><code>Note:</code>  If, when creating a storage, you define a specific profile to be used, Boto3 will only search for that profile in the ~/.aws/config file.</p> <p>You can configure the credentials in three ways (environment variables, through the console or with the AWS Command Line Interface). These are described in the following sections.</p> <p>1 - Environment Variables</p> <p>Linux or macOS:</p> <pre><code>export AWS_ACCESS_KEY_ID=your-access-key\nexport AWS_SECRET_ACCESS_KEY=your-secret-access-key\n</code></pre> <p>Windows:</p> <pre><code>C:\\&gt; setx AWS_ACCESS_KEY_ID your-access-key\nC:\\&gt; setx AWS_SECRET_ACCESS_KEY your-secret-access-key\n</code></pre> <p>2 -  Console </p> <p>From the home directory (UserProfile) execute:   </p> <pre><code>mkdir .aws\n</code></pre> <p>You need to create two files to store the sensitive credential information (\\~/.aws/credentials) separated from the less sensitive configuration options (\\~/.aws/config). To create these two files type the following commands:</p> <p>For config file:</p> <pre><code>echo \"\n[your-profile-name]\noutput=json \n\" &gt; .aws/config\n</code></pre> <p>For credentials file: <pre><code>echo \"\n[your-profile-name]\naws_access_key_id = your-access-key\naws_secret_access_key = your-secret-access-key     \n\" &gt; .aws/credentials\n</code></pre></p> <p>3 - AWS CLI</p> <p>For general use, the aws configure command is the fastest way to set up but requires the AWS CLI installed. To install and configure type the following commands:</p> <p><pre><code>pip install awscli\naws configure\n</code></pre> <pre><code>AWS Access Key ID [None]: your-access-key\nAWS Secret Access Key [None]: your-secret-access-key\nDefault region name [None]: \nDefault output format [None]: json\n</code></pre></p> <p>These commands will create the files ~/.aws/credentials and ~/.aws/config.</p> <ul> <li>Demonstrating AWS Configure</li> </ul> <p></p>"},{"location":"mlgit_commands/","title":"ML-Git commands","text":"<code> ml-git --help </code> <pre><code>Usage: ml-git [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --version  Show the version and exit.\n\nCommands:\n  clone       Clone an ml-git repository ML_GIT_REPOSITORY_URL\n  datasets    Management of datasets within this ml-git repository.\n  labels      Management of labels sets within this ml-git repository.\n  models      Management of models within this ml-git repository.\n  repository  Management of this ml-git repository.\n</code></pre> <p>Example: <pre><code>ml-git --help\n</code></pre></p> <code> ml-git --version </code> <p>Displays the installed version of ML-Git.</p> <code> ml-git &lt;ml-entity&gt; add </code> <p></p> <pre><code>Usage: ml-git datasets add [OPTIONS] ML_ENTITY_NAME [FILE_PATH]...\n\n  Add datasets change set ML_ENTITY_NAME to the local ml-git staging area.\n\nOptions:\n  --bumpversion                   Increment the version number when adding\n                                  more files.\n  --fsck                          Run fsck after command execution.\n  --metric &lt;TEXT FLOAT&gt;...        Metric key and value.\n  --metrics-file                  Metrics file path.\n  --wizard                        Enable the wizard to request information\n                                  when needed.\n  --verbose                       Debug mode\n</code></pre> <p>Dataset example: <pre><code>ml-git datasets add dataset-ex --bumpversion\n</code></pre></p> <p>ml-git expects datasets to be managed under dataset directory. \\&lt;ml-entity-name&gt; is also expected to be a repository under the tree structure and ml-git will search for it in the tree. Under that repository, it is also expected to have a \\&lt;ml-entity-name&gt;.spec file, defining the ML entity to be added. Optionally, one can add a README.md which will describe the dataset and be what will be shown in the github repository for that specific dataset.</p> <p>Internally, the ml-git add will add all the files under the \\&lt;ml-entity&gt; directory into the ml-git index / staging area.</p> <p>Model example: <pre><code>ml-git models add model-ex --metrics-file='/path/to/your/file.csv'\n</code></pre></p> <p>ml-git allows you to enter a metrics file or the metrics themselves on the command line when adding a model.</p> <code> ml-git &lt;ml-entity&gt; branch </code> <p></p> <pre><code>Usage: ml-git datasets branch [OPTIONS] ML_ENTITY_NAME\n\n  This command allows to check which tag is checked out in the ml-git\n  workspace.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets branch imagenet8\n</code></pre> Output: <pre><code>('vision-computing__images__imagenet8__1', '48ba1e994a1e39e1b508bff4a3302a5c1bb9063e')\n</code></pre></p> <p>That information is equal to the HEAD reference from a git concept. ml-git keeps that information on a per \\&lt;ml-entity-name&gt; basis. which enables independent checkout of each of these \\&lt;ml-entity-name&gt;.</p> <p>The output is a tuple: 1) the tag auto-generated by ml-git based on the \\&lt;ml-entity-name&gt;.spec (composite with categories, \\&lt;ml-entity-name&gt;, version) 2) the sha of the git commit of that \\&lt;ml-entity&gt; version Both are the same representation. One is human-readable and is also used internally by ml-git to find out the path to the referenced \\&lt;ml-entity-name&gt;.</p> <code> ml-git &lt;ml-entity&gt; checkout </code> <p></p> <pre><code>Usage: ml-git models checkout [OPTIONS] ML_ENTITY_TAG|ML_ENTITY\n\n  Checkout the ML_ENTITY_TAG|ML_ENTITY of a model set into user workspace.\n\nOptions:\n  -l, --with-labels           The checkout associated labels  in user\n                              workspace as well.\n  -d, --with-dataset          The checkout associated dataset in user\n                              workspace as well.\n  --retry INTEGER RANGE       Number of retries to download the files from the\n                              storage. This number must be in the range \n                              0-999999999 [default: 2].\n  --force                     Force checkout command to delete\n                              untracked/uncommitted files from local\n                              repository.\n  --bare                      Ability to add/commit/push without having the\n                              ml-entity checked out.\n  --version INTEGER RANGE     Number of artifact version to be downloaded.\n                              This number must be in the range 0-999999999 \n                              [default: latest].\n  --fail-limit INTEGER RANGE  Number of failures before aborting the command.\n                              This number must be in the range 0-999999999\n                              [default: no limit].\n  --full                      Show all contents for each directory.\n  --wizard                    Enable the wizard to request information when\n                              needed.\n  --verbose                   Debug mode\n</code></pre> <p>Examples: <pre><code>ml-git datasets checkout computer-vision__images__faces__fddb__1\n</code></pre> or you can use the name of the entity directly and download the latest available tag <pre><code>ml-git datasets checkout fddb\n</code></pre></p> <p>Note:</p> <p><code>--d:</code> It can only be used in checkout of labels and models to get the entities that are associated with the entity.</p> <p><code>--l:</code> It can only be used in checkout of models to get the label entity that are associated with the entity.</p> <p><code>--sample-type, --sampling, --seed:</code> These options are available only for dataset. If you use this option ml-git will not allow you to make changes to the entity and create a new tag.</p> <code> ml-git &lt;ml-entity&gt; commit </code> <p></p> <pre><code>Usage: ml-git models commit [OPTIONS] ML_ENTITY_NAME\n\n  Commit model change set of ML_ENTITY_NAME locally to this ml-git\n  repository.\n\nOptions:\n  --dataset NOT EMPTY STRING  Link a dataset entity name to this model set\n                              version\n  --labels NOT EMPTY STRING   Link a labels entity name to this model set\n                              version\n  --version INTEGER RANGE     Set the version number of the artifact. This\n                              number must be in the range 0-999999999.\n  -m, --message TEXT          Use the provided &lt;msg&gt; as the commit message.\n  --fsck                      Run fsck after command execution.\n  --wizard                    Enable the wizard to request information when\n                              needed.\n  --verbose                   Debug mode\n</code></pre> <p>Example: <pre><code>ml-git models commit model-ex --dataset=dataset-ex\n</code></pre></p> <p>This command commits the index / staging area to the local repository. It is a 2-step operation in which 1) the actual data (blobs) is copied to the local repository, 2) committing the metadata to the git repository managing the metadata. Internally, ml-git keeps track of files that have been added to the data storage and is storing that information to the metadata management layer to be able to restore any version of each \\&lt;ml-entity-name&gt;.</p> <p>Another important feature of ml-git is the ability to keep track of the relationship between the ML entities. So when committing a label set, one can (should) provide the option <code>--dataset=&lt;dataset-name&gt;</code>. Internally, ml-git will inspect the HEAD / ref of the specified \\&lt;dataset-name&gt; checked out in the ml-git repository and will add that information to the specificatino file that is committed to the metadata repository. With that relationship kept into the metadata repository, it is now possible for anyone to checkout exactly the same versions of labels and dataset.</p> <p>Same for ML model, one can specify which dataset and label set that have been used to generate that model through <code>--dataset=&lt;dataset-name&gt;</code> and <code>--labels=&lt;labels-name&gt;</code></p> <code> ml-git &lt;ml-entity&gt; create </code> <p></p> <pre><code>Usage: ml-git datasets create [OPTIONS] ARTIFACT_NAME\n\n  This command will create the workspace structure with data and spec file\n  for an entity and set the git and storage configurations. [This command \n  has a wizard that will request the necessary information if it is not \n  informed]\n\nOptions:\n  --categories TEXT               Artifact's categories names. The categories\n                                  names must be separated by comma. \n                                  E.g: \"category1,category2,category3\". [required]\n  --mutability [strict|flexible|mutable]\n                                  Mutability type.  [required]\n  --storage-type [s3h|azureblobh|gdriveh|sftph]\n                                  Storage type (s3h, azureblobh, gdriveh,\n                                  sftph) [default: s3h]\n  --version INTEGER RANGE         Set the version number of the artifact. This\n                                  number must be in the range 0-999999999.\n  --import NOT EMPTY STRING       Path to be imported to the project. NOTE:\n                                  Mutually exclusive with argument:\n                                  credentials_path, import_url.\n  --wizard-config                 If specified, ask interactive questions at\n                                  console for git &amp; storage configurations.\n                                  [DEPRECATED: This option should no longer be\n                                  used.]\n  --bucket-name NOT EMPTY STRING  Bucket name\n  --import-url NOT EMPTY STRING   Import data from a google drive url. NOTE:\n                                  Mutually exclusive with argument: import.\n  --credentials-path NOT EMPTY STRING\n                                  Directory of credentials.json. NOTE: This\n                                  option is required if --import-url is used.\n  --unzip                         Unzip imported zipped files. Only available\n                                  if --import-url is used.\n  --entity-dir NOT EMPTY STRING   The relative path where the entity will be\n                                  created inside the ml entity directory.\n  --wizard                        Enable the wizard to request information\n                                  when needed.\n  --verbose                       Debug mode\n</code></pre> <p>Examples:  - To create an entity with s3 as storage and importing files from a path of your computer: <pre><code>ml-git datasets create imagenet8 --storage-type=s3h --categories=\"computer-vision, images\" --version=0 --import='/path/to/dataset' --mutability=strict\n</code></pre></p> <ul> <li>To create an entity with s3 as storage and importing files from a google drive URL: <pre><code>ml-git datasets create imagenet8 --storage-type=s3h --categories=computer-vision,images --import-url='gdrive.url' --credentials-path='/path/to/gdrive/credentials' --mutability=strict --unzip\n</code></pre></li> </ul> <code> ml-git &lt;ml-entity&gt; diff </code> <p></p> <pre><code>Usage: ml-git datasets diff [OPTIONS] ML_ENTITY_NAME FIRST_TAG SECOND_TAG\n\n  Print the difference between two entity tag versions. The command will\n  show added, updated and deleted files.\n\nOptions:\n  --full     Show all contents for each directory.\n  --verbose  Debug mode\n</code></pre> <p>Examples:  - To check the difference between entity tag versions: <pre><code>ml-git datasets diff dataset-ex computer-vision__images__dataset-ex__1 computer-vision__images__dataset-ex__4\n</code></pre> Output: <pre><code>Added files:\n    data/   -&gt;      4 FILES\n    tabular.csv\nUpdated files:\n    data/dataset_test.csv\nDeleted files:\n    data/dataset_old.csv\n</code></pre></p> <ul> <li>To check the difference between entity tag versions showing all contents for each directory: <pre><code>ml-git datasets diff --full dataset-ex computer-vision__images__dataset-ex__1 computer-vision__images__dataset-ex__4\n</code></pre> Output: <pre><code>Added files:\n    data/dataset_1.csv\n    data/dataset_2.csv\n    data/dataset_3.csv\n    data/dataset_4.csv\n    tabular.csv\nUpdated files:\n    data/dataset_test.csv\nDeleted files:\n    data/dataset_old.csv\n</code></pre></li> </ul> <code> ml-git &lt;ml-entity&gt; export </code> <p></p> <pre><code>Usage: ml-git datasets export [OPTIONS] ML_ENTITY_TAG BUCKET_NAME\n\n  This command allows you to export files from one storage (S3|MinIO) to\n  another (S3|MinIO).\n\nOptions:\n  --credentials TEXT     Profile of AWS credentials [default: default].\n  --endpoint TEXT        Storage endpoint url.\n  --region TEXT          AWS region name [default: us-east-1].\n  --retry INTEGER RANGE  Number of retries to download the files from the\n                         storage. This number must be in the range\n                         0-999999999 [default: 2].\n  --verbose              Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets export computer-vision__images__faces__fddb__1 minio\n</code></pre></p> <code> ml-git &lt;ml-entity&gt; fetch </code> <p></p> <pre><code>Usage: ml-git datasets fetch [OPTIONS] ML_ENTITY_TAG\n\n  Allows you to download just the metadata files of an entity.\n\nOptions:\n  --sample-type [group|range|random]\n  --sampling TEXT                 The group: &lt;amount&gt;:&lt;group&gt; The group sample\n                                  option consists of amount and group used to\n                                  download a sample.\n                                  range: &lt;start:stop:step&gt;\n                                  The range sample option consists of start,\n                                  stop and step used to download a sample. The\n                                  start parameter can be equal or greater than\n                                  zero.The stop parameter can be 'all', -1 or\n                                  any integer above zero.\n                                  random:\n                                  &lt;amount:frequency&gt; The random sample option\n                                  consists of amount and frequency used to\n                                  download a sample.\n  --seed TEXT                     Seed to be used in random-based samplers.\n  --retry INTEGER RANGE           Number of retries to download the files from\n                                  the storage. This number must be in the\n                                  range 0-999999999 [default: 2].\n  --verbose                       Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets fetch computer-vision__images__faces__fddb__1\n</code></pre></p> <code> ml-git &lt;ml-entity&gt; fsck </code> <p></p> <pre><code>Usage: ml-git datasets fsck [OPTIONS]\n\nOptions:\n  --fix-workspace  Use this option to repair files identified as corrupted in\n                   the entity workspace.\n  --full           Show the list of corrupted files.\n  --verbose        Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets fsck\n</code></pre></p> <p>This command will walk through the internal ml-git directories (index &amp; local repository) and check the presence and integrity of all file blobs under its management.</p> <p>This command will basically try to:</p> <ul> <li>Detect any chunk/blob that is corrupted or missing in the internal ml-git directory (.ml-git/{entity-type}/objects)</li> <li>Fetch files detected as corrupted or missing from storage</li> <li>Check the integrity of files mounted in the entities workspace</li> <li>In fix-workspace mode, repair corrupted files found in the entities workspace. A file in the entities workspace is considered 'corrupted' based on the business rule defined by the mutability of the entity. If you want to know more about each type of mutability and how it works, please take a look at Mutability documentation.</li> </ul> <p>It will return the list of blobs that are corrupted/missing if the user passes the --full option.</p> <code> ml-git &lt;ml-entity&gt; import </code> <p></p> <pre><code>Usage: ml-git datasets import [OPTIONS] BUCKET_NAME ENTITY_DIR\n\n  This command allows you to download a file or directory from the S3 or\n  Gdrive to ENTITY_DIR.\n\nOptions:\n  --credentials TEXT          Input your profile to an s3 storage or your\n                              credentials path to a gdrive storage.(eg,\n                              --credentials=path/to/.credentials\n  --region TEXT               AWS region name [default: us-east-1].\n  --retry INTEGER RANGE       Number of retries to download the files from the\n                              storage. This number must be in the range \n                              0-999999999 [default: 2].\n  --path TEXT                 Storage folder path.\n  --object TEXT               Filename in storage.\n  --storage-type [s3|gdrive]  Storage type (s3, gdrive) [default: s3]\n  --endpoint-url TEXT         Storage endpoint url.\n  --verbose                   Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets import bucket-name dataset/computer-vision/imagenet8/data\n</code></pre> For google drive storage: <pre><code>ml-git datasets import gdrive-folder --storage-type=gdrive --object=file_to_download --credentials=credentials-path dataset/\n</code></pre></p> <code> ml-git &lt;ml-entity&gt; init </code> <p></p> <pre><code>Usage: ml-git datasets init [OPTIONS]\n\n  Init a ml-git datasets repository.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets init\n</code></pre></p> <p>This command is mandatory to be executed just after the addition of a remote metadata repository (ml-git \\&lt;ml-entity&gt; remote add). It initializes the metadata by pulling all metadata to the local repository.</p> <code> ml-git &lt;ml-entity&gt; metrics </code> <p></p> <pre><code>Usage: ml-git models metrics [OPTIONS] ML_ENTITY_NAME\n\n  Shows metrics information for each tag of the entity.\n\nOptions:\n  --export-path TEXT        Set the path to export metrics to a file. NOTE:\n                            This option is required if --export-type is used.\n  --export-type [csv|json]  Choose the format of the file that will be\n                            generated with the metrics [default: json].\n  --verbose                 Debug mode\n</code></pre> <p>Example: <pre><code>ml-git models metrics model-ex\n</code></pre></p> <p>Note: <pre><code>This command is only available for model entities.\n</code></pre></p> <code> ml-git &lt;ml-entity&gt; list </code> <p></p> <pre><code>Usage: ml-git datasets list [OPTIONS]\n\n  List datasets managed under this ml-git repository.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets list\n</code></pre> Output: <pre><code>ML dataset\n|-- computer-vision\n|   |-- images\n|   |   |-- dataset-ex-minio\n|   |   |-- imagenet8\n|   |   |-- dataset-ex\n</code></pre></p> <code>ml-git &lt;ml-entity&gt; log </code> <p></p> <pre><code>Usage: ml-git datasets log [OPTIONS] ML_ENTITY_NAME\n\n  This command shows ml-entity-name's commit information like author, date,\n  commit message.\n\nOptions:\n  --stat      Show amount of files and size of an ml-entity.\n  --fullstat  Show added and deleted files.\n  --verbose   Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets log dataset-ex\n</code></pre></p> <code> ml-git &lt;ml-entity&gt; push </code> <p></p> <pre><code>Usage: ml-git datasets push [OPTIONS] ML_ENTITY_NAME\n\n  Push local commits from ML_ENTITY_NAME to remote ml-git repository &amp;\n  storage.\n\nOptions:\n  --retry INTEGER RANGE       Number of retries to download the files from the\n                              storage. This number must be in the range \n                              0-999999999 [default: 2].\n  --clearonfail               Remove the files from the storage in case of\n                              failure during the push operation.\n  --fail-limit INTEGER RANGE  Number of failures before aborting the command.\n                              This number must be in the range 0-999999999 \n                              [default: no limit].\n  --verbose                   Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets push dataset-ex\n</code></pre></p> <p>This command will perform a 2-step operations: 1. push all blobs to the configured data storage. 2. push all metadata related to the commits to the remote metadata repository.</p> <code> ml-git &lt;ml-entity&gt; remote-fsck </code> <p></p> <pre><code>Usage: ml-git datasets remote-fsck [OPTIONS] ML_ENTITY_NAME\n  This command will check and repair the remote, by default it will \n  only repair by uploading lacking chunks/blobs. Options bring more \n  specialized repairs.\n\nOptions:\n  --thorough             Try to download the IPLD if it is not present in the\n                         local repository to verify the existence of all\n                         contained IPLD links associated.\n  --paranoid             Adds an additional step that will download all IPLD\n                         and its associated IPLD links to verify the content\n                         by computing the multihash of all these.\n  --retry INTEGER RANGE  Number of retries to download the files from the\n                         storage. This number must be in the range 0-999999999 \n                         [default: 2].\n  --full                 Show the list of fixed and unfixed blobs and IPLDs.\n  --wizard               Enable the wizard to request information when needed.\n  --verbose              Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets remote-fsck dataset-ex\n</code></pre></p> <p>This ml-git command will basically try to:</p> <ul> <li>Detects any chunk/blob lacking in a remote storage for a specific ML artefact version</li> <li>Repair - if possible - by uploading lacking chunks/blobs</li> <li>In paranoid mode, verifies the content of all the blobs</li> </ul> <code> ml-git &lt;ml-entity&gt; reset </code> <p></p> <pre><code>Usage: ml-git datasets reset [OPTIONS] ML_ENTITY_NAME\n\n  Reset ml-git state(s) of an ML_ENTITY_NAME\n\nOptions:\n  --hard                     Remove untracked files from workspace, files to\n                             be committed from staging area as well as\n                             committed files upto &lt;reference&gt;.\n  --mixed                    Revert the committed files and the staged files\n                             to 'Untracked Files'. This is the default action.\n  --soft                     Revert the committed files to 'Changes to be\n                             committed'.\n  --reference [head|head~1]  head:Will keep the metadata in the current\n                             commit.\n                             head~1:Will move the metadata to the last\n                             commit.\n  --verbose                  Debug mode\n</code></pre> <p>Examples:</p> <pre><code>ml-git datasets reset dataset-ex --hard\n</code></pre> <ul> <li>Undo the committed changes.</li> <li>Undo the added/tracked files.</li> <li>Reset the workspace to fit with the current HEAD state.</li> </ul> <p><pre><code>ml-git datasets reset dataset-ex --mixed\n</code></pre> if HEAD: * nothing happens. else: * Undo the committed changes. * Undo the added/tracked files.</p> <p><pre><code>ml-git datasets reset dataset-ex --soft\n</code></pre> if HEAD: * nothing happens. else: * Undo the committed changes.</p> <code> ml-git &lt;ml-entity&gt; show </code> <p></p> <pre><code>Usage: ml-git datasets show [OPTIONS] ML_ENTITY_NAME\n\n  Print the specification file of the entity.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets show dataset-ex\n</code></pre> Output: <pre><code>-- dataset : imagenet8 --\ncategories:\n- vision-computing\n- images\nmanifest:\n  files: MANIFEST.yaml\n  storage: s3h://mlgit-datasets\nname: imagenet8\nversion: 1\n</code></pre></p> <code> ml-git &lt;ml-entity&gt; status </code> <p></p> <pre><code>Usage: ml-git datasets status [OPTIONS] ML_ENTITY_NAME [STATUS_DIRECTORY]\n\n  Print the files that are tracked or not and the ones that are in the\n  index/staging area.\n\nOptions:\n  --full     Show all contents for each directory.\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets status dataset-ex\n</code></pre></p> <code> ml-git &lt;ml-entity&gt; tag add</code> <p></p> <pre><code>Usage: ml-git datasets tag add [OPTIONS] ML_ENTITY_NAME TAG\n\n  Use this command to associate a tag to a commit.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets tag add dataset-ex my_tag\n</code></pre></p> <code> ml-git &lt;ml-entity&gt; tag list </code> <p></p> <pre><code>Usage: ml-git datasets tag list [OPTIONS] ML_ENTITY_NAME\n\n  List tags of ML_ENTITY_NAME from this ml-git repository.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets tag list dataset-ex\n</code></pre></p> <code> ml-git &lt;ml-entity&gt; update </code> <p></p> <pre><code>Usage: ml-git datasets update [OPTIONS]\n\n  This command will update the metadata repository.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets update\n</code></pre></p> <p>This command enables one to have the visibility of what has been shared since the last update (new ML entity, new versions).</p> <code> ml-git &lt;ml-entity&gt; unlock </code> <p></p> <pre><code>Usage: ml-git datasets unlock [OPTIONS] ML_ENTITY_NAME FILE\n\n  This command add read and write permissions to file or directory. Note:\n  You should only use this command for the flexible mutability option.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git datasets unlock dataset-ex data/file1.txt\n</code></pre></p> <p>Note:</p> <pre><code>You should only use this command for the flexible mutability option.\n</code></pre> <code> ml-git clone &lt;repository-url&gt; </code> <p></p> <pre><code>Usage: ml-git clone [OPTIONS] REPOSITORY_URL [DIRECTORY]\n\n  Clone an ml-git repository ML_GIT_REPOSITORY_URL\n\nOptions:\n  --untracked  Does not preserve git repository tracking.\n  --verbose    Debug mode\n</code></pre> <p>Example: <pre><code>ml-git clone https://git@github.com/mlgit-repository\n</code></pre></p> <code> ml-git login </code> <p></p> <pre><code>Usage: ml-git login [OPTIONS]\n\n  login command generates new Aws credential.\n\nOptions:\n  --credentials TEXT  profile name for store credentials [default: default].\n  --insecure          use this option when operating in a insecure location.\n                      This option prevents storage of a cookie in the folder.\n                      Never execute this program without --insecure option in\n                      a compute device you do not trust.\n  --rolearn TEXT      directly STS to this AWS Role ARN instead of the\n                      selecting the option during runtime.\n  --help              Show this message and exit.\n</code></pre> <p>Example: <pre><code>ml-git login\n</code></pre></p> <code> ml-git repository config </code> <p></p> <pre><code>Usage: ml-git repository config [OPTIONS] COMMAND [ARGS]...\n\n  Management of the ML-Git config file.\n\nOptions:\n  --set-wizard [enabled|disabled] Enable or disable the wizard for all\n                                  supported commands.\n  --help                          Show this message and exit.\n\nCommands:\n  push  Create a new version of the ML-Git configuration file.\n  show  Configuration of this ML-Git repository\n</code></pre> <p>Example: <pre><code>ml-git repository config --set-wizard=enabled\n</code></pre></p> <code> ml-git repository config push</code> <p></p> <pre><code>Usage: ml-git repository config push [OPTIONS]\n\n  Create a new version of the ML-Git configuration file. This command\n  internally runs git's add, commit and push commands.\n\nOptions:\n  -m, --message TEXT  Use the provided &lt;msg&gt; as the commit message.\n  --verbose           Debug mode\n</code></pre> <p>Example: <pre><code>ml-git repository config push -m \"My commit message\"\n</code></pre></p> <code> ml-git repository config show</code> <p></p> <pre><code>Usage: ml-git repository config show [OPTIONS]\n\n  Configuration of this ml-git repository\n\nOptions:\n  -l, --local   Local configurations\n  -g, --global  Global configurations\n  --verbose     Debug mode\n</code></pre> <p>Example: <pre><code>ml-git repository config show\n</code></pre> Output: <pre><code>config:\n{'datasets': {'git': 'git@github.com:example/your-mlgit-datasets'},\n 'storages': {'s3': {'mlgit-datasets': {'aws-credentials': {'profile': 'mlgit'},\n                                     'region': 'us-east-1'}}},\n 'verbose': 'info'}\n</code></pre></p> <p>Use this command if you want to check what configuration ml-git is running with. It is highly likely one will need to  change the default configuration to adapt for her needs.</p> <code> ml-git repository gc </code> <p></p> <pre><code>Usage: ml-git repository gc [OPTIONS]\n\n  Cleanup unnecessary files and optimize the use of the disk space.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>This command will remove unnecessary files contained in the cache and objects directories of the ml-git metadata (.ml-git).</p> <code> ml-git repository graph </code> <p></p> <pre><code>Usage: ml-git repository graph [OPTIONS]\n\n  Creates a graph of all entity relations as an HTML file and automatically\n  displays it in the default system application.\n\nOptions:\n  --verbose           Debug mode\n  --dot               Instead of creating an HTML file, it displays the graph\n                      on the command line as a DOT language.\n  --export-path TEXT  Set the directory path to export the generated graph file.\n</code></pre> <p>Example: <pre><code>ml-git repository graph\n</code></pre> Output: <pre><code>digraph \"Entities Graph\" {\n\"models-ex (1)\" [color=\"#d63638\"];\n\"dataset-ex (1)\" [color=\"#2271b1\"];\n\"models-ex (1)\" -&gt; \"dataset-ex (1)\";\n\"models-ex (1)\" [color=\"#d63638\"];\n\"labels-ex (1)\" [color=\"#996800\"];\n\"models-ex (1)\" -&gt; \"labels-ex (1)\";\n}\n</code></pre></p> <p>This command will iterate through the tags of all ML-Git entities and create the relationships between them.</p> <p>Note: </p> <pre><code>To successfully execute the command it is necessary that it is in an ML-Git project initialized, and with the URLs of the remote repositories properly configured.\n</code></pre> <code> ml-git repository init </code> <p></p> <pre><code>Usage: ml-git repository init [OPTIONS]\n\n  Initialization of this ML-Git repository\n\nOptions:\n  --help  Show this message and exit.\n</code></pre> <p>Example: <pre><code>ml-git repository init\n</code></pre></p> <p>This is the first command you need to run to initialize a ml-git project. It will bascially create a default .ml-git/config.yaml</p> <code> ml-git repository remote &lt;ml-entity&gt; add </code> <p></p> <pre><code>Usage: ml-git repository remote datasets add [OPTIONS] REMOTE_URL\n\n  Add remote dataset metadata REMOTE_URL to this ml-git repository.\n\nOptions:\n  -g, --global  Use this option to set configuration at global level\n  --verbose     Debug mode\n</code></pre> <p>Example: <pre><code>ml-git repository remote datasets add https://git@github.com/mlgit-datasets\n</code></pre></p> <code> ml-git repository remote &lt;ml-entity&gt; del </code> <p></p> <pre><code>Usage: ml-git repository remote datasets del [OPTIONS]\n\n  Remove the REMOTE_URL datasets' metadata from this ml-git repository\n\nOptions:\n  -g, --global  Use this option to set configuration at global level\n  --verbose     Debug mode\n</code></pre> <p>Example: <pre><code>ml-git repository remote datasets del\n</code></pre></p> <code> ml-git repository remote config add </code> <p></p> <pre><code>Usage: ml-git repository remote config add [OPTIONS] REMOTE_URL\n\n  Starts a git at the root of the project and configure the remote.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git repository remote config add https://git@github.com/mlgit-config\n</code></pre></p> <code> ml-git repository storage add </code> <p></p> <pre><code>Usage: ml-git repository storage add [OPTIONS] BUCKET_NAME\n\n  Add a storage BUCKET_NAME to ml-git [This command has a wizard that \n  will request the necessary information if it is not informed]\n\nOptions:\n  --credentials TEXT              Profile name for storage credentials\n  --type [s3h|azureblobh|gdriveh|sftph]\n                                  Storage type (s3h, azureblobh, gdriveh,\n                                  sftph) [default: s3h]\n  --region TEXT                   AWS region name for S3 bucket\n  --endpoint-url TEXT             Storage endpoint url.\n  --username TEXT                 The username for the sftp login.\n  --private-key TEXT              Full path for the private key file.\n  --port INTEGER                  SFTP port [default: 22].\n  -g, --global                    Use this option to set configuration at\n                                  global level\n  --wizard                        Enable the wizard to request information\n                                  when needed.\n  --verbose                       Debug mode\n</code></pre> <p>Example: <pre><code>ml-git repository storage add minio --endpoint-url=&lt;minio-endpoint-url&gt;\n</code></pre></p> <p>Use this command to add a data storage to a ml-git project.</p> <code> ml-git repository storage del </code> <p></p> <pre><code>Usage: ml-git repository storage del [OPTIONS] BUCKET_NAME\n\n  Delete a storage BUCKET_NAME from ml-git\n\nOptions:\n  --type [s3h|azureblobh|gdriveh|sftph]\n                                  Storage type (s3h, azureblobh, gdriveh,\n                                  sftph) [default: s3h]\n  -g, --global                    Use this option to set configuration at\n                                  global level\n  --wizard                        Enable the wizard to request information \n                                  when needed.\n  --verbose                       Debug mode\n</code></pre> <p>Example: <pre><code>ml-git repository storage del minio\n</code></pre></p> <code> ml-git repository update </code> <p></p> <pre><code>Usage: ml-git repository update [OPTIONS]\n\n  This command will update all ml-entities' metadata repository.\n\nOptions:\n  --verbose  Debug mode\n</code></pre> <p>Example: <pre><code>ml-git repository update\n</code></pre></p>"},{"location":"mlgit_internals/","title":"ML-Git: architecture and internals","text":""},{"location":"mlgit_internals/#metadata-data-decoupling","title":"Metadata &amp; data decoupling","text":"<p>ML-Git's first design concept is to decouple the ML entities' metadata management from the actual data. So, the tool has two main layers:</p> <ol> <li> <p>The metadata management, is responsible for organize the ML entities (Models, Datasets, and Labels) through specification files. Then, these files are managed by a git repository to store and retrieve versions of the ML entities. </p> </li> <li> <p>The data storage, is responsible to keep the files of the ML entities.</p> </li> </ol> Figure 1. Decoupling Metadata &amp; Data Management Layers"},{"location":"mlgit_internals/#cas-for-ml-git","title":"CAS for ML-Git","text":"<p>ML-Git has been implemented as a Content Addressable Storage (CAS), meaning that we can retrieve the information based on the content and not based on the information's location.</p> Figure 2. Self-Describing Content-Addressed ID <p>Figure 2 shows the basic principle of multihash to obtain a Content Identifier (CID) which is used under the hood by ML-Git to implement its CAS layer.</p> <p>In a nutshell, CID is a self-describing content-addressed identifier that enables natural evolution and customization over simple and fixed cryptographic hashing schemes. An argument why multihash is a valuable feature is that any cryptographic function ultimately ends being weak. It's been a challenge for many software to use another cryptographic hash (including git). For example, when collisions have been proven with SHA-1.</p> <p>Summarizing, a CID is:</p> <ul> <li>A unique identifier/hash of \u201cmultihash\u201d content.</li> <li>Encoding the digest of the original content enabling anyone to retrieve thatcontent wherever it lies (through some routing).</li> <li>Enabling the integrity check of the retrieved content (thx to multihash and the encoded digest).</li> </ul> Figure 3. IPLD - CID for a file <p>There are a few steps to chunk a file to get an IPLD - CID format:</p> <ol> <li>Slide the file in piece of, say, 256KB.</li> <li>For each slice, compute its digest (currently, ml-git uses sha2-256).</li> <li>Obtain the CID for all these digests. These slice of files will be saved in a data storage with the computed CID as their filename. </li> <li>Build a json describing all the chunks of the file.</li> <li>Obtain the CID of that json. That json will also be saved in the data storage with the computed CID as its filename.</li> </ol> <p>Note that this last CID is the only piece of information you need to keep to retrieve the whole image.jpg file. And last but not least, one can ensure the integrity of the file while downloading by computing the digests of all downloaded chunks and checking against the digest encoded in their CID.</p> <p>Below, you can find useful links for more information on:</p> <ul> <li>Multihash </li> <li>CID</li> <li>IPLD</li> </ul>"},{"location":"mlgit_internals/#why-slicing-files-in-chunks","title":"Why slicing files in chunks?","text":"<p>IPFS uses small chunk size of 256KB \u2026 Why?</p> <ul> <li>security - easy to DOS nodes without forcing small chunks</li> <li>deduplication - small chunks can dedup. big ones effectively dont.</li> <li>latency - can externalize small pieces already (think a stream)</li> <li>bandwidth - optimize the use of bandwidth across many peers</li> <li>performance - better perf to hold small pieces in memory. Hash along the dag to verify integrity of the whole thing.</li> </ul> <p>The big DOS problem with huge leaves is that malicious nodes can serve bogus stuff for a long time before a node can detect the problem (imagine having to download 4GB before you can check whether any of it is valid). This was super harmful for bittorrent (when people started choosing huge piece sizes), attackers would routinely do this, very cheaply - just serve bogus random data. This is why smaller chunks are used in our approach.</p>"},{"location":"mlgit_internals/#high-level-architecture-and-metadata","title":"High-level architecture and metadata","text":"Figure 4. ML-Git high-level architecture and metadata relationships <p>So IPLD/CID has been implemented on top of the storage. The chunking strategy is a recommendation to turn S3 interactions more efficient when dealing with large files. This approach is also valid when using the other supported storages: Azure, Google Drive, MinIO and SFTP. It's also interesting to note that ML-Git implements a Thread pool to concurrently upload &amp; download files to the storage. Taking into account the use of an S3 bucket, it would be possible to further accelerate ML-Git interactions with the bucket through the AWS CloudFront (not implemented yet).</p>"},{"location":"mlgit_internals/#ml-git-baseline-performance-numbers","title":"ML-Git baseline performance numbers","text":""},{"location":"mlgit_internals/#camseq01-under-ml-git","title":"CamSeq01 under ML-Git","text":"<ul> <li>CamSeq01 size : 92MB</li> <li>Locations: website in Cambridge -- S3 bucket in us-east-1 -- me in South Brazil</li> <li>Download from website: ~4min22s</li> <li>Upload to S3 with ml-git : 6m49s</li> <li>Download to S3 with ml-git : 1m11s</li> </ul>"},{"location":"mlgit_internals/#mscoco-all-files-under-ml-git","title":"MSCoco (all files) under ML-Git","text":"<ul> <li>MSCoco :<ul> <li>Size : 26GB</li> <li>Number of files : 164065 ; chunked into ~400-500K blobs (todo: exact blob count)</li> </ul> </li> <li>Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil</li> <li>Download from website: unknown</li> <li>Upload to S3 with ml-git : 12h30m</li> <li>Download to S3 with ml-git : 10h45m</li> </ul>"},{"location":"mlgit_internals/#mscoco-zip-files-under-ml-git","title":"MSCoco (zip files) under ML-Git","text":"<ul> <li>MSCoco :<ul> <li>Size : 25GB</li> <li>number of files : 3 (train.zip, test.zip, val.zip) ; 102299 blobs</li> </ul> </li> <li>Locations: original dataset: unknown -- S3 bucket in us-east-1 -- me in South Brazil</li> <li>Download from website: unknown</li> <li>Upload to S3 with ml-git : 4h35m</li> <li>Download to S3 with ml-git : 3h39m</li> </ul> <p>A couple of comments:</p> <ol> <li>Even though Python GIL is a challenge for true concurrency in the Python interpreter, it still is very helpful and provides a significant improvement for ML-Git performance.</li> <li>Not surprisingly, the number of files will affect the overall performance as it means there will be many more connections to AWS. However, ML-Git have an option to download some dataset partially (checkout with sampling) to enable CI/CD workflows for which some ML engineers may run some experiments locally on their own machine. For that reason, it is interesting to avoid downloading the full dataset if it's very large. This option is not applicable if the data set was loaded as some zip files.</li> </ol>"},{"location":"mutability_helper/","title":"Mutability","text":""},{"location":"mutability_helper/#what-is-the-mutability","title":"What is the mutability?","text":"<p>Mutability is the attribute that defines whether an entity's files can be changed by the user from one version to another.</p> <p>It is important to note that for all types of mutability the user is able to add and remove files, the mutability attribute refers to editing files already added.</p> <p>You must define carefully because once mutability is defined, it cannot be changed.</p>"},{"location":"mutability_helper/#where-to-define-mutability-policy","title":"Where to define mutability policy?","text":"<p>Mutability is defined when creating a new entity. </p> <p>With the command <code>ml-git (datasets|labels|models) create</code> you must pass the mandatory attribute <code>mutability</code> to define the type of mutability for the created entity.</p> <p>Your entity specification file (.spec) should look like this:</p> <pre><code>dataset:\n  categories:\n    - computer-vision\n    - images\n  mutability: flexible\n  manifest:\n    storage: s3h://mlgit-datasets\n  name: imagenet8\n  version: 1\n</code></pre> <p>If you create an entity without using the create command and without mutability, when trying to perform the <code>ml-git (datasets|labels|models) add</code> the command will not be executed and you will be informed that you must define a mutability for that new entity.</p> <p>Note: <pre><code>For entities that were created before the mutability parameter became mandatory and that did not define mutability, ml-git treats these entities as strict.\n</code></pre></p> <p>Because it is an attribute defined in the spec, you can define a type of mutability for each entity that the project has. For example, you can have in the same project a dataset-ex1 that has strict mutability while a dataset-ex2 has flexible mutability.</p>"},{"location":"mutability_helper/#polices","title":"Polices","text":"<p>Currently the user can define one of the following three types of mutability for their entities:</p> <ul> <li> <p>Mutable:</p> <ul> <li>Disable ml-git cache (will slow down some operations).</li> <li>Files that were already added may be changed and added again.</li> </ul> </li> <li> <p>Flexible:</p> <ul> <li>Added files will be set to read-only after added to avoid any changes.</li> <li>If you want to change a file, you MUST use <code>ml-git &lt;ml-entity&gt; unlock &lt;file&gt;</code>. About unlock command:<ul> <li>Decouple the file from ml-git cache to avoid propagating changes and creating \"corruptions\".</li> <li>Enable file write permission, so that you can edit the file.</li> <li>If you modify a file without previously executing the unlock command, the file will be considered corrupted.</li> </ul> </li> </ul> </li> <li> <p>Strict:</p> <ul> <li>Added files will be set to read-only after added to avoid any changes.</li> <li>Once added, the files could not be modified in any other tag.</li> </ul> </li> </ul>"},{"location":"mutability_helper/#choosing-the-type-of-mutability","title":"Choosing the type of mutability","text":"<p>The type of mutability must be chosen based on the characteristics of the entity you are working with.  You must define carefully because once mutability is defined, it cannot be changed.</p> <p>If you are working with images, it is recommended that the type of mutability chosen is strict, since it is not common for images to be changed. Rather, new images are added to the set. In a scenario such as data augmentation, new images will be created from the originals, but the originals must remain intact.</p> <p>If you are working with shared cache we encourage to use mutability strict only. Take a look at the document about centralized cache.</p> <p>When dealing with files that must be modified over time, such as a csv file with your dataset labels, or your model file, we encourage you to use flexible or mutable. The choice will depend on how often you believe these files will be modified.</p>"},{"location":"plugins/","title":"ML-Git Data Specialization Plugins","text":"<p>Data specialization plugins are resources that can be added to ML-Git providing specific processing and metadata collection for specific data formats. This document aims to provide instructions on how data specialization plugins can be developed for ML-Git, defining interface methods that must be implemented to provide the necessary functionalities for processing these data.</p>"},{"location":"plugins/#plugin-contracts","title":"Plugin contracts","text":"<code> add_metadata </code> <p>This method is responsible for processing or gathering information about the versioned data and inserting it into the specification file. If the plugin is installed and properly configured, this signature will be triggered before the metadata is committed. </p> <p>Definition:</p> <pre><code>def add_metadata(work_space_path, metadata):\n\"\"\"\n    Args:\n        work_space_path (str): Absolute path where the files managed by \n        ml-git will be used to generate extra information that can be\n        inserted in metadata.\n        metadata (dict): Content of spec file that can be changed to add extra data.\n    \"\"\"\n</code></pre> <code> compare_metadata </code>   This method is responsible for displaying a formatted output containing the comparison of the information that was added by the plugin in the specification file for each version of the entity. If the plugin is installed and configured correctly, this signature will be triggered during the execution of the ml-git log command.  *Definition:*  <pre><code>def compare_metadata(specs_to_compare):\n\"\"\"\n    Args:\n        specs_to_compare (Iterator[dict]): List containing current spec file and predecessors to be compared for each version.\n    \"\"\"\n</code></pre> <code> get_status_output </code>   Responsible for generating status outputs for files in the user's workspace. Returns two lists containing the formatted status output for untracked and added files and a summarized output string for the total added. This signature will be triggered during the execution of the ml-git status command.  *Definition:*  <pre><code>def get_status_output(path, untracked_files, files_to_be_commited, full_option=False):\n\"\"\"\n    Args:\n        path (str): The path where the data is in the user workspace.\n        files_to_be_commited (list): The list of files to be commited in the user workspace.\n        untracked_files (list): The list of untracked files in the user workspace.\n        full_option (bool): Option to show the entire files or summarized by path.\n\n    Returns:\n        output_untracked_data (list): List of strings formatted with the number of rows for each untracked file.\n        output_to_be_commited_data (list): List of strings formatted with the number of rows for each added file to be commited.\n        output_total_rows (str): String formatted with the sum of the rows for each file to be commited.\n\n    \"\"\"\n</code></pre> <p>Note: The plugin doesn't need to implement all methods defined in the plugin contract.</p>"},{"location":"plugins/#how-to-create-a-plugin","title":"How to create a plugin","text":"<p>When developing the plugin we recommend that the user follow the structure proposed below: <pre><code>ml-git-plugin-project-name/\n    tests/\n        core_tests.py\n    package_name/ &lt;-- name of your main package.\n        __init__.py\n        core.py &lt;-- main module where the entry function is located.\n    setup.py\n</code></pre></p> <p>In <code>package_name/core.py</code> it is expected to contain only the contract methods essential to the operation of the plugin.</p> <pre><code># package_name/core.py\n\ndef add_metadata(work_space_path, metadata):\n    ...\n    ...\n</code></pre> <p>In <code>package_name/__init__.py</code> it's necessary to import the implemented contract's methods. As in the following example:</p> <pre><code># package_name/__init__.py\n\nfrom package_name.core import add_metadata\n</code></pre> <p>The main purpose of the setup script is to describe your module distribution.</p> <pre><code># setup.py\n\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='ml-git-plugin-project-name',\n    version='0.1',\n    license='GNU General Public License v2.0',\n    author='',\n    description='',\n    packages=find_packages(),\n    platforms='Any',\n    zip_safe=True,\n)\n</code></pre>"},{"location":"plugins/#how-to-install-a-plugin","title":"How to install a plugin","text":"<pre><code>cd plugin-project-name\npip3 install --user .\n</code></pre> <p>For an entity of your preference, change the spec file like below: </p> <p>(ex: dataset/dataset-ex/dataset-ex.spec) <pre><code>dataset:\ncategories:\n- computer-vision\n- images\nmanifest:\ndata-plugin: package_name &lt;-- type here the package name in your plugin project.\nfiles: MANIFEST.yaml\nstorage: s3h://mlgit\nmutability: strict\nname: dataset-ex\nversion: 1\n</code></pre></p>"},{"location":"qs_checkout/","title":"Qs checkout","text":""},{"location":"qs_checkout/#downloading-a-dataset-from-a-configured-repository","title":"Downloading a dataset from a configured repository","text":"<p>To download a dataset, you need to be in an initialized and configured ML-Git project. If you have a repository with your saved settings, you can run the following command to set up your environment:</p> <pre><code>ml-git clone git@github.com:example/your-mlgit-repository.git\n</code></pre> <p>Then, go to the project folder: <pre><code>cd your-mlgit-repository\n</code></pre></p> <p>Now you can retrieve a specific version of a dataset to run an experiment. To achieve that, you can use the version tag to download this version to your local environment using one of the following commands:</p> <pre><code>ml-git datasets checkout computer-vision__images__faces__fddb__1\n</code></pre> <p>or </p> <pre><code>ml-git datasets checkout fddb --version=1\n</code></pre> <p>If you want to get the latest available version of a dataset, you can pass its name in the checkout command, as shown below:</p> <pre><code>ml-git datasets checkout fddb\n</code></pre> <p>Then, your directory should look like this:</p> <pre><code>computer-vision/\n\u2514\u2500\u2500 images\n    \u2514\u2500\u2500 faces\n        \u2514\u2500\u2500 fddb\n            \u251c\u2500\u2500 README.md\n            \u251c\u2500\u2500 data\n            \u2502   \u251c\u2500\u2500 2002\n            \u2502   \u2514\u2500\u2500 2003\n            \u2514\u2500\u2500 fddb.spec\n</code></pre>"},{"location":"qs_configure_repository/","title":"Qs configure repository","text":""},{"location":"qs_configure_repository/#creating-a-configured-repository","title":"Creating a configured repository","text":"<p>It's recommended to version, in a git repository, the .ml-git folder containing the settings you frequently use. This way, you will be able to use it in future projects or share it with another ML-Git user if you want.</p> <p>To create the .ml-git folder that will be versioned, the following commands are necessary:</p> <ol> <li> <p>Initialize the ML-Git project.     <pre><code>ml-git repository init\n</code></pre></p> </li> <li> <p>Configure remotes for the entities that will be used.     <pre><code>ml-git datasets remote add git@github.com:example/your-mlgit-datasets.git\n</code></pre></p> </li> <li> <p>Configure the storages which will be used.     <pre><code>ml-git repository storage add mlgit-datasets --credentials=mlgit --endpoint-url=&lt;minio-endpoint-url&gt;\n</code></pre></p> </li> </ol> <p>After that, you should version, in a git repository, the .ml-git folder created during this process.</p> <p>To use these settings in a new project, all you have to do is execute the command <code>ml-git clone</code> to import the project's settings.</p> <p>NOTE: If you would like to share these settings with another ML-Git user, this user must have access to the git repository where the settings are stored.</p>"},{"location":"quick_start/","title":"Quick start","text":"<p>In this document we describe all steps necessary to execute the following basic tasks with ML-Git:</p> <ol> <li>Downloading a dataset from a configured repository<ul> <li>Having a repository that stores the settings used by ML-Git, learn how to download a dataset.</li> </ul> </li> <li>Creating a configured repository<ul> <li>Learn how to create a repository that stores the settings used by ML-Git.</li> </ul> </li> </ol>"},{"location":"resources_initialization/","title":"Resource initialization script","text":""},{"location":"resources_initialization/#about","title":"About","text":"<p>As mentioned in ML-Git internals, the design concept about ML-Git is to decouple the ML entities metadata management from the actual data, such that there are two main layers in the tool:</p> <ol> <li>The metadata management: There are for each ML entities managed under ml-git, the user needs to define a small specification file. These files are then managed by a git repository to retrieve the different versions.</li> <li>The data store management: To store data from managed artifacts.</li> </ol> <p>This script aims to facilitate the creation of resources (buckets and repositories) that are needed to use ML-Git.</p>"},{"location":"resources_initialization/#prerequisites","title":"Prerequisites","text":"<p>To use this script, you must have configured it in your environment:</p> <ul> <li> <p>Github Access Token: You must create a personal access token to use instead of a password with a command line or with an API.   See github documentation to learn how to configure a token.</p> <p><code>Note:</code> As this script uses the github API, it is necessary that you store the token in <code>GITHUB_TOKEN</code> environment variable.</p> </li> </ul> <p>If you are setting up a bucket of S3 type:</p> <ul> <li>AWS CLI: The AWS Command Line Interface (CLI) is a unified tool for managing your AWS services.</li> </ul> <p>If you are setting up a bucket of azure type:</p> <ul> <li>Azure CLI: The Azure command-line interface (Azure CLI) is a set of commands used to create and manage Azure resources.</li> </ul> <p>If you are setting up a bucket of MinIO type:</p> <ul> <li>MinIO: In addition to having MinIO configured and running, you will also need the AWS Command Line Interface (CLI) to perform with it.</li> </ul>"},{"location":"resources_initialization/#how-to-use","title":"How to use","text":"<p>Once all the necessary requirements for the settings you want to make are installed, just run the command:</p> <p>Linux:</p> <p>Execute on terminal:</p> <pre><code>cd ml-git\n./scripts/resources_initialization/resources_initialization.sh\n</code></pre> <p>Windows:</p> <p>Execute on Powershell or CMD:</p> <pre><code>cd ml-git\n.\\scripts\\resources_initialization\\resources_initialization.bat\n</code></pre> <p>At the end of the script execution, the user must have configured the repositories to store the metadata,  a repository available to perform the <code>ml-git clone</code> command and import these settings,  in addition to having instantiated the buckets in the chosen services.</p>"},{"location":"sftp_configurations/","title":"SFTP bucket configuration","text":"<p>This section explains how to configure the settings that the ml-git uses to interact with your bucket using the SFTP storage. This requires you to have configured a public key in your SFTP server and use the private key pair to connect through ml-git.</p>"},{"location":"sftp_configurations/#setting-up-a-ml-git-project-with-sftp","title":"Setting up a ML-Git project with SFTP","text":"<p>Add store configurations example:</p> <pre><code>ml-git repository storage add path-in-your-sftp-server --type=sftph --username=your-user-name --endpoint-url=your-host --private-key=/home/profile/your_private_key\n</code></pre> <p>After that initialize the metadata repository:</p> <pre><code>ml-git datasets init\n</code></pre>"},{"location":"shell_completion_guide/","title":"ML-Git Shell Completion Support","text":"<p>The Shell completion is a function that allows you to autocomplete your ml-git commands by partially typing the commands or options, then pressing the <code>[Tab]</code> key. This will help you when writing the command in the terminal.</p> <p>The shell completion support will complete commands and options. Options are only listed if at least a dash has been entered.</p> <p>In order to activate shell completion, you need to inform your shell that completion is available for the ML-Git. For this purpose, we provide the necessary modifications in the script for each type of terminal that is supported by the autocomplete functionality: - Bash - Fish - Windows PowerShell - Zsh</p> <p>Note: If you have the shell open before making the modification, you will need to restart it after modifying the script.</p>"},{"location":"shell_completion_guide/#for-bash-add-this-to-bashrc","title":"For Bash, add this to ~/.bashrc:","text":"<pre><code>if command -v ml-git &amp;&gt; /dev/null\nthen\n   eval \"$(_ML_GIT_COMPLETE=source_bash ml-git)\"\nfi\n</code></pre>"},{"location":"shell_completion_guide/#for-fish-create-the-file-configfishcompletionsml-gitfish-and-add","title":"For Fish, create the file ~/.config/fish/completions/ml-git.fish and add:","text":"<pre><code>complete --command ml-git --arguments \"(env _ML_GIT_COMPLETE=complete-fish COMMANDLINE=(commandline -cp) ml-git)\" -f\n</code></pre>"},{"location":"shell_completion_guide/#for-windows-powershell-add-this-to-powershell-profile-file","title":"For Windows PowerShell, add this to PowerShell Profile file*:","text":"<pre><code>if ((Test-Path Function:\\TabExpansion) -and -not (Test-Path Function:\\ml-gitTabExpansionBackup)) {\n    Rename-Item Function:\\TabExpansion ml-gitTabExpansionBackup\n}\n\nfunction TabExpansion($line, $lastWord) {\n    $lastBlock = [regex]::Split($line, '[|;]')[-1].TrimStart()\n    $aliases = @(\"ml-git\") + @(Get-Alias | where { $_.Definition -eq \"ml-git\" } | select -Exp Name)\n    $aliasPattern = \"($($aliases -join '|'))\"\n    if($lastBlock -match \"^$aliasPattern \") {\n        $Env:_ML_GIT_COMPLETE = \"complete-powershell\"\n        $Env:COMMANDLINE = \"$lastBlock\"\n        (ml-git) | ? {$_.trim() -ne \"\" }\n        Remove-Item Env:_ML_GIT_COMPLETE\n        Remove-Item Env:COMMANDLINE\n    }\n    elseif (Test-Path Function:\\ml-gitTabExpansionBackup) {\n        # Fall back on existing tab expansion\n        ml-gitTabExpansionBackup $line $lastWord\n    }\n}\n</code></pre> <p>*To find out where the file for your PowerShell Profile is located, you can run <code>$profile</code> in Windows Powershell. If you don't have such a file yet, follow the steps described in this link (How to create a profile) to create a new one.</p>"},{"location":"shell_completion_guide/#for-zsh-add-this-to-zshrc","title":"For Zsh, add this to ~/.zshrc:","text":"<pre><code>#compdef ml-git\n_ml-git() {\n  eval $(env COMMANDLINE=\"${words[1,$CURRENT]}\" _ML_GIT_COMPLETE=complete-zsh  ml-git)\n}\nif [[ \"$(basename -- ${(%):-%x})\" != \"_ml-git\" ]]; then  \n  compdef _ml-git ml-git\nfi\n</code></pre>"},{"location":"step_by_step_guides/","title":"ML-Git Step-by-Step Guide","text":""},{"location":"step_by_step_guides/#about","title":"About","text":"<p>In order to facilitate the learning process of using the ML-Git API, we offer a series of step-by-step guides that emulate real situations in the learning context by applying the correct API command sequences that should be used in each scenario.</p> <p>The guides were created using Jupyter notebooks, as they offer the possibility to run, step-by-step, code snippets in a format similar to an explanatory document, thus becoming ideal for teaching new users how to use the API in real case scenarios.</p>"},{"location":"step_by_step_guides/#how-execute-notebooks","title":"How execute notebooks:","text":"<ol> <li>To run notebooks more easily, a docker environment has been created that performs all the environment settings required by the user. So make sure that the procedures in the local environment configuration section have been performed.</li> </ol>"},{"location":"step_by_step_guides/#summary-of-existing-notebooks","title":"Summary of existing notebooks:","text":"<ul> <li> <p>basic_flow</p> <ul> <li>This notebook describes a basic execution flow of ML-Git with its API. </li> <li>GitHub link</li> </ul> </li> <li> <p>clone_repository</p> <ul> <li>This notebook describes how to clone an ML-Git repository.</li> <li>GitHub link</li> </ul> </li> <li> <p>multiple_projects</p> <ul> <li>This notebook describes how to work with multiple projects in the ML-Git API.</li> <li>GitHub link</li> </ul> </li> <li> <p>relationship_api_commands</p> <ul> <li>This notebook describes a basic flow in the context of relationships between entities with the API provided by ML-Git.</li> <li>GitHub link</li> </ul> </li> <li> <p>mnist_random_forest_api</p> <ul> <li>This notebook describes a basic execution flow of ML-Git with its API using the MNIST dataset.</li> <li>GitHub link</li> </ul> </li> <li> <p>mnist_random_forest_cli</p> <ul> <li>This notebook describes a basic execution flow of ML-Git with its CLI using the MNIST dataset.</li> <li>GitHub link</li> </ul> </li> <li> <p>checkout_with_sample</p> <ul> <li>This notebook describes how to perform a checkout operation with ML-Git using samples of a dataset.</li> <li>GitHub link</li> </ul> </li> <li> <p>multiple_datasets</p> <ul> <li>This notebook describes how to handle the scenario where the same file is present in more than one dataset.</li> <li>GitHub link</li> </ul> </li> </ul>"},{"location":"storage_configurations/","title":"Storage configurations","text":"<p>Currently, ML-Git supports five types of storage (S3, MinIO, Azure, GoogleDrive and SFTP). You can find files describing how to configure each of these types of storage below:</p> <ol> <li>MinIO</li> <li>S3</li> <li>Azure</li> <li>Google Drive</li> <li>SFTP</li> </ol>"},{"location":"api/","title":"ML-Git API","text":"<p>The ML-Git API offers the developer the possibility to work with ML-Git programmatically by using the MLGitAPI class.</p>"},{"location":"api/#methods-available-in-the-mlgitapi-class","title":"Methods available in the MLGitAPI class","text":"<code> add </code> <pre><code>def add(self, entity_type, entity_name, bumpversion=False, fsck=False, file_path=None, metric=None, metrics_file=''):\n\"\"\"This command will add all the files under the directory into the ml-git index/staging area.\n\n    Example:\n        api = MLGitApi()\n        api.add('datasets', 'dataset-ex', bumpversion=True)\n\n    Args:\n        entity_type (str): The type of an ML entity (datasets, labels or models).\n        entity_name (str): The name of the ML entity you want to add the files.\n        bumpversion (bool, optional): Increment the entity version number when adding more files [default: False].\n        fsck (bool, optional): Run fsck after command execution [default: False].\n        file_path (list, optional): List of files that must be added by the command [default: all files].\n        metric (dictionary, optional): The metric dictionary, example: { 'metric': value } [default: empty].\n        metrics_file (str, optional): The metrics file path. It is expected a CSV file containing the metric names in the header and\n         the values in the next line [default: empty].\n    \"\"\"\n</code></pre> <code> checkout </code> <pre><code>def checkout(self, entity, tag, sampling=None, retries=2, force=False, dataset=False, labels=False, version=-1,\n             fail_limit=None, full=False):\n\"\"\"This command allows retrieving the data of a specific version of an ML entity.\n\n    Example:\n        api = MLGitApi()\n        api.checkout('datasets', 'computer-vision__images3__imagenet__1')\n\n    Args:\n        entity (str): The type of an ML entity (datasets, labels or models).\n        tag (str): An ml-git tag to identify a specific version of an ML entity.\n        sampling (dict): group: &lt;amount&gt;:&lt;group&gt; The group sample option consists of amount and group used to\n                                 download a sample.\\n\n                         range: &lt;start:stop:step&gt; The range sample option consists of start, stop and step used\n                                to download a sample. The start parameter can be equal or greater than zero. The\n                                stop parameter can be 'all', -1 or any integer above zero.\\n\n                         random: &lt;amount:frequency&gt; The random sample option consists of amount and frequency\n                                used to download a sample.\n                         seed: The seed is used to initialize the pseudorandom numbers.\n        retries (int, optional): Number of retries to download the files from the storage [default: 2].\n        force (bool, optional): Force checkout command to delete untracked/uncommitted files from the local repository [default: False].\n        dataset (bool, optional): If exist a dataset related with the model or labels, this one must be downloaded [default: False].\n        labels (bool, optional): If exist labels related with the model, they must be downloaded [default: False].\n        version (int, optional): The entity version [default: -1].\n        fail_limit (int, optional): Number of failures before aborting the command [default: no limit].\n        full (bool, optional): Show all contents for each directory. [default: False].\n\n    Returns:\n        str: Return the path where the data was checked out.\n    \"\"\"\n</code></pre> <code> clone </code> <pre><code>def clone(self, repository_url, untracked=False):\n\"\"\"This command will clone minimal configuration files from repository-url with valid .ml-git/config.yaml,\n    then initialize the metadata according to configurations.\n\n    Example:\n        api = MLGitApi()\n        api.clone('https://git@github.com/mlgit-repository')\n\n    Args:\n        repository_url (str): The git repository that will be cloned.\n        untracked (bool, optional): Set whether cloned repository trace should not be kept [default: False].\n    \"\"\"\n</code></pre> <code> commit </code> <pre><code>def commit(self, entity, ml_entity_name, commit_message=None, related_dataset=None, related_labels=None):\n\"\"\"That command commits the index / staging area to the local repository.\n\n    Example:\n        api = MLGitApi()\n        api.commit('datasets', 'dataset-ex')\n\n    Args:\n        entity (str): The type of an ML entity (datasets, labels or models).\n        ml_entity_name (str): Artefact name to commit.\n        commit_message (str, optional): Message of commit.\n        related_dataset (str, optional): Artefact name of dataset related to commit.\n        related_labels (str, optional): Artefact name of labels related to commit.\n    \"\"\"\n</code></pre> <code> create </code> <pre><code>def create(self, entity, entity_name, categories, mutability, **kwargs):\n\"\"\"This command will create the workspace structure with data and spec file for an entity and set the storage configurations.\n\n    Example:\n        api = MLGitApi()\\n\n        api.create('datasets', 'dataset-ex', categories=['computer-vision', 'images'], mutability='strict')\n\n    Args:\n        entity (str): The type of an ML entity (datasets, labels or models).\n        entity_name (str): An ml-git entity name to identify a ML entity.\n        categories (list): Artifact's categories name.\n        mutability (str): Mutability type. The mutability options are strict, flexible and mutable.\n        storage_type (str, optional): Data storage type [default: s3h].\n        version (int, optional): Number of artifact version [default: 1].\n        import_path (str, optional): Path to be imported to the project.\n        bucket_name (str, optional): Bucket name.\n        import_url (str, optional): Import data from a google drive url.\n        credentials_path (str, optional): Directory of credentials.json.\n        unzip (bool, optional): Unzip imported zipped files [default: False].\n        entity_dir (str, optional): The relative path where the entity will be created inside the ml entity directory [default: empty].\n    \"\"\"\n</code></pre> <code> init </code> <pre><code>def init(self, entity):\n\"\"\"This command will start the ml-git entity.\n\n    Examples:\n        api = MLGitApi()\\n\n        api.init('repository')\\n\n        api.init('datasets')\n\n    Args:\n        entity (str): The type of an ML entity (datasets, labels or models).\n    \"\"\"\n</code></pre> <code> models metrics </code> <pre><code>def get_models_metrics(self, entity_name, export_path=None, export_type=FileType.JSON.value):\n\"\"\"Get metrics information for each tag of the entity.\n\n    Examples:\n        api = MLGitApi()\\n\n        api.get_models_metrics('model-ex', export_type='csv')\n\n    Args:\n        entity_name (str): An ml-git entity name to identify a ML entity.\n        export_path(str, optional): Set the path to export metrics to a file.\n        export_type (str, optional): Choose the format of the file that will be generated with the metrics [default: json].\n    \"\"\"\n</code></pre> <code> push </code> <pre><code>def push(self, entity, entity_name,  retries=2, clear_on_fail=False, fail_limit=None):\n\"\"\"This command allows pushing the data of a specific version of an ML entity.\n\n    Example:\n        api = MLGitApi()\\n\n        api.push('datasets', 'dataset-ex')\n\n    Args:\n        entity (str): The type of an ML entity. (datasets, labels or models)\n        entity_name (str): An ml-git entity name to identify a ML entity.\n        retries (int, optional): Number of retries to upload the files to the storage [default: 2].\n        clear_on_fail (bool, optional): Remove the files from the storage in case of failure during the push operation [default: False].\n        fail_limit (int, optional): Number of failures before aborting the command [default: no limit].\n    \"\"\"\n</code></pre> <code> remote add </code> <pre><code>def remote_add(self, entity, remote_url, global_configuration=False):\n\"\"\"This command will add a remote to store the metadata from this ml-git project.\n\n    Examples:\n        api = MLGitApi()\\n\n        api.remote_add('datasets', 'https://git@github.com/mlgit-datasets')\n\n    Args:\n        entity (str): The type of an ML entity (datasets, labels or models).\n        remote_url(str): URL of an existing remote git repository.\n        global_configuration (bool, optional): Use this option to set configuration at global level [default: False].\n    \"\"\"\n</code></pre> <code> storage add </code> <pre><code>def storage_add(self, bucket_name, bucket_type=StorageType.S3H.value, credentials=None, global_configuration=False,\n                endpoint_url=None, username=None, private_key=None, port=22, region=None):\n\"\"\"This command will add a storage to the ml-git project.\n\n    Examples:\n        api = MLGitApi()\\n\n        api.storage_add('my-bucket', bucket_type='s3h')\n\n    Args:\n        bucket_name (str): The name of the bucket in the storage.\n        bucket_type (str, optional): Storage type (s3h, azureblobh or gdriveh) [default: s3h].\n        credentials (str, optional): Name of the profile that stores the credentials or the path to the credentials.\n        global_configuration (bool, optional): Use this option to set configuration at global level [default: False].\n        endpoint_url (str, optional): Storage endpoint url.\n        username (str, optional): The username for the sftp login.\n        private_key (str, optional): Full path for the private key file.\n        port (int, optional): The port to be used when connecting to the storage.\n        region (str, optional): AWS region for S3 bucket.\n    \"\"\"\n</code></pre> <code> init entity manager </code> <pre><code>def init_entity_manager(github_token, url):\n\"\"\"Initialize an entity manager to operate over github API.\n\n        Examples:\n            init_entity_manager('github_token', 'https://api.github.com')\n\n        Args:\n            github_token (str): The personal access github token.\n            url (str): The github api url.\n\n        Returns:\n            object of class EntityManager.\n\n    \"\"\"\n</code></pre> <code> init local entity manager </code> <pre><code>def init_local_entity_manager():\n\"\"\"Initialize an entity manager to operate over local git repository.\n\n        Returns:\n            object of class LocalEntityManager.\n\n    \"\"\"\n</code></pre>"},{"location":"api/#classes-used-by-the-api","title":"Classes used by the API.","text":"<p>Some methods available in the API use the classes described below:</p> <code> EntityManager </code> <p></p> <pre><code>class EntityManager:\n\"\"\"Class that operate over github api to manage entity's operations\"\"\"\n    def get_entities(self, config_path=None, config_repo_name=None):\n\"\"\"Get a list of entities found in config.yaml.\n\n        Args:\n            config_path (str): The absolute path of the config.yaml file.\n            config_repo_name (str): The repository name where is the config.yaml located in github.\n\n        Returns:\n            list of class Entity.\n        \"\"\"\n    def get_entity_versions(self, name, metadata_repo_name):\n\"\"\"Get a list of spec versions found for an especific entity.\n\n        Args:\n            name (str): The name of the entity you want to get the versions.\n            metadata_repo_name (str): The repository name where the entity metadata is located in GitHub.\n\n        Returns:\n            list of class SpecVersion.\n        \"\"\"\n    def get_linked_entities(self, name, version, metadata_repo_name):\n\"\"\"Get a list of linked entities found for an entity version.\n\n        Args:\n            name (str): The name of the entity you want to get the linked entities.\n            version (str): The version of the entity you want to get the linked entities.\n            metadata_repo_name (str): The repository name where the entity metadata is located in GitHub.\n\n        Returns:\n            list of LinkedEntity.\n        \"\"\"\n    def get_entity_relationships(self, name, metadata_repo_name, export_type=FileType.JSON.value, export_path=None):\n\"\"\"Get a list of relationships for an entity.\n\n        Args:\n            name (str): The name of the entity you want to get the linked entities.\n            metadata_repo_name (str): The repository name where the entity metadata is located in GitHub.\n            export_type (str): Set the format of the return (json, csv, dot) [default: json].\n            export_path (str): Set the path to export metrics to a file.\n\n        Returns:\n            list of EntityVersionRelationships.\n        \"\"\"\n    def get_project_entities_relationships(self, config_repo_name, export_type=FileType.JSON.value, export_path=None):\n\"\"\"Get a list of relationships for all project entities.\n\n        Args:\n            config_repo_name (str): The repository name where the config file is located in GitHub.\n            export_type (str): Set the format of the return (json, csv, dot) [default: json].\n            export_path (str): Set the path to export metrics to a file.\n\n        Returns:\n            list of EntityVersionRelationships.\n        \"\"\"\n</code></pre> <code> LocalEntityManager </code> <p></p> <pre><code>class LocalEntityManager:\n\"\"\"Class that operate over local git repository to manage entity's operations\"\"\"\n\n    def get_entities(self):\n\"\"\"Get a list of entities found in config.yaml.\n\n        Returns:\n            list of class Entity.\n        \"\"\"\n\n    def get_entity_versions(self, name, type_entity):\n\"\"\"Get a list of spec versions found for an especific entity.\n\n        Args:\n            name (str): The name of the entity you want to get the versions.\n            type_entity (str): The type of the ml-entity (datasets, models, labels).\n\n        Returns:\n            list of class SpecVersion.\n        \"\"\"\n\n    def get_linked_entities(self, name, version, type_entity):\n\"\"\"Get a list of linked entities found for an entity version.\n\n        Args:\n            name (str): The name of the entity you want to get the linked entities.\n            version (str): The version of the entity you want to get the linked entities.\n            type_entity (str): The type of the ml-entity (datasets, models, labels).\n\n        Returns:\n            list of LinkedEntity.\n        \"\"\"\n\n    def get_entity_relationships(self, name, type_entity, export_type=FileType.JSON.value, export_path=None):\n\"\"\"Get a list of relationships for an entity.\n\n        Args:\n            name (str): The name of the entity you want to get the linked entities.\n            type_entity (str): The type of the ml-entity (datasets, models, labels).\n            export_type (str): Set the format of the return (json, csv, dot) [default: json].\n            export_path (str): Set the path to export metrics to a file.\n\n        Returns:\n            list of EntityVersionRelationships.\n        \"\"\"\n\n    def get_project_entities_relationships(self, export_type=FileType.JSON.value, export_path=None):\n\"\"\"Get a list of relationships for all project entities.\n\n        Args:\n            export_type (str): Set the format of the return [default: json].\n            export_path (str): Set the path to export metrics to a file.\n\n        Returns:\n            list of EntityVersionRelationships.\n        \"\"\"\n\n    def export_graph(self, dot_graph, export_path):\n\"\"\"Creates a graph of all entity relations as an HTML file.\n\n         Args:\n             dot_graph (str): String of graph in DOT language format.\n             export_path (str): Set the path to export the HTML with the graph. [default: project root path]\n\n         Returns:\n             Path of HTML file.\n         \"\"\"\n</code></pre> <code> Entity </code> <p></p> <pre><code>class Entity:\n\"\"\"Class that represents an ml-entity.\n\n    Attributes:\n        name (str): The name of the entity.\n        type (str): The type of the ml-entity (datasets, models, labels).\n        private (str): The access of entity metadata.\n        metadata (Metadata): The metadata of the entity.\n        last_spec_version (SpecVersion): The specification file of the entity last version.\n    \"\"\"\n</code></pre> <code> SpecVersion </code> <p></p> <pre><code>class SpecVersion:\n\"\"\"Class that represents an ml-entity spec version.\n\n    Attributes:\n        name (str): The name of the entity.\n        type (str): The type of the ml-entity (datasets, models, labels).\n        version (str): The version of the ml-entity.\n        tag (str): The tag of the ml-entity spec version.\n        mutability (str): The mutability of the ml-entity.\n        categories (list): Labels to categorize the entity.\n        storage (Storage): The storage of the ml-entity.\n        total_versioned_files (int): The amount of versioned files.\n        size (str): The size of the version files.\n    \"\"\"\n</code></pre> <code> Metadata </code> <p></p> <pre><code>class Metadata:\n\"\"\"Class that represents an ml-entity metadata.\n    Attributes:\n        full_name (str): The full name of the metadata.\n        git_url (str): The git url of the metadata.\n        html_url (str): The html url of the metadata.\n        owner_email (str): The owner email of the ml-entity metadata.\n        owner_name (str): The owner name of the ml-entity metadata.\n    \"\"\"\n</code></pre> <code> Storage </code> <p></p> <pre><code>class Storage:\n\"\"\"Class that represents an ml-entity storage.\n    Attributes:\n        type (str): The storage type (s3h|azureblobh|gdriveh|sftph).\n        bucket (str): The name of the bucket.\n    \"\"\"\n</code></pre> <code> EntityVersionRelationships </code> <p></p> <pre><code>class EntityVersionRelationships:\n\"\"\"Class that represents the relationships of an ml-entity in a specified version.\n\n    Attributes:\n        version (str): The version of the ml-entity.\n        tag (str): The tag of the ml-entity.\n        relationships (list): List of linked entities of the ml-entity in the specified version.\n    \"\"\"\n</code></pre> <code> LinkedEntity </code> <p></p> <pre><code>class LinkedEntity:\n\"\"\"Class that represents a linked ml-entity.\n\n    Attributes:\n        name (str): The name of the entity.\n        type (str): The type of the ml-entity (datasets, models, labels).\n        version (str): The version of the ml-entity.\n        tag (str): The tag of the ml-entity spec version.\n    \"\"\"\n</code></pre>"},{"location":"api/#api-notebooks","title":"API notebooks","text":"<p>In the api_scripts directory, you can find notebooks running the ML-Git API for some scenarios.  To run them, you need to boot the jupyter notebook server in an environment with ML-Git installed and navigate to the notebook of your choice.</p>"},{"location":"api/quick_start/","title":"ML-Git API","text":""},{"location":"api/quick_start/#quick-start","title":"Quick start","text":"<p>To use the ML-Git API, it's necessary to have ML-Git installed in the environment that will be executed. When instantiating the MLGitAPI class, it's required to either inform an existing directory in the root_path parameter or not pass any value at all. </p>"},{"location":"api/quick_start/#instantiating-the-api","title":"Instantiating the API","text":"<p>You can inform the root directory of your ML-Git Project by passing an absolute path or a relative path to the root_path parameter.</p> <p><pre><code>from ml_git.api import MLGitAPI\n\napi = MLGitAPI(root_path='/absolute/path/to/your/project')\n# or\napi = MLGitAPI(root_path='./relative/path/to/your/project')\n</code></pre> <code>Note:</code> The root_path parameter can receive any string accepted by the pathlib.Path class.</p> <p>You can also work with your current working directory (CWD) by not passing any value.</p> <pre><code>from ml_git.api import MLGitAPI\n\napi = MLGitAPI()\n</code></pre>"},{"location":"api/quick_start/#multiple-ml-git-projects","title":"Multiple ML-Git Projects","text":"<p>It's also possible to work with multiple projects in the same python script by instantiating the MLGitAPI class for each project.</p> <pre><code>from ml_git.api import MLGitAPI\n\napi_project_1 = MLGitAPI(root_path='/path/to/project_1')\napi_project_2 = MLGitAPI(root_path='/path/to/project_2')\n</code></pre> <p>Each instance will run its commands in the context of its project. It's important to note that these instances are not aware of each other nor follow the singleton pattern, so it's possible to have multiple instances pointing to the same directory, so if you end up in this situation, be careful not to run commands that can be conflicting, like trying to create the same entity in more than one instance.</p>"},{"location":"api/quick_start/#ml-git-repository","title":"ML-Git Repository","text":"<p>To use most of the commands available in the API, you need to be working with a directory containing a valid ML-Git Project. For that, you can clone a repository by using the clone command, or you can start a new repository with the init command.</p>"},{"location":"api/quick_start/#clone","title":"Clone","text":"<pre><code>repository_url = 'https://git@github.com/mlgit-repository'\n\napi.clone(repository_url)\n</code></pre> <p>output:</p> <pre><code>INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-repository] @ [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata]\nINFO - Metadata: Successfully loaded configuration files!\n</code></pre>"},{"location":"api/quick_start/#init","title":"Init","text":"<pre><code>api.init('repository')\n</code></pre> <p>output:</p> <pre><code>INFO - Admin: Initialized empty ml-git repository in /home/user/Documentos/project/.ml-git\n</code></pre> <p><code>Note:</code> To use these commands, the instance must be pointing to an empty (or not previously initialized) directory.</p>"},{"location":"api/quick_start/#checkout","title":"Checkout","text":""},{"location":"api/quick_start/#checkout-dataset","title":"Checkout dataset","text":"<pre><code>entity = 'datasets'\ntag = 'computer-vision__images__imagenet__1'\n\ndata_path = api.checkout(entity, tag)\n</code></pre> <p>output:</p> <pre><code>INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata]\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00&lt;00:00, 2.87kblobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00&lt;00:00, 2.35kchunks/s]\nfiles into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00&lt;00:00, 3.00kfiles into cache/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00&lt;00:00, 1.72kfiles into workspace/s]\n</code></pre>"},{"location":"api/quick_start/#checkout-labels-with-dataset","title":"Checkout labels with dataset","text":"<p><pre><code>entity = 'labels'\ntag = 'computer-vision__images__mscoco__2'\n\ndata_path = api.checkout(entity, tag, dataset=True)\n</code></pre> output:</p> <pre><code>INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/labels/metadata]\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00&lt;00:00, 205blobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00&lt;00:00, 173chunks/s]\nfiles into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00&lt;00:00, 788files into cache/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.00/6.00 [00:00&lt;00:00, 1.28kfiles into workspace/s]\nINFO - Repository: Initializing related dataset download\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00&lt;00:00, 3.27kblobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00&lt;00:00, 2.37kchunks/s]\nfiles into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00&lt;00:00, 2.40kfiles into cache/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.00/4.00 [00:00&lt;00:00, 1.72kfiles into workspace/s]\n</code></pre>"},{"location":"api/quick_start/#checkout-dataset-with-sample","title":"Checkout dataset with sample","text":""},{"location":"api/quick_start/#group-sample","title":"Group-Sample","text":"<pre><code>entity = 'datasets'\ntag = 'computer-vision__images__imagenet__1'\n\nsampling = {'group': '1:2', 'seed': '10'}\n\ndata_path = api.checkout(entity, tag, sampling)\n</code></pre> <p>output:</p> <pre><code>INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata]\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.04kblobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.83kchunks/s]\nfiles into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.09kfiles into cache/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.16kfiles into workspace/s]\n</code></pre>"},{"location":"api/quick_start/#range-sample","title":"Range-Sample","text":"<pre><code>entity = 'datasets'\ntag = 'computer-vision__images__imagenet__1'\n\nsampling = {'range': '0:4:3'}\n\ndata_path = api.checkout(entity, tag, sampling)\n</code></pre> <p>output:</p> <pre><code>INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata]\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.71kblobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.54kchunks/s]\nfiles into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.22kfiles into cache/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.55kfiles into workspace/s]\n</code></pre>"},{"location":"api/quick_start/#random-sample","title":"Random-Sample","text":"<pre><code>entity = 'datasets'\ntag = 'computer-vision__images__imagenet__1'\n\nsampling = {'random': '1:2', 'seed': '1'}\n\ndata_path = api.checkout(entity, tag, sampling)\n</code></pre> <p>output:</p> <pre><code>INFO - Metadata Manager: Pull [/home/user/Documentos/project/.ml-git/dataset/metadata]\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.47kblobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.00kchunks/s]\nfiles into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 3.77kfiles into cache/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.17kfiles into workspace/s]\n</code></pre>"},{"location":"api/quick_start/#add","title":"Add","text":"<pre><code>api.add('datasets', 'dataset-ex')\n</code></pre> <p>output:</p> <pre><code>INFO - Metadata Manager: Pull [/home/user/Documentos/mlgit-api/mlgit/.ml-git/dataset/metadata]\nINFO - Repository: dataset adding path [[/home/user/Documentos/mlgit-api/mlgit/dataset//dataset-ex] to ml-git index\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 381files/s]\n</code></pre>"},{"location":"api/quick_start/#commit","title":"Commit","text":"<pre><code>entity = 'datasets'\nentity_name = 'dataset-ex'\nmessage = 'Commit example'\n\napi.commit(entity, entity_name, message)\n</code></pre> <p>output:</p> <pre><code>INFO - Metadata Manager: Commit repo[/home/user/Documentos/project/.ml-git/dataset/metadata] --- file[computer-vision/images/dataset-ex]\n</code></pre>"},{"location":"api/quick_start/#push","title":"Push","text":"<pre><code>entity = 'datasets'\nspec = 'dataset-ex'\n\napi.push(entity, spec)\n</code></pre> <p>output:</p> <pre><code>files: 100%|##########| 24.0/24.0 [00:00&lt;00:00, 34.3files/s]\n</code></pre>"},{"location":"api/quick_start/#create","title":"Create","text":"<pre><code>entity = 'datasets'\nspec = 'dataset-ex'\ncategories = ['computer-vision', 'images']\nmutability = 'strict'\n\napi.create(entity, spec, categories, mutability, import_path='/path/to/dataset', unzip=True, version=2)\n</code></pre> <p>output:</p> <pre><code>INFO - MLGit: Dataset artifact created.\n</code></pre>"},{"location":"api/quick_start/#init_1","title":"Init","text":"<p>The init command is used to start either an entity or, as shown before, a repository.</p>"},{"location":"api/quick_start/#repository","title":"Repository","text":"<pre><code>api.init('repository')\n</code></pre> <p>output:</p> <pre><code>INFO - Admin: Initialized empty ml-git repository in /home/user/Documentos/project/.ml-git\n</code></pre>"},{"location":"api/quick_start/#entity","title":"Entity","text":"<pre><code>entity_type = 'datasets'\n\napi.init(entity_type)\n</code></pre> <p>output:</p> <pre><code>INFO - Metadata Manager: Metadata init [https://git@github.com/mlgit-datasets] @ [/home/user/Documentos/project/.ml-git/dataset/metadata]\n</code></pre>"},{"location":"api/quick_start/#remote-add","title":"Remote add","text":"<pre><code>entity_type = 'datasets'\ndatasets_repository = 'https://git@github.com/mlgit-datasets'\n\napi.remote_add(entity_type, datasets_repository)\n</code></pre> <p>output:</p> <pre><code>INFO - Admin: Add remote repository [https://git@github.com/mlgit-datasets] for [dataset]\n</code></pre>"},{"location":"api/quick_start/#storage-add","title":"Storage add","text":"<pre><code>bucket_name = 'minio'\nbucket_type='s3h'\nendpoint_url = 'http://127.0.0.1:9000/'\n\napi.storage_add(bucket_name=bucket_name,bucket_type=bucket_type, endpoint_url=endpoint_url)\n</code></pre> <p>output:</p> <pre><code>INFO - Admin: Add storage [s3h://minio]\n</code></pre>"},{"location":"api/quick_start/#list-entities","title":"List entities","text":""},{"location":"api/quick_start/#list-entities-from-a-config-file","title":"List entities from a config file","text":"<pre><code>github_token = ''\napi_url = 'https://api.github.com'\nmanager = api.init_entity_manager(github_token, api_url)\n\nentities = manager.get_entities(config_path='path/to/config.yaml')\n</code></pre>"},{"location":"api/quick_start/#list-entities-from-a-repository","title":"List entities from a repository","text":"<pre><code>github_token = ''\napi_url = 'https://api.github.com'\nmanager = api.init_entity_manager(github_token, api_url)\n\nentities = manager.get_entities(config_repo_name='user/config_repository')\n</code></pre>"},{"location":"api/quick_start/#list-versions-from-a-entity","title":"List versions from a entity","text":"<pre><code>github_token = ''\napi_url = 'https://api.github.com'\nmanager = api.init_entity_manager(github_token, api_url)\n\nversions = manager.get_entity_versions('entity_name', metadata_repo_name='user/metadata_repository')\n</code></pre>"},{"location":"api/quick_start/#list-linked-entities-from-a-entity-version","title":"List linked entities from a entity version","text":"<pre><code>github_token = ''\napi_url = 'https://api.github.com'\nmanager = api.init_entity_manager(github_token, api_url)\n\nlinked_entities = manager.get_linked_entities('entity_name', 'entity_version', metadata_repo_name='user/metadata_repository')\n</code></pre>"},{"location":"api/quick_start/#list-relationships-from-a-entity","title":"List relationships from a entity","text":"<pre><code>github_token = ''\napi_url = 'https://api.github.com'\nmanager = api.init_entity_manager(github_token, api_url)\n\nrelationships = manager.get_entity_relationships('entity_name', metadata_repo_name='user/metadata_repository')\n</code></pre>"},{"location":"api/quick_start/#list-relationships-from-all-project-entities","title":"List relationships from all project entities","text":"<pre><code>github_token = ''\napi_url = 'https://api.github.com'\nmanager = api.init_entity_manager(github_token, api_url)\n\nrelationships = manager.get_project_entities_relationships(config_repo_name='user/config_repository')\n</code></pre>"},{"location":"api/api_scripts/","title":"ML-Git Step-by-Step Guide","text":""},{"location":"api/api_scripts/#about","title":"About","text":"<p>In order to facilitate the learning process of using the ML-Git API, we offer a series of step-by-step guides that emulate real situations in the learning context by applying the correct API command sequences that should be used in each scenario.</p> <p>The guides were created using Jupyter notebooks, as they offer the possibility to run, step-by-step, code snippets in a format similar to an explanatory document, thus becoming ideal for teaching new users how to use the API in real case scenarios.</p>"},{"location":"api/api_scripts/#how-execute-notebooks","title":"How execute notebooks:","text":"<ol> <li>To run notebooks more easily, a docker environment has been created that performs all the environment settings required by the user. So make sure that the procedures in the local environment configuration section have been performed.</li> </ol>"},{"location":"api/api_scripts/#summary-of-existing-notebooks","title":"Summary of existing notebooks:","text":"<ul> <li> <p>basic_flow</p> <ul> <li>This notebook describes a basic execution flow of ML-Git with its API. </li> <li>GitHub link</li> </ul> </li> <li> <p>clone_repository</p> <ul> <li>This notebook describes how to clone an ML-Git repository.</li> <li>GitHub link</li> </ul> </li> <li> <p>multiple_projects</p> <ul> <li>This notebook describes how to work with multiple projects in the ML-Git API.</li> <li>GitHub link</li> </ul> </li> <li> <p>relationship_api_commands</p> <ul> <li>This notebook describes a basic flow in the context of relationships between entities with the API provided by ML-Git.</li> <li>GitHub link</li> </ul> </li> <li> <p>mnist_random_forest_api</p> <ul> <li>This notebook describes a basic execution flow of ML-Git with its API using the MNIST dataset.</li> <li>GitHub link</li> </ul> </li> <li> <p>mnist_random_forest_cli</p> <ul> <li>This notebook describes a basic execution flow of ML-Git with its CLI using the MNIST dataset.</li> <li>GitHub link</li> </ul> </li> <li> <p>checkout_with_sample</p> <ul> <li>This notebook describes how to perform a checkout operation with ML-Git using samples of a dataset.</li> <li>GitHub link</li> </ul> </li> <li> <p>multiple_datasets</p> <ul> <li>This notebook describes how to handle the scenario where the same file is present in more than one dataset.</li> <li>GitHub link</li> </ul> </li> </ul>"},{"location":"api/api_scripts/basic_flow/","title":"Basic Flow","text":""},{"location":"api/api_scripts/basic_flow/#basic-flow","title":"Basic Flow","text":"<p>This notebook describes a basic execution flow of ml-git with its API. There, you will learn how to initialize an ML-Git project, how to perform all the necessary configuration steps and how to version a dataset. We will divide this quick how-to into 3 main sections:</p>"},{"location":"api/api_scripts/basic_flow/#ml-git-repository-configuationintialization","title":"ml-git repository configuation/intialization","text":"<pre><code> This section explains how to initialize and configure a repository for ml-git.\n</code></pre>"},{"location":"api/api_scripts/basic_flow/#versioning-a-dataset","title":"versioning a dataset","text":"<pre><code>Having a repository initialized, this section explains how to create and upload a dataset to the storage.\n</code></pre>"},{"location":"api/api_scripts/basic_flow/#downloading-a-dataset","title":"downloading a dataset","text":"<pre><code>This section describes how to download a versioned data set using ml-git.\n</code></pre>"},{"location":"api/api_scripts/basic_flow/#notebook-state-management","title":"Notebook state management","text":"<p>If you have already run this notebook or another notebook in this same folder, it is recommended that you execute the cell below to clean the environment, as state changes made previously may interfere with the notebook running. Be aware that this procedure does not affect any remote repositories.</p> <pre><code>!sh ./clean_environment.sh\n</code></pre>"},{"location":"api/api_scripts/basic_flow/#1-ml-git-repository-configuationintialization","title":"1 - ml-git repository configuation/intialization","text":"<p>To start using the ml-git api we need to import it into our script</p> <pre><code>from ml_git.api import MLGitAPI\n</code></pre> <p>Then we must create a new instance of the API class</p> <pre><code>api = MLGitAPI()\n</code></pre> <p>To use ml-git, it is necessary to configure storages and remotes that will be used in a project. This configuration can be done through a sequence of commands, or if you already have a git repository with the stored settings, you can run the clone command to import those settings. The following subsections demonstrate how to configure these two forms. </p> <p>Note: You should only perform one of the following two subsections.</p> <p>1.1 Configuring with clone command</p> <p>With the clone command all settings will be imported and initialized from the repository that was informed.</p> <pre><code>repository_url = '/local_ml_git_config_server.git'\n\napi.clone(repository_url)\n</code></pre> <p>After that, you can skip to section 2 which teaches you how to create a version of a dataset.</p> <p>1.2 Configuring from start</p> <p>In this section we will consider the scenario of a user who wants to configure their project from scratch. The first step is to define that the directory we are working on will be an ml-git project, for this we execute the following command:</p> <pre><code>api.init('repository')\n</code></pre> <pre><code>INFO - Admin: Initialized empty ml-git repository in /api_scripts/.ml-git\n</code></pre> <p>After initializing an ml-git project, it is necessary that you inform the remotes and storages that will be used by the entities to be versioned. If you want to better understand why ml-git uses these resources, please take a look at the architecture and internals documentation.</p> <p>In this notebook we will configure our ml-git project with a local git repository and a local minio as storage. For this, the following commands are necessary:</p> <pre><code>remote_url = '/local_server.git/'\nbucket_name= 'mlgit'\nend_point = 'http://127.0.0.1:9000'\n\n# The type of entity we are working on\nentity_type = 'datasets'\n\napi.remote_add(entity_type, remote_url)\napi.storage_add(bucket_name, endpoint_url=end_point)\n</code></pre> <pre><code>INFO - Admin: Add remote repository [/local_server.git/] for [datasets]\nINFO - Admin: Add storage [s3h://mlgit]\n</code></pre> <p>Last but not least, initialize the metadata repository</p> <pre><code>api.init(entity_type)\n</code></pre> <pre><code>INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/.ml-git/datasets/metadata]\n</code></pre>"},{"location":"api/api_scripts/basic_flow/#2-versioning-a-dataset","title":"2 - versioning a dataset","text":"<p>After the entities have been initialized and are ready for use. We can continue with the process to version our first dataset.</p> <p>ml-git expects any dataset to be specified under dataset/ directory of your project and it expects a specification file with the name of the dataset.</p> <p>To create this specification file for a new entity you must run the following command:</p> <pre><code># The entity name we are working on\nentity_name = 'dataset-ex'\n\napi.create(entity_type, entity_name, categories=['computer-vision', 'images'], mutability='strict', bucket_name=bucket_name)\n</code></pre> <pre><code>INFO - MLGit: Dataset artifact created.\n</code></pre> <p>Once we create our dataset entity we can add the data to be versioned within the entity's directory. For this, the following code generate a new file in our dataset path.</p> <pre><code>import os\n\ndef create_file(file_name='file'):\n    file_path = os.path.join('datasets', 'dataset-ex', 'data', file_name)\n    open(file_path, 'a').close()\n\ncreate_file()\n</code></pre> <p>We can now proceed with the necessary steps to send the new data to storage.</p> <pre><code>api.add(entity_type, entity_name, bumpversion=True)\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/.ml-git/datasets/metadata]\nINFO - Repository: datasets adding path [/api_scripts/datasets/dataset-ex] to ml-git index\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 360files/s]\n\n\n\u280b Creating hard links in cache\u2819 Creating hard links in cache\n\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 4.13kfiles/s]\n</code></pre> <p>After add the files, you need commit the metadata to the local repository. For this purpose type the following command:</p> <pre><code># Custom commit message\nmessage = 'Commit example'\n\napi.commit(entity_type, entity_name, message)\n</code></pre> <pre><code>\u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest\n\nINFO - Metadata Manager: Commit repo[/api_scripts/.ml-git/datasets/metadata] --- file[dataset-ex]\n</code></pre> <p>Last but not least, ml-git dataset push will update the remote metadata repository just after storing all actual data under management in the specified remote data storage.</p> <pre><code>api.push(entity_type, entity_name)\n</code></pre> <pre><code>files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 57.4files/s]\n\n\n\u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository\n</code></pre> <p>As you can observe, ml-git follows very similar workflows as for git.</p>"},{"location":"api/api_scripts/basic_flow/#3-downloading-a-dataset","title":"3 - downloading a dataset","text":"<p>Once you have an entity versioned by ml-git, and being within an initialized directory, it is really simple to obtain data from a specific entity. As an example, in this notebook we will checkout an entity that was previously versioned, the mnist. For this, the following command is necessary:</p> <pre><code>entity_name = 'mnist'\n\ndata_path = api.checkout(entity_type, entity_name, version=1)\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/.ml-git/datasets/metadata]\nINFO - Metadata: Performing checkout in tag handwritten__digits__mnist__1\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 186blobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.24chunks/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 12.6files into workspace/s]\n</code></pre> <p>Getting the data will auto-create a directory structure under dataset directory. That structure computer-vision/images is actually coming from the categories defined in the dataset spec file. Doing that way allows for easy download of many datasets in one single ml-git project without creating any conflicts.</p> <p>Now the user can perform the processes he wants with the data that was downloaded in the workspace.</p>"},{"location":"api/api_scripts/clone_repository/","title":"Clone Repository","text":""},{"location":"api/api_scripts/clone_repository/#cloning-an-ml-git-repository","title":"Cloning an ml-git repository","text":"<p>This notebook describes how to clone an ML-Git repository.</p>"},{"location":"api/api_scripts/clone_repository/#notebook-state-management","title":"Notebook state management","text":"<p>If you have already run this notebook or another notebook in this same folder, it is recommended that you execute the cell below to clean the environment, as state changes made previously may interfere with the notebook running. Be aware that this procedure does not affect any remote repositories.</p> <pre><code>!sh ./clean_environment.sh\n</code></pre>"},{"location":"api/api_scripts/clone_repository/#to-start-using-the-ml-git-api-we-need-to-import-it-into-our-script","title":"To start using the ml-git api we need to import it into our script","text":"<pre><code>from ml_git.api import MLGitAPI\n</code></pre>"},{"location":"api/api_scripts/clone_repository/#after-importing-you-can-use-the-api-clone-method-passing-the-url-of-the-git-repository-as-a-parameter","title":"After importing you can use the api clone method, passing the url of the git repository as a parameter.","text":"<pre><code>repository_url = '/local_ml_git_config_server.git'\napi = MLGitAPI()\n\napi.clone(repository_url)\n</code></pre> <pre><code>INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/tmprr5gsm40/mlgit/.ml-git/datasets/metadata]\nINFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/tmprr5gsm40/mlgit/.ml-git/models/metadata]\nINFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/tmprr5gsm40/mlgit/.ml-git/labels/metadata]\nINFO - Metadata: Successfully loaded configuration files!\n</code></pre>"},{"location":"api/api_scripts/clone_repository/#when-the-clone-is-successfully-completed-the-entities-are-initialized-and-ready-for-use","title":"When the clone is successfully completed, the entities are initialized and ready for use.","text":""},{"location":"api/api_scripts/multiple_projects/","title":"Multiple Projects","text":""},{"location":"api/api_scripts/multiple_projects/#multiple-projects","title":"Multiple Projects","text":"<p>This notebook describes how to work with multiple projects in the ML-Git API.</p>"},{"location":"api/api_scripts/multiple_projects/#notebook-state-management","title":"Notebook state management","text":"<p>If you have already run this notebook or another notebook in this same folder, it is recommended that you execute the cell below to clean the environment, as state changes made previously may interfere with the notebook running. Be aware that this procedure does not affect any remote repositories.</p> <pre><code>!sh ./clean_environment.sh\n</code></pre>"},{"location":"api/api_scripts/multiple_projects/#using-multiples-projects","title":"Using multiples projects","text":"<p>Let's start creating, for each project, a folder to work with.</p> <pre><code># importing os module\nimport os\n\nos.mkdir('project_1')\nos.mkdir('project_2')\n</code></pre> <p>To start using the ML-Git API, we need to import it into our script.</p> <pre><code>from ml_git.api import MLGitAPI\n</code></pre> <p>Then we must create a new instance of the API for each project. You can inform the root directory of the project as a parameter. In this scenario, we will be using the relative path to each directory.</p> <pre><code>project_1 = MLGitAPI(root_path='./project_1')\nproject_2 = MLGitAPI(root_path='./project_2')\n</code></pre> <p>We will consider the scenario of a user who wants to configure their projects from scratch. The first step is to define that the directory we are working on will be an ml-git project. To do that, execute the following command:</p> <pre><code>project_1.init('repository')\nproject_2.init('repository')\n</code></pre> <pre><code>INFO - Admin: Initialized empty ml-git repository in C:\\Git\\HP\\ml-git\\docs\\api\\api_scripts\\project_1\\.ml-git\nINFO - Admin: Initialized empty ml-git repository in C:\\Git\\HP\\ml-git\\docs\\api\\api_scripts\\project_2\\.ml-git\n</code></pre> <p>We will configure each project with a local git repository and a local MinIO as storage. For this, the following commands are necessary:</p> <pre><code>remote_url = '/local_server.git/'\nbucket_name= 'mlgit'\nend_point = 'http://127.0.0.1:9000'\n\n# The type of entity we are working on\nentity_type = 'datasets'\n\nproject_1.remote_add(entity_type, remote_url)\nproject_1.storage_add(bucket_name, endpoint_url=end_point)\n\nprint('/n')\n\nproject_2.remote_add(entity_type, remote_url)\nproject_2.storage_add(bucket_name, endpoint_url=end_point)\n</code></pre> <pre><code>INFO - Admin: Add remote repository [/local_server.git/] for [datasets]\nINFO - Repository: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help\nINFO - Admin: Add storage [s3h://mlgit]\nINFO - Admin: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help\n\n\n/n\n\n\nINFO - Admin: Add remote repository [/local_server.git/] for [datasets]\nINFO - Repository: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help\nINFO - Admin: Add storage [s3h://mlgit]\nINFO - Admin: When making changes to the config file we strongly recommend that you upload these changes to the Git repository. For this, see: ml-git repository config push --help\n</code></pre> <p>After the projects have been initialized we can continue with the process to create new datasets. To create the specification file for new entities you must run the following commands:</p> <pre><code># The entity name we are working on project 1\nentity_name_1 = 'dataset-ex-1'\n# The entity name we are working on project 2\nentity_name_2 = 'dataset-ex-2'\n\nproject_1.create(entity_type, entity_name_1, categories=['img'], mutability='strict')\nproject_2.create(entity_type, entity_name_2, categories=['img'], mutability='strict')\n</code></pre> <pre><code>INFO - MLGit: Dataset artifact created.\nINFO - MLGit: Dataset artifact created.\n</code></pre> <p>In this example, we demonstrated how to work with multiple projects in the ML-Git API. You can use all commands available in the API with this concept of multiple projects, a complete flow of how to version an entity can be found in the Basic Flow Notebook.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/","title":"ML-Git","text":"<p>This notebook describes a basic execution flow of ml-git with its API. In it, we show how to obtain a dataset already versioned by ml-git, how to perform the versioning process of a model and the new data generated, using the MNIST dataset.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#notebook-state-management","title":"Notebook state management","text":"<p>If you have already run this notebook or another notebook in this same folder, it is recommended that you execute the cell below to clean the environment, as state changes made previously may interfere with the notebook running. Be aware that this procedure does not affect any remote repositories.</p> <pre><code>!sh ./clean_environment.sh\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#1-the-dataset","title":"1 - The dataset","text":"<p>Dataset MNIST is a set of small images of handwritten digits, in the version available in our docker environment, the set has a total of 70,000 images from numbers 0 to 9. Look at the below image which has a few examples instances:</p> <p></p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#2-getting-the-data","title":"2 - Getting the data","text":"<p>To start working with our dataset it is necessary to carry out the checkout command of ml-git in order to bring the data from our storage to the user's workspace.</p> <pre><code>from ml_git.api import MLGitAPI\napi = MLGitAPI()\n\n# def checkout(entity, tag, sampling=None, retries=2, force=False, dataset=False, labels=False, version=-1)\napi.checkout('labels', 'labelsmnist', dataset=True)\nmnist_dataset_path = 'datasets/handwritten/digits/mnist/data/'\nmnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/'\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/labels/metadata]\nINFO - Metadata: Performing checkout on the entity's lastest tag (handwritten__digits__labelsmnist__1)\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 184blobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 206chunks/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 885files into workspace/s]\nINFO - Repository: Initializing related datasets download\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 226blobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.24chunks/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 13.2files into workspace/s]\n</code></pre> <p>Some important points to highlight here are that the tag parameter can be the name of the entity, this way the ml-git will get the latest version available for this entity. With the dataset=True signals that ml-git should look for the dataset associated with these labels</p> <p>Once we have the data in the workspace, we can load it into variables</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#training-data","title":"Training data","text":"<pre><code>from mlxtend.data import loadlocal_mnist\nimport numpy as np\nimport pickle\n\nX_train = pickle.load(open(mnist_dataset_path + 'train-images.idx3-ubyte', 'rb' ))\ny_train = pickle.load(open(mnist_labels_path + 'train-labels.idx1-ubyte', 'rb' ))\n\nprint('Training data: ')\nprint('Dimensions: %s x %s' % (X_train.shape[0], X_train.shape[1]))\nprint('Digits: %s' % np.unique(y_train))\nprint('Class distribution: %s' % np.bincount(y_train))\n</code></pre> <pre><code>Training data: \nDimensions: 60000 x 784\nDigits: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n</code></pre> <p>The training data consists of 60,000 entries of 784 pixels, distributed among the possible values \u200b\u200baccording to the output above.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#test-data","title":"Test data","text":"<pre><code>X_test, y_test = loadlocal_mnist(\n    images_path= mnist_dataset_path + 't10k-images.idx3-ubyte', \n    labels_path= mnist_labels_path + 't10k-labels.idx1-ubyte')\n\nprint('Test data: ')\nprint('Dimensions: %s x %s' % (X_test.shape[0], X_test.shape[1]))\nprint('Digits: %s' % np.unique(y_test))\nprint('Class distribution: %s' % np.bincount(y_test))\n</code></pre> <pre><code>Test data: \nDimensions: 10000 x 784\nDigits: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [ 980 1135 1032 1010  982  892  958 1028  974 1009]\n</code></pre> <p>The test data consists of 10,000 entries of 784 pixels, distributed among the possible values according to the output above.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#3-training-and-evaluating","title":"3 - Training and evaluating","text":"<p>Let\u2019s take an example of RandomForest Classifier and train it on the dataset and evaluate it.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Training on the existing dataset\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Evaluating the model\ny_pred = rf_clf.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint('Accuracy score after training on existing dataset', score)\n</code></pre> <pre><code>Accuracy score after training on existing dataset 0.9705\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#4-versioning-our-model","title":"4 - Versioning our model","text":"<p>As we do not have any previously versioned models, it will be necessary to create a new entity. For this we use the following command:</p> <pre><code># def create(entity, entity_name, categories, mutability, **kwargs)\napi.create('models', 'modelmnist', categories=['handwritten', 'digits'], mutability='mutable', bucket_name='mlgit', entity_dir='handwritten/digits')\n</code></pre> <pre><code>INFO - MLGit: Model artifact created.\n</code></pre> <p>Once we have our model trained and evaluated, we will version it with ml-git. For that we need to save it in a file.</p> <pre><code>def save_model(model):\n    filename = 'models/handwritten/digits/modelmnist/data/rf_mnist.sav'\n    pickle.dump(model, open(filename, 'wb'))\n\nsave_model(rf_clf)\n</code></pre> <p>With the file in the workspace we use the following commands to create a version:</p> <pre><code>entity_type = 'models'\nentity_name = 'modelmnist'\n\n# def add(entity_type, entity_name, bumpversion=False, fsck=False, file_path=[])\napi.add(entity_type, entity_name, metric={'accuracy': score})\n\n# def commit(entity, ml_entity_name, commit_message=None, related_dataset=None, related_labels=None)\napi.commit(entity_type, entity_name, related_dataset='mnist', related_labels='labelsmnist')\n\n# def push(entity, entity_name, retries=2, clear_on_fail=False)\napi.push(entity_type, entity_name)\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata]\nINFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 1.56files/s]\n\n\n\u280b Creating hard links in cache\u2819 Creating hard links in cache\n\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 7.13kfiles/s]\n\n\n\u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest\n\nINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__1] to the models.\nINFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__1] to the models.\nINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist]\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 520/520 [00:06&lt;00:00, 82.0files/s]\n\n\n\u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#5-adding-new-data","title":"5 - Adding new data","text":"<p>At some point after training a model it may be the case that new data is available.</p> <p>It is interesting that this new data is added to our entity to generate a second version of our dataset.</p> <p>Let's add this data to our entity's directory:</p> <pre><code>! cp train-images.idx3-ubyte datasets/handwritten/digits/mnist/data/.\n! cp train-labels.idx1-ubyte labels/handwritten/digits/labelsmnist/data/.\n</code></pre> <p>Let's take a look at our new dataset</p> <pre><code># loading the dataset\nX_train = pickle.load(open(mnist_dataset_path + 'train-images.idx3-ubyte', 'rb' ))\ny_train = pickle.load(open(mnist_labels_path + 'train-labels.idx1-ubyte', 'rb' ))\n\nprint('Test data: ')\nprint('Dimensions: %s x %s' % (X_train.shape[0], X_train.shape[1]))\nprint('Digits: %s' % np.unique(y_train))\nprint('Class distribution: %s' % np.bincount(y_train))\n</code></pre> <pre><code>Test data: \nDimensions: 180000 x 784\nDigits: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [17769 20226 17874 18393 17526 16263 17754 18795 17553 17847]\n</code></pre> <p>The train data now consists of 180,000 entries of 784 pixels, distributed among the possible values according to the output above.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#6-versioning-the-dataset-and-labels-with-the-new-entries","title":"6 - Versioning the dataset and labels with the new entries","text":"<pre><code>dataset_file = 'datasets/handwritten/digits/mnist/data/train-images.idx3-ubyte'\npickle.dump(X_train, open(dataset_file, 'wb'))\n\nlabels_file = 'labels/handwritten/digits/labelsmnist/data/train-labels.idx1-ubyte'\npickle.dump(y_train, open(labels_file, 'wb'))\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#versioning-the-dataset","title":"Versioning the dataset","text":"<pre><code>entity_type = 'datasets'\nentity_name = 'mnist'\n\napi.add(entity_type, entity_name, bumpversion=True)\napi.commit(entity_type, entity_name)\napi.push(entity_type, entity_name)\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/datasets/metadata]\nINFO - Repository: datasets adding path [/api_scripts/mnist_notebook/datasets/handwritten/digits/mnist] to ml-git index\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:05&lt;00:00, 2.63s/files]\n\n'zdj7WmvrKwjWMCQMFFgcMFmGxEgQS2uXatYWxcE3ByNJeDmze'\n\n\n\n\n\n\u280b Creating hard links in cache\u2819 Creating hard links in cache\n\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.24kfiles/s]\n\n\n\u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest\n\nINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/datasets/metadata] --- file[handwritten/digits/mnist]\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 540/540 [00:17&lt;00:00, 31.0files/s]\n\n\n\u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#versioning-the-labels","title":"Versioning the labels","text":"<pre><code>entity_type = 'labels'\nentity_name = 'labelsmnist'\n\napi.add(entity_type, entity_name, bumpversion=True)\napi.commit(entity_type, entity_name, related_dataset='mnist')\napi.push(entity_type, entity_name)\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/labels/metadata]\nINFO - Repository: labels adding path [/api_scripts/mnist_notebook/labels/handwritten/digits/labelsmnist] to ml-git index\nfiles: 0.00files [00:00, ?files/s]\n\n'zdj7WaeJerH7ACFKfFgh9itrCoWt3iBCYagzYS2M7VhidBAmR'\n\n\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 227files/s]\n\n\n\u280b Creating hard links in cache\u2819 Creating hard links in cache\n\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 8.93kfiles/s]\n\n\n\u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest\n\nINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the labels.\nINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/labels/metadata] --- file[handwritten/digits/labelsmnist]\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 71.8files/s]\n\n\n\u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#7-training-and-evaluating","title":"7 - Training and evaluating","text":"<pre><code># Training on new data\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Evaluating the model\ny_pred = rf_clf.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint(\"Accuracy score after training on augmented dataset\", score)\n</code></pre> <pre><code>Accuracy score after training on augmented dataset 0.9746\n</code></pre> <p>We\u2019ve improved the accuracy by ~0.4%. This is great.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#8-versioning-our-model","title":"8 - Versioning our model","text":"<pre><code>save_model(rf_clf)\n\nentity_type = 'models'\nentity_name = 'modelmnist'\n\napi.add(entity_type, entity_name, bumpversion=True, metric={'accuracy': score})\napi.commit(entity_type, entity_name, related_dataset='mnist', related_labels='labelsmnist')\napi.push(entity_type, entity_name)\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata]\nINFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:04&lt;00:00, 4.46s/files]\n\n'zdj7WfjwEm3XVzoggyG1kSPQhYMSMpbBdLzYMx5Z61StSbu5H'\n\n\n\n\n\n\u280b Creating hard links in cache\u2819 Creating hard links in cache\n\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 6.36kfiles/s]\n\n\n\u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest\n\nINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the models.\nINFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__2] to the models.\nINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist]\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.39k/1.39k [00:22&lt;00:00, 62.1files/s]\n\n\n\u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository\u2839 Pushing metadata to the git repository\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#9-reproducing-our-experiment-with-ml-git","title":"9 - Reproducing our experiment with ml-git","text":"<p>Once the experiment data is versioned, it is common that it is necessary to re-evaluate the result, or that someone else wants to see the result of an already trained model.</p> <p>For this, we will perform the model checkout in version 1 (without the data augmentation), to get the test data and the trained model.</p> <pre><code>mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/'\nmnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/'\nmnist_model_path = 'models/handwritten/digits/modelmnist/data/'\n\napi.checkout('models', 'handwritten__digits__modelmnist__1', dataset=True, labels=True)\n\n# Getting test data\nX_test, y_test = loadlocal_mnist(images_path= mnist_dataset_path + 't10k-images.idx3-ubyte', \n                                 labels_path= mnist_labels_path + 't10k-labels.idx1-ubyte')\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata]\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 95.1blobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 3.44chunks/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:14&lt;00:00, 14.1s/files into workspace]\nINFO - Repository: Initializing related datasets download\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 228blobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 20.6chunks/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:04&lt;00:00, 2.15s/files into workspace]\nINFO - Repository: Initializing related labels download\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 810blobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 234chunks/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 761files into workspace/s]\n</code></pre> <p>With the test data in hand, let's upload the model and evaluate it for our dataset.</p> <pre><code>loaded_model = pickle.load(open(mnist_model_path + 'rf_mnist.sav', 'rb'))\ny_pred = loaded_model.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint('Accuracy score for version 1: ', score)\n</code></pre> <pre><code>Accuracy score for version 1:  0.9705\n</code></pre> <p>Now let's take the model from the version 2 (model trained with more data) and evaluate it for the test set.</p> <pre><code>api.checkout('models', 'handwritten__digits__modelmnist__2')\nloaded_model = pickle.load(open(mnist_model_path + 'rf_mnist.sav', 'rb'))\ny_pred = loaded_model.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint('Accuracy score for version 2: ', score)\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata]\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 1.96kblobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 18.4chunks/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:05&lt;00:00, 5.36s/files into workspace]\n\n\nAccuracy score for version 2:  0.9746\n</code></pre> <p>In a quick and practical way it was possible to obtain the models generated in the experiments and to evaluate them again.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_api/#conclusions","title":"Conclusions","text":"<p>At the end of this execution we have two versions of each entity. If someone else wants to replicate this experiment, they can check out the model with the related dataset and labels.</p> <pre><code>info_data = api.get_models_metrics('modelmnist', export_type='csv')\n\nimport pandas as pd\ninfo_table = pd.read_csv(info_data)\n\n# Displays whole table\ninfo_table\n</code></pre> Date Tag Related dataset - (version) Related labels - (version) accuracy 0 2021-03-25 14:52:42 handwritten__digits__modelmnist__1 mnist - (1) labelsmnist - (1) 0.9705 1 2021-03-25 14:55:52 handwritten__digits__modelmnist__2 mnist - (2) labelsmnist - (2) 0.9746"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/","title":"ML-Git","text":"<p>This notebook describes a basic execution flow with ml-git. In it, we show how to obtain a dataset already versioned by ml-git, how to perform the versioning process of a model and the new data generated, using the MNIST dataset.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#notebook-state-management","title":"Notebook state management","text":"<p>If you have already run this notebook or another notebook in this same folder, it is recommended that you execute the cell below to clean the environment, as state changes made previously may interfere with the notebook running. Be aware that this procedure does not affect any remote repositories.</p> <pre><code>!sh ./clean_environment.sh\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#1-the-dataset","title":"1 - The dataset","text":"<p>Dataset MNIST is a set of small images of handwritten digits, in the version available in our docker environment, the set has a total of 70,000 images from numbers 0 to 9. Look at the below image which has a few examples instances:</p> <p></p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#2-getting-the-data","title":"2 - Getting the data","text":"<p>To start working with our dataset it is necessary to carry out the checkout command of ml-git in order to bring the data from our storage to the user's workspace.</p> <pre><code>! ml-git labels checkout labelsmnist -d\nmnist_dataset_path = 'datasets/handwritten/digits/mnist/data/'\nmnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/'\n</code></pre> <pre><code>INFO - Metadata: Performing checkout on the entity's lastest tag (handwritten__digits__labelsmnist__1)\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 198blobs/s]\u001b[0m\u001b[0m\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 207chunks/s]\u001b[0m\u001b[0m\nfiles into workspace: 100%|\u2588| 2.00/2.00 [00:00&lt;00:00, 919files into workspace/s]\u001b[0m\u001b[0m\n\u001b[0mINFO - Repository: Initializing related datasets download\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 216blobs/s]\u001b[0m\u001b[0m\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.31chunks/s]\u001b[0m\u001b[0m\nfiles into workspace: 100%|\u2588| 2.00/2.00 [00:00&lt;00:00, 12.9files into workspace/s]\u001b[0m[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m\n</code></pre> <p>Some important points to highlight here are that the tag parameter can be the name of the entity, this way the ml-git will get the latest version available for this entity. With the -d signals that ml-git should look for the dataset associated with these labels</p> <p>Once we have the data in the workspace, we can load it into variables</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#training-data","title":"Training data","text":"<pre><code>from mlxtend.data import loadlocal_mnist\nimport numpy as np\nimport pickle\n\nX_train = pickle.load(open(mnist_dataset_path + 'train-images.idx3-ubyte', 'rb' ))\ny_train = pickle.load(open(mnist_labels_path + 'train-labels.idx1-ubyte', 'rb' ))\n\nprint('Training data: ')\nprint('Dimensions: %s x %s' % (X_train.shape[0], X_train.shape[1]))\nprint('Digits: %s' % np.unique(y_train))\nprint('Class distribution: %s' % np.bincount(y_train))\n</code></pre> <pre><code>Training data: \nDimensions: 60000 x 784\nDigits: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n</code></pre> <p>The training data consists of 60,000 entries of 784 pixels, distributed among the possible values \u200b\u200baccording to the output above.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#test-data","title":"Test data","text":"<pre><code>X_test, y_test = loadlocal_mnist(\n    images_path= mnist_dataset_path + 't10k-images.idx3-ubyte', \n    labels_path= mnist_labels_path + 't10k-labels.idx1-ubyte')\n\nprint('Test data: ')\nprint('Dimensions: %s x %s' % (X_test.shape[0], X_test.shape[1]))\nprint('Digits: %s' % np.unique(y_test))\nprint('Class distribution: %s' % np.bincount(y_test))\n</code></pre> <pre><code>Test data: \nDimensions: 10000 x 784\nDigits: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [ 980 1135 1032 1010  982  892  958 1028  974 1009]\n</code></pre> <p>The test data consists of 10,000 entries of 784 pixels, distributed among the possible values according to the output above.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#3-training-and-evaluating","title":"3 - Training and evaluating","text":"<p>Let\u2019s take an example of RandomForest Classifier and train it on the dataset and evaluate it.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Training on the existing dataset\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Evaluating the model\ny_pred = rf_clf.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint('Accuracy score after training on existing dataset', score)\n</code></pre> <pre><code>Accuracy score after training on existing dataset 0.9705\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#4-versioning-our-model","title":"4 - Versioning our model","text":"<p>As we do not have any previously versioned models, it will be necessary to create a new entity. For this we use the following command:</p> <pre><code>! ml-git models create modelmnist --categories=\"handwritten, digits\" --bucket-name=mlgit --mutability=mutable --entity-dir='handwritten/digits'\n</code></pre> <pre><code>INFO - MLGit: Model artifact created.\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m\n</code></pre> <p>Once we have our model trained and evaluated, we will version it with ml-git. For that we need to save it in a file.</p> <pre><code>def save_model(model):\n    filename = 'models/handwritten/digits/modelmnist/data/rf_mnist.sav'\n    pickle.dump(model, open(filename, 'wb'))\n\nsave_model(rf_clf)\n</code></pre> <p>With the file in the workspace we use the following commands to create a version:</p> <pre><code>! ml-git models add modelmnist --metric accuracy $score\n! ml-git models commit modelmnist --dataset=mnist --labels=labelsmnist\n! ml-git models push modelmnist \n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata]\n\u001b[0mINFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 2.14files/s]\u001b[0m\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 6.44kfiles/s]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__1] to the models.\n\u001b[0mINFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__1] to the models.\n\u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist]\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 520/520 [00:06&lt;00:00, 84.8files/s]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#5-adding-new-data","title":"5 - Adding new data","text":"<p>At some point after training a model it may be the case that new data is available.</p> <p>It is interesting that this new data is added to our entity to generate a second version of our dataset.</p> <p>Let's add this data to our entity's directory:</p> <pre><code>! cp train-images.idx3-ubyte datasets/handwritten/digits/mnist/data/.\n! cp train-labels.idx1-ubyte labels/handwritten/digits/labelsmnist/data/.\n</code></pre> <p>Let's take a look at our new dataset</p> <pre><code># loading the dataset\nX_train = pickle.load(open(mnist_dataset_path + 'train-images.idx3-ubyte', 'rb' ))\ny_train = pickle.load(open(mnist_labels_path + 'train-labels.idx1-ubyte', 'rb' ))\n\nprint('Test data: ')\nprint('Dimensions: %s x %s' % (X_train.shape[0], X_train.shape[1]))\nprint('Digits: %s' % np.unique(y_train))\nprint('Class distribution: %s' % np.bincount(y_train))\n</code></pre> <pre><code>Test data: \nDimensions: 180000 x 784\nDigits: [0 1 2 3 4 5 6 7 8 9]\nClass distribution: [17769 20226 17874 18393 17526 16263 17754 18795 17553 17847]\n</code></pre> <p>The train data now consists of 180,000 entries of 784 pixels, distributed among the possible values according to the output above.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#6-versioning-the-dataset-and-labels-with-the-new-entries","title":"6 - Versioning the dataset and labels with the new entries","text":"<pre><code>dataset_file = 'datasets/handwritten/digits/mnist/data/train-images.idx3-ubyte'\npickle.dump(X_train, open(dataset_file, 'wb'))\n\nlabels_file = 'labels/handwritten/digits/labelsmnist/data/train-labels.idx1-ubyte'\npickle.dump(y_train, open(labels_file, 'wb'))\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#versioning-the-dataset","title":"Versioning the dataset","text":"<pre><code>! ml-git datasets add mnist --bumpversion\n! ml-git datasets commit mnist \n! ml-git datasets push mnist \n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/datasets/metadata]\n\u001b[0mINFO - Repository: datasets adding path [/api_scripts/mnist_notebook/datasets/handwritten/digits/mnist] to ml-git index\nfiles: 0.00files [00:00, ?files/s]\u001b[0m'zdj7WmvrKwjWMCQMFFgcMFmGxEgQS2uXatYWxcE3ByNJeDmze'\u001b[0m\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 2.10files/s]\u001b[0m\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 13.3kfiles/s]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/datasets/metadata] --- file[handwritten/digits/mnist]\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 540/540 [00:06&lt;00:00, 86.2files/s]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#versioning-the-labels","title":"Versioning the labels","text":"<pre><code>! ml-git labels add labelsmnist --bumpversion\n! ml-git labels commit labelsmnist --dataset=mnist\n! ml-git labels push labelsmnist \n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/labels/metadata]\n\u001b[0mINFO - Repository: labels adding path [/api_scripts/mnist_notebook/labels/handwritten/digits/labelsmnist] to ml-git index\nfiles: 0.00files [00:00, ?files/s]\u001b[0m'zdj7WaeJerH7ACFKfFgh9itrCoWt3iBCYagzYS2M7VhidBAmR'\u001b[0m\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 410files/s]\u001b[0m\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 12.7kfiles/s]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the labels.\n\u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/labels/metadata] --- file[handwritten/digits/labelsmnist]\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 64.7files/s]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#7-training-and-evaluating","title":"7 - Training and evaluating","text":"<pre><code># Training on new data\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Evaluating the model\ny_pred = rf_clf.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint(\"Accuracy score after training on augmented dataset\", score)\n</code></pre> <pre><code>Accuracy score after training on augmented dataset 0.9746\n</code></pre> <p>We\u2019ve improved the accuracy by ~0.4%. This is great.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#8-versioning-our-model","title":"8 - Versioning our model","text":"<pre><code>save_model(rf_clf)\n\n! ml-git models add modelmnist --bumpversion --metric accuracy $score\n! ml-git models commit modelmnist --dataset=mnist --labels=labelsmnist\n! ml-git models push modelmnist \n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/mnist_notebook/.ml-git/models/metadata]\n\u001b[0mINFO - Repository: models adding path [/api_scripts/mnist_notebook/models/handwritten/digits/modelmnist] to ml-git index\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:05&lt;00:00, 5.41s/files]\u001b[0m'zdj7WfjwEm3XVzoggyG1kSPQhYMSMpbBdLzYMx5Z61StSbu5H'\u001b[0m\n\u001b[0m\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 9.71kfiles/s]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0mINFO - Local Repository: Associate datasets [mnist]-[handwritten__digits__mnist__2] to the models.\n\u001b[0mINFO - Local Repository: Associate labels [labelsmnist]-[handwritten__digits__labelsmnist__2] to the models.\n\u001b[0mINFO - Metadata Manager: Commit repo[/api_scripts/mnist_notebook/.ml-git/models/metadata] --- file[handwritten/digits/modelmnist]\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.39k/1.39k [00:36&lt;00:00, 38.0files/s]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mtadata to the git repository\u001b[0m\u001b[0m\n</code></pre>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#9-reproducing-our-experiment-with-ml-git","title":"9 - Reproducing our experiment with ml-git","text":"<p>Once the experiment data is versioned, it is common that it is necessary to re-evaluate the result, or that someone else wants to see the result of an already trained model.</p> <p>For this, we will perform the model checkout in version 1, to get the test data and the trained model.</p> <pre><code>mnist_dataset_path = 'datasets/handwritten/digits/mnist/data/'\nmnist_labels_path = 'labels/handwritten/digits/labelsmnist/data/'\nmnist_model_path = 'models/handwritten/digits/modelmnist/data/'\n\n! ml-git models checkout modelmnist --version=1 -d -l\n\n# Getting test data\nX_test, y_test = loadlocal_mnist(images_path= mnist_dataset_path + 't10k-images.idx3-ubyte', \n                                 labels_path= mnist_labels_path + 't10k-labels.idx1-ubyte')\n</code></pre> <pre><code>INFO - Metadata: Performing checkout in tag handwritten__digits__modelmnist__1\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 912blobs/s]\u001b[0m\u001b[0m\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 3.59chunks/s]\u001b[0m\u001b[0m\nfiles into workspace: 100%|\u2588| 1.00/1.00 [00:01&lt;00:00, 1.60s/files into workspace]\u001b[0m\u001b[0m\n\u001b[0mINFO - Repository: Initializing related datasets download\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.05kblobs/s]\u001b[0m\u001b[0m\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 17.4chunks/s]\u001b[0m\u001b[0m\nfiles into workspace: 100%|\u2588| 2.00/2.00 [00:00&lt;00:00, 3.17files into workspace/s]\u001b[0m[0m\n\u001b[0mINFO - Repository: Initializing related labels download\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.19kblobs/s]\u001b[0m\u001b[0m\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 625chunks/s]\u001b[0m\u001b[0m\nfiles into workspace: 100%|\u2588| 2.00/2.00 [00:00&lt;00:00, 806files into workspace/s]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m\n</code></pre> <p>With the test data in hand, let's upload the model and evaluate it for our dataset.</p> <pre><code>loaded_model = pickle.load(open(mnist_model_path + 'rf_mnist.sav', 'rb'))\ny_pred = loaded_model.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint('Accuracy score for version 1: ', score)\n</code></pre> <pre><code>Accuracy score for version 1:  0.9705\n</code></pre> <p>Now let's take the model from the version 2 (model trained with more data) and evaluate it for the test set.</p> <pre><code>! ml-git models checkout modelmnist --version=2\nloaded_model = pickle.load(open(mnist_model_path + 'rf_mnist.sav', 'rb'))\ny_pred = loaded_model.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\nprint('Accuracy score for version 2: ', score)\n</code></pre> <pre><code>INFO - Metadata: Performing checkout in tag handwritten__digits__modelmnist__2\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 1.70kblobs/s]\u001b[0m\u001b[0m\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00/1.00 [00:00&lt;00:00, 11.1chunks/s]\u001b[0m\u001b[0m\nfiles into workspace: 100%|\u2588| 1.00/1.00 [00:04&lt;00:00, 4.32s/files into workspace]\u001b[0m\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0mAccuracy score for version 2:  0.9746\n</code></pre> <p>In a quick and practical way it was possible to obtain the models generated in the experiments and to evaluate them again.</p>"},{"location":"api/api_scripts/mnist_notebook/mnist_random_forest_cli/#conclusions","title":"Conclusions","text":"<p>At the end of this execution we have two versions of each entity. If someone else wants to replicate this experiment, they can check out the model with the related dataset and labels.</p> <pre><code>! ml-git models metrics modelmnist\n</code></pre> <pre><code>Tag: handwritten__digits__modelmnist__1\n+-----------------------------+---------------------+\n|             Name            |        Value        |\n+-----------------------------+---------------------+\n|             Date            | 2021-03-25 13:54:04 |\n| Related dataset - (version) |     mnist - (1)     |\n|  Related labels - (version) |  labelsmnist - (1)  |\n|           accuracy          |        0.9705       |\n+-----------------------------+---------------------+\n\u001b[0m\n\u001b[0mTag: handwritten__digits__modelmnist__2\n+-----------------------------+---------------------+\n|             Name            |        Value        |\n+-----------------------------+---------------------+\n|             Date            | 2021-03-25 13:56:48 |\n| Related dataset - (version) |     mnist - (2)     |\n|  Related labels - (version) |  labelsmnist - (2)  |\n|           accuracy          |        0.9746       |\n+-----------------------------+---------------------+\n\u001b[0m\n\u001b[0m\u001b[K\u001b[0m\u001b[?25h\u001b[0m\u001b[0m\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/","title":"Checkout With Sample","text":""},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#checkout-with-sample","title":"Checkout with sample","text":"<p>This notebook describes how to perform a checkout operation with ML-Git using samples of a dataset.</p> <p>The checkout command has three types of sampling options available only for dataset: <code>--sample-type=group --seed</code>,<code>--sample-type=random --seed</code>,<code>--sample-type=range</code> . We use random.sample(population, k) to return a sample of the size k from the population elements. We use random.seed() to set the seed so that the sample generated by <code>random.sample()</code> can be reproduced between experiments. We use the range() object to take samples from a given range.</p>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#example","title":"Example:","text":"<p>Let's assume that we have a dataset that contains 12 files.</p> <p><code>ml-git datasets checkout computer-vision__images__dataset-ex__22 --sample-type=group --sampling=2:5 --seed=1</code> : This command selects 2 files randomly from every group of five files to download.</p> <p></p> <p><code>ml-git datasets checkout computer-vision__images__dataset-ex__22 --sample-type=random --sampling=2:6 --seed=1</code> : This command makes a sample = (amount * len (dataset))% frequency ratio, sample = 4, so four files are selected randomly to download.  </p> <p></p> <p><code>ml-git datasets checkout computer-vision__images__dataset-ex__22 --sample-type=range --sampling=2:11:2</code> : This command selects the files at indexes generated by <code>range(start=2, stop=11, step=2)</code>.</p> <p></p>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#notebook-state-management","title":"Notebook state management","text":"<p>Before execute any cell from this notebook, make sure that you have executed all cells from notebook multiple_datasets in same folder.</p>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#to-start-using-the-ml-git-api-we-need-to-import-it-into-our-script-and-make-sure-that-we-are-in-correct-folder","title":"To start using the ml-git api we need to import it into our script, and make sure that we are in correct folder","text":"<pre><code>from ml_git.api import MLGitAPI\n</code></pre> <pre><code>%cd /api_scripts/multiple_datasets_notebook\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#after-that-we-define-some-variables-that-will-be-used-by-the-script","title":"After that, we define some variables that will be used by the script","text":"<pre><code># The type of entity we are working on\nentity = 'datasets'\n\n# Existing tag in our repository\ntag = 'peopleFaces'\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#before-using-the-sample-option-we-will-checkout-the-entity-to-check-the-files-contained-in-the-tag","title":"Before using the sample option, we will checkout the entity to check the files contained in the tag","text":"<p>The datapath returned by the function tells us where the entity's data was downloaded. That way we can use the following method to print the files that are in the entity's directory</p> <pre><code>import os\nimport glob\nfrom IPython.display import Image\n\ndata_type = '*.jpg'\n\ndef print_files():\n    data_path = 'datasets/peopleFaces/data/people_faces'\n    folder = os.path.join(data_path, data_type)\n    print('Downloaded files: ')\n    for imageName in glob.glob(folder):\n        print ('\\t{}'.format(imageName))\n        display(Image(filename=imageName, width = 150, height = 150))\n\nprint_files()\n</code></pre> <pre><code>Downloaded files: \n    datasets/people_faces/data/people_faces/3.jpg\n</code></pre> <p></p> <pre><code>    datasets/people_faces/data/people_faces/4.jpg\n</code></pre> <p></p> <pre><code>    datasets/people_faces/data/people_faces/8.jpg\n</code></pre> <p></p> <pre><code>    datasets/people_faces/data/people_faces/5.jpg\n</code></pre> <p></p> <pre><code>    datasets/people_faces/data/people_faces/1.jpg\n</code></pre> <p></p> <pre><code>    datasets/people_faces/data/people_faces/9.jpg\n</code></pre> <p></p> <pre><code>    datasets/people_faces/data/people_faces/10.jpg\n</code></pre> <p></p> <pre><code>    datasets/people_faces/data/people_faces/7.jpg\n</code></pre> <p></p> <pre><code>    datasets/people_faces/data/people_faces/6.jpg\n</code></pre> <p></p> <pre><code>    datasets/people_faces/data/people_faces/2.jpg\n</code></pre> <p></p>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#to-be-able-to-checkout-the-same-tag-we-use-the-following-method-to-remove-some-files","title":"To be able to checkout the same tag, we use the following method to remove some files.","text":"<pre><code>import shutil\nimport stat\n\n# function created to clear directory\ndef clear_path(path):\n    if not os.path.exists(path):\n        return\n    # SET the permission for files inside the .git directory to clean up\n    for root, dirs, files in os.walk(path):\n        for f in files:\n            os.chmod(os.path.join(root, f), stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n    try:\n        shutil.rmtree(path)\n    except Exception as e:\n        print('except: ', e)\n\n\ndef clear_environment():\n    clear_path(os.path.join('.ml-git', entity, 'index'))\n    clear_path(os.path.join('.ml-git', entity, 'refs'))\n    clear_path(os.path.join(entity))\n\nclear_environment()\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#checkout-with-group-sample","title":"Checkout with group sample","text":"<pre><code>sampling = {'group': '1:5', 'seed': '10'}\n\napi = MLGitAPI()\ndata_path = api.checkout(entity, tag, sampling)\n\nprint_files()\n\nclear_environment()\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata]\nINFO - Metadata: Performing checkout on the entity's lastest tag (computer-vision__images__people_faces__2)\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.73kblobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.38kchunks/s]\nfiles into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.54kfiles into cache/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.50kfiles into workspace/s]\n\n\nDownloaded files: \n    datasets/people_faces/data/people_faces/1.jpg\n</code></pre> <pre><code>    datasets/people_faces/data/people_faces/6.jpg\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#checkout-with-range-sample","title":"Checkout with range sample","text":"<pre><code>sampling = {'range': '0:4:3'}\n\ndata_path = api.checkout(entity, tag, sampling)\n\nprint_files()\n\nclear_environment()\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata]\nINFO - Metadata: Performing checkout on the entity's lastest tag (computer-vision__images__people_faces__2)\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.28kblobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.37kchunks/s]\nfiles into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.16kfiles into cache/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.13kfiles into workspace/s]\n\n\nDownloaded files: \n    datasets/people_faces/data/people_faces/6.jpg\n</code></pre> <pre><code>    datasets/people_faces/data/people_faces/2.jpg\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/checkout_with_sample/#checkout-with-random-sample","title":"Checkout with random sample","text":"<pre><code>sampling = {'random': '1:5', 'seed': '1'}\n\ndata_path = api.checkout(entity, tag, sampling)\n\nprint_files()\n\nclear_environment()\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata]\nINFO - Metadata: Performing checkout on the entity's lastest tag (computer-vision__images__people_faces__2)\nblobs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.13kblobs/s]\nchunks: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.14kchunks/s]\nfiles into cache: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.81kfiles into cache/s]\nfiles into workspace: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.00/2.00 [00:00&lt;00:00, 1.01kfiles into workspace/s]\n\n\nDownloaded files: \n    datasets/people_faces/data/people_faces/4.jpg\n</code></pre> <pre><code>    datasets/people_faces/data/people_faces/5.jpg\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/","title":"Multiple Datasets","text":""},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#storage-reuse-with-multiple-datasets","title":"Storage reuse with multiple datasets","text":""},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#this-notebook-describes-how-to-handle-the-scenario-where-the-same-file-is-present-in-more-than-one-dataset","title":"This notebook describes how to handle the scenario where the same file is present in more than one dataset.","text":"<p>When the same file is used in multiple datasets, that file will be added to the bucket only once, in order to optimize the space usage in the bucket. To exemplify this use case, two entities will be created: the people entity contains 10 images with faces of people, while famous entity contains 7 images with faces of famous people, being 5 of them also contained in the people entity.</p> <p>This way, when sending the files to the repository, the 5 images that are being used in the two entities will not be duplicated in the bucket. The two entities will refer to the same image stored in the bucket.</p>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#notebook-state-management","title":"Notebook state management","text":"<p>If you have already run this notebook or another notebook in this same folder, it is recommended that you execute the cell below to clean the environment, as state changes made previously may interfere with the notebook running. Be aware that this procedure does not affect any remote repositories.</p> <pre><code>!sh ./../clean_environment.sh\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#to-start-using-the-ml-git-api-we-need-to-import-it-into-our-script","title":"To start using the ml-git api we need to import it into our script","text":"<pre><code>from ml_git.api import MLGitAPI\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#after-that-we-define-some-variables-that-will-be-used-by-the-notebook","title":"After that, we define some variables that will be used by the notebook","text":"<pre><code># The type of entity we are working on\nentity = 'datasets'\n\n# The entity name we are working on\nentity_name_people = 'peopleFaces'\n\n# The entity name we are working on\nentity_name_famous = 'famousFaces'\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#to-start-lets-take-into-account-that-you-have-a-repository-with-git-settings-to-make-the-clone-if-this-is-not-your-scenario-you-will-need-to-configure-ml-git-outside-this-notebook-at-the-moment-the-api-does-not-have-the-necessary-methods-to-perform-this-configuration","title":"To start, let's take into account that you have a repository with git settings to make the clone. If this is not your scenario, you will need to configure ml-git outside this notebook (At the moment the api does not have the necessary methods to perform this configuration).","text":""},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#or-you-can-manually-configure-the-repository-using-the-command-line-following-the-steps-in-the-first-project-documentation","title":"Or you can manually configure the repository using the command line, following the steps in the First Project documentation.","text":"<pre><code>repository_url = '/local_ml_git_config_server.git'\napi = MLGitAPI()\n\napi.clone(repository_url)\n</code></pre> <pre><code>INFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/multiple_datasets_notebook/tmpfyoxmtjy/mlgit/.ml-git/datasets/metadata]\nINFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/multiple_datasets_notebook/tmpfyoxmtjy/mlgit/.ml-git/models/metadata]\nINFO - Metadata Manager: Metadata init [/local_server.git/] @ [/api_scripts/multiple_datasets_notebook/tmpfyoxmtjy/mlgit/.ml-git/labels/metadata]\nINFO - Metadata: Successfully loaded configuration files!\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#create-the-people-dataset","title":"Create the people dataset","text":"<pre><code>!ml-git datasets create peopleFaces --categories=\"computer-vision, images\" --bucket-name=faces_bucket --mutability=strict --import='people_faces' --unzip\n</code></pre> <pre><code>\u001b[?25h\u280b Importing files\u2819 Importing filesINFO - MLGit: Unzipping files\nINFO - MLGit: Dataset artifact created.\n\u001b[?25h\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#we-can-now-proceed-with-the-necessary-steps-to-send-the-new-data-to-storage","title":"We can now proceed with the necessary steps to send the new data to storage.","text":"<pre><code>api.add(entity, entity_name_people, bumpversion=True)\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata]\nINFO - Repository: datasets adding path [/api_scripts/multiple_datasets_notebook/datasets/people_faces] to ml-git index\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.0/10.0 [00:00&lt;00:00, 646files/s]\n\n\n\u280b Creating hard links in cache\u2819 Creating hard links in cache\n\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.0/10.0 [00:00&lt;00:00, 32.7kfiles/s]\n</code></pre> <p>Commit the changes</p> <pre><code># Custom commit message\nmessage = 'Commit example'\n\napi.commit(entity, entity_name_people, message)\n</code></pre> <pre><code>\u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest\n\nINFO - Metadata Manager: Commit repo[/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] --- file[people_faces]\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#as-we-are-using-minio-locally-to-store-the-data-in-the-bucket-we-were-able-to-check-the-number-of-files-that-are-in-the-local-bucket","title":"As we are using MinIO locally to store the data in the bucket, we were able to check the number of files that are in the local bucket.","text":"<pre><code>import os\n\ndef get_bucket_files_count():\n  print(\"Number of files on bucket: \" +  str(len(os.listdir('../../data/faces_bucket'))))\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#amount-of-files-in-the-buket-before-pushing-the-people-dataset","title":"Amount of files in the buket before pushing the people dataset","text":"<pre><code>get_bucket_files_count()\n</code></pre> <pre><code>Number of files on bucket: 0\n</code></pre> <p>As we have not yet uploaded any version of our dataset, the bucket is empty.</p>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#pushing-the-people-dataset","title":"Pushing the people dataset","text":"<pre><code>api.push(entity, entity_name_people)\n</code></pre> <pre><code>files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.0/20.0 [00:00&lt;00:00, 92.3files/s]\n\n\n\u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#amount-of-files-in-the-buket-after-pushing-the-people-dataset","title":"Amount of files in the buket after pushing the people dataset","text":"<pre><code>get_bucket_files_count()\n</code></pre> <pre><code>Number of files on bucket: 20\n</code></pre> <p>After sending the data, we can observe the presence of 20 blobs related to the 10 images that were versioned. In this case, two blobs were added for each image in our dataset.</p>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#create-the-famous-dataset","title":"Create the famous dataset","text":"<p>Let's create our second dataset that has some images equals to the first dataset.</p> <p></p> <pre><code>!ml-git datasets create famousFaces --categories=\"computer-vision, images\" --bucket-name=faces_bucket --mutability=strict --import='famous_faces' --unzip\n</code></pre> <pre><code>\u001b[?25h\u280b Importing files\u2819 Importing filesINFO - MLGit: Unzipping files\nINFO - MLGit: Dataset artifact created.\n\u001b[?25h\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#we-can-now-proceed-with-the-necessary-steps-to-send-the-new-data-to-storage_1","title":"We can now proceed with the necessary steps to send the new data to storage.","text":"<pre><code>api.add(entity, entity_name_famous, bumpversion=True)\n</code></pre> <pre><code>INFO - Metadata Manager: Pull [/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata]\nINFO - Repository: datasets adding path [/api_scripts/multiple_datasets_notebook/datasets/famous_faces] to ml-git index\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.00/7.00 [00:00&lt;00:00, 703files/s]\n\n\n\u280b Creating hard links in cache\u2819 Creating hard links in cache\n\nfiles: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.00/7.00 [00:00&lt;00:00, 40.2kfiles/s]\n</code></pre> <p>Commit the changes</p> <pre><code># Custom commit message\nmessage = 'Commit example'\n\napi.commit(entity, entity_name_famous, message)\n</code></pre> <pre><code>\u280b Updating index\u2819 Updating index Checking removed files\u2819 Checking removed files Commit manifest\u2819 Commit manifest\n\nINFO - Metadata Manager: Commit repo[/api_scripts/multiple_datasets_notebook/.ml-git/datasets/metadata] --- file[famous_faces]\n</code></pre> <p>And finally, sending the data</p> <pre><code>api.push(entity, entity_name_famous)\n</code></pre> <pre><code>files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.0/14.0 [00:00&lt;00:00, 177files/s]\n\n\n\u280b Pushing metadata to the git repository\u2819 Pushing metadata to the git repository\n</code></pre>"},{"location":"api/api_scripts/multiple_datasets_notebook/multiple_datasets/#amount-of-files-in-the-buket-after-pushing-the-famous-dataset","title":"Amount of files in the buket after pushing the famous dataset","text":"<pre><code>get_bucket_files_count()\n</code></pre> <pre><code>Number of files on bucket: 24\n</code></pre> <p>As you can see, only 4 blobs were added to our bucket. Of the set of 7 images, only 2 images were different from the other dataset, so ml-git can optimize storage by adding blobs related only to these new images.</p>"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/","title":"Relationships API methods","text":"<p>This notebook describes a basic flow in the context of relationships between entities with the API provided by ML-Git.</p> <p>In it, we'll show you how to use the set of commands provided. You can check the documentation for more information: API documentation</p>"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#notebook-prerequisites","title":"Notebook prerequisites","text":"<p>This notebook uses the GitHub API to get data from a repository and performs Ml-Git operations on its contents, so before running this notebook, take the following steps:</p> <ul> <li> <p>Have a GitHub SHH access key so that you can use the repository information retrieval API.</p> </li> <li> <p>Have a GitHub repository that the SHH key has access to.</p> </li> </ul>"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#1-context","title":"1 - Context","text":"<p>In this notebook we consider a scenario of an ML-Git project with the following settings:</p> <ul> <li> <p>A versioned config file in GitHub. Pointing to the entities' metadata repositories.</p> </li> <li> <p>Each entity type having its metadata repository.</p> </li> <li> <p>One mode entity (model-ex), two labels entities (labels-ex and labels-ex2) and one dataset entity (dataset-ex)</p> </li> <li> <p>Entities have relationships defined at versioning time.</p> </li> </ul> <p>This settings mentioned above can be better visualized in the diagram below:</p> <p></p>"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#2-configuring","title":"2 - Configuring","text":"<p>To use the methods, you will need to import the API and define some constants related to the user's credential:</p> <p>Below are the constants described in the pre-requirements section, where:['removed'] should be replaced by the SHH access key and api_url can be modified if necessary as reported in the GitHub API documentation.</p> <pre><code>from ml_git.api import MLGitAPI\nfrom ml_git import api\n\ngithub_token = ['removed']\napi_url = 'https://api.github.com'\n</code></pre> <p>After defining the variables to configure, it will be possible to start a manager that will be responsible for operating on the github API.</p> <pre><code>api = MLGitAPI()\nmanager = api.init_entity_manager(github_token, api_url)\n</code></pre> <p>We will use the manager to execute the commands in the next steps.</p>"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#3-methods","title":"3 - Methods","text":""},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#31-get-entities","title":"3.1 - Get Entities","text":"<p>The get_entities method allows the user to get a list of entities being versioned in a project. For this, the user must inform the path to the configuration file, whether this path is a local directory or the name of a git repository. The path can be modified using the config_repository_name field, in our example case the configuration file is in 'user/mlgit-config-repository.</p> <pre><code>config_repository_name='user/mlgit-config-repository'\n\nproject_entities = manager.get_entities(config_repo_name=config_repository_name)\n\nprint(\"Entities found: {}\".format(len(project_entities)))\nprint(\"Example of output object:\\n{}\".format(project_entities[3]))\n</code></pre> <pre><code>Entities found: 4\nExample of output object:\n{\n  \"name\": \"model-ex\",\n  \"entity_type\": \"model\",\n  \"metadata\": {\n    \"full_name\": \"user/mlgit-models\",\n    \"git_url\": \"git@github.com:user/mlgit-models.git\",\n    \"html_url\": \"https://github.com/user/mlgit-models\",\n    \"owner_email\": \"user@gmail.com\",\n    \"owner_name\": \"User Name\"\n  },\n  \"last_spec_version\": {\n    \"version\": 3,\n    \"tag\": \"test__model-ex__3\",\n    \"mutability\": \"flexible\",\n    \"categories\": [\n      \"test\"\n    ],\n    \"amount\": 3,\n    \"size\": \"27 Bytes\",\n    \"storage\": {\n      \"type\": \"s3h\",\n      \"bucket\": \"mlgit-bucket\"\n    }\n  }\n}\n</code></pre> <p>As expected the API found 4 entities in the repository (dataset-ex, model-ex, labels-ex, labels-ex2).</p>"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#32-get-entity-versions","title":"3.2 - Get Entity Versions","text":"<p>The get_entity_version method allows the user to get a list of spec versions found for an especific entity.</p> <pre><code>selected_entity = project_entities[3]\n\nentity_versions = manager.get_entity_versions(selected_entity.name, selected_entity.metadata.full_name)\nprint(\"Versions found: {}\".format(len(entity_versions)))\nprint(\"Example of output object:\\n{}\".format(entity_versions[len(entity_versions)-1]))\n</code></pre> <pre><code>Versions found: 3\nExample of output object:\n{\n  \"version\": 1,\n  \"tag\": \"test__model-ex__1\",\n  \"mutability\": \"flexible\",\n  \"categories\": [\n    \"test\"\n  ],\n  \"amount\": 1,\n  \"size\": \"9 Bytes\",\n  \"storage\": {\n    \"type\": \"s3h\",\n    \"bucket\": \"mlgit-bucket\"\n  }\n}\n</code></pre> <p>As expected the API found 3 versions for the model-ex entity.</p>"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#33-get-linked-entities","title":"3.3 - Get Linked Entities","text":"<p>The get_linked_entities method allows the user to get a list of linked entities found for an entity in a specific version.</p> <pre><code>entity_version = 1\nlinked_entities_in_version = manager.get_linked_entities(selected_entity.name, entity_version, selected_entity.metadata.full_name)\nprint(\"Output: \\n{}\".format(linked_entities_in_version))\n</code></pre> <pre><code>Output: \n[{\n  \"tag\": \"test__dataset-ex__1\",\n  \"name\": \"dataset-ex\",\n  \"version\": \"1\",\n  \"entity_type\": \"dataset\"\n}, {\n  \"tag\": \"test__labels-ex__1\",\n  \"name\": \"labels-ex\",\n  \"version\": \"1\",\n  \"entity_type\": \"labels\"\n}]\n</code></pre> <p>If we go back to the diagram, we can see that as shown in the output, version 1 of the model-ex entity is related to dataset-ex in version 1 and labels-ex in version 1.</p>"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#34-get-entity-relationships","title":"3.4 - Get Entity Relationships","text":"<p>The get_linked_entities method allows the user to get the list of all relationships that the specific entity has. For this it goes through all versions of the entity and checks the relationships that have been established.</p> <pre><code>entity_relationships = manager.get_entity_relationships(selected_entity.name, selected_entity.metadata.full_name)\n\ncount_relationships = 0\nfor version in entity_relationships[selected_entity.name]:\n    count_relationships += len(version.relationships)\n\nprint(\"Relationships found: {}\".format(count_relationships))\nprint(\"Example of output object:\\n{}\".format(entity_relationships[selected_entity.name][0]))\n</code></pre> <pre><code>Relationships found: 6\nExample of output object:\n{\n  \"version\": 3,\n  \"tag\": \"test__model-ex__3\",\n  \"relationships\": [\n    {\n      \"tag\": \"test__dataset-ex__3\",\n      \"name\": \"dataset-ex\",\n      \"version\": \"3\",\n      \"entity_type\": \"dataset\"\n    },\n    {\n      \"tag\": \"test__labels-ex2__2\",\n      \"name\": \"labels-ex2\",\n      \"version\": \"2\",\n      \"entity_type\": \"labels\"\n    }\n  ]\n}\n</code></pre> <p>In addition, this command allows the user to define the output format, which can be json (as in the previous example) or CSV. If he wants, he can also define the export_path to export the data to a file.</p> <p>An example of how to use the generated csv can be seen below:</p> <pre><code>import pandas as pd\n\nentity_relationships_csv = manager.get_entity_relationships(selected_entity.name, selected_entity.metadata.full_name, export_type='csv')\n\ndf = pd.read_csv(entity_relationships_csv)\ndf\n</code></pre> from_tag from_name from_version from_type to_tag to_name to_version to_type 0 test__model-ex__3 model-ex 3 model test__dataset-ex__3 dataset-ex 3 dataset 1 test__model-ex__3 model-ex 3 model test__labels-ex2__2 labels-ex2 2 labels 2 test__model-ex__2 model-ex 2 model test__dataset-ex__1 dataset-ex 1 dataset 3 test__model-ex__2 model-ex 2 model test__labels-ex__2 labels-ex 2 labels 4 test__model-ex__1 model-ex 1 model test__dataset-ex__1 dataset-ex 1 dataset 5 test__model-ex__1 model-ex 1 model test__labels-ex__1 labels-ex 1 labels"},{"location":"api/api_scripts/relationship_notebook/relationship_api_commands/#35-get-project-entities-relationships","title":"3.5 - Get Project Entities Relationships","text":"<p>Like the previous command, the get_project_entities_relationships command aims to present the entity relationships, but with this single command the user can capture the relationships of all entities that are in the project.</p> <p>In our case we have 4 versioned entities, so the command will check the relationships of these 4 entities.</p> <pre><code>project_entities_relationships = manager.get_project_entities_relationships(config_repository_name)\n\ncount_relationships = 0\nfor entity in project_entities_relationships:\n    for version in project_entities_relationships[entity]:\n        count_relationships += len(version.relationships)\n\nprint(\"Relationships found: {}\".format(count_relationships))\nprint(\"Example of output object:\\n{}\".format(project_entities_relationships[entity][0]))\n</code></pre> <pre><code>Relationships found: 10\nExample of output object:\n{\n  \"version\": 3,\n  \"tag\": \"test__model-ex__3\",\n  \"relationships\": [\n    {\n      \"tag\": \"test__dataset-ex__3\",\n      \"name\": \"dataset-ex\",\n      \"version\": \"3\",\n      \"entity_type\": \"dataset\"\n    },\n    {\n      \"tag\": \"test__labels-ex2__2\",\n      \"name\": \"labels-ex2\",\n      \"version\": \"2\",\n      \"entity_type\": \"labels\"\n    }\n  ]\n}\n</code></pre> <p>Like the previous one, it is possible to export the result in csv.</p> <pre><code>project_entities_relationships_csv = manager.get_project_entities_relationships(config_repository_name, export_type='csv')\ndf = pd.read_csv(project_entities_relationships_csv)\ndf\n</code></pre> from_tag from_name from_version from_type to_tag to_name to_version to_type 0 test__labels-ex2__2 labels-ex2 2 labels test__dataset-ex__3 dataset-ex 3 dataset 1 test__labels-ex2__1 labels-ex2 1 labels test__dataset-ex__3 dataset-ex 3 dataset 2 test__labels-ex__2 labels-ex 2 labels test__dataset-ex__1 dataset-ex 1 dataset 3 test__labels-ex__1 labels-ex 1 labels test__dataset-ex__1 dataset-ex 1 dataset 4 test__model-ex__3 model-ex 3 model test__dataset-ex__3 dataset-ex 3 dataset 5 test__model-ex__3 model-ex 3 model test__labels-ex2__2 labels-ex2 2 labels 6 test__model-ex__2 model-ex 2 model test__dataset-ex__1 dataset-ex 1 dataset 7 test__model-ex__2 model-ex 2 model test__labels-ex__2 labels-ex 2 labels 8 test__model-ex__1 model-ex 1 model test__dataset-ex__1 dataset-ex 1 dataset 9 test__model-ex__1 model-ex 1 model test__labels-ex__1 labels-ex 1 labels <p>As expected, all the relationships that were highlighted in the diagram were captured by the API.</p>"},{"location":"tabular_data/tabular_data/","title":"Working with tabular data","text":""},{"location":"tabular_data/tabular_data/#what-is-tabular-data","title":"What is tabular data?","text":"<p>For most people working with small amounts of data, the data table is the fundamental unit of organization. The data table, arguably the oldest data structure, is both a way of organizing data for processing by machines and of presenting data visually for consumption by humans.</p> <p>Tabular data is data that is structured into rows, each of which contains information about some thing.  Each row contains the same number of cells (although some of these cells may be empty), which provide values of properties of the thing described by the row.  In tabular data, cells within the same column provide values for the same property of the things described by each row. This is what differentiates tabular data from other line-oriented formats.</p> <p>An example of a tabular data structure can be seen below:</p> <p></p>"},{"location":"tabular_data/tabular_data/#versioning","title":"Versioning","text":"<p>Due to the way the data is versioned by ML-Git (see internals documentation) the data organization structure can influence the performance and optimization of the data storage that ML-Git has.</p> <p>When ML-Git is dealing with tabular data, in order to obtain higher storage usage efficiency, it is recommended to avoid actions that edit data that were previously added.\u200b</p> <p>We strongly recommend that the user organize their data in such a way that the entry of new data into the set is done without changing the data already added. Examples of this type of organization is to partition the data by insertion date. This way, each partition should not be modified by future data insertions.\u200b</p> <p>One good way how we can achieve partitioning is using the folders structure to split data in different physical sets, even with several levels,  with a part of the information of the table. As we can see in the picture, the name of each folder should contain the concrete value of the column  and optionally also the name of the column.</p> <p></p> <p>Some criteria must be met when choosing the key partition columns:</p> <ul> <li>Be used very frequently with the same conditions.<ul> <li>Time-based data: combination of year, month, and day associated with time values.</li> <li>Location-based data: geographic region data associated with some place.</li> </ul> </li> <li>Have a reasonable number of different values (cardinality).<ul> <li>The number of possible values has to be reasonable to gain efficiency splitting the data. For example a valid range could be between 10 and 1000.</li> </ul> </li> </ul>"},{"location":"tabular_data/tabular_data/#adding-or-modifying-the-data","title":"Adding or modifying the data","text":"<p>Once your data is versioned as suggested in the previous section, you may at some point wish to add new data to this dataset.  Whenever this type of operation is to be performed, try to take into consideration editing the smallest number of files that have already been versioned. The increment of new data must be given by the creation of new files.</p> <p>One way to make these changes without modifying the data is to use the append save mode if you are working with parquet data. Using append save mode, you can append a dataframe to an existing parquet file. See more in this link.</p> <p>Note:  In exploratory tests it was observed that the use of parquet data with the append writing mode is the most efficient in terms of performance and optimization for ML-Git, since this writing mode avoids the modification of previous files.</p> <p>If you are working with another type of data, such as CSV, whenever new data is added to your dataset you must create a new file for that data.</p> <p>Note: CSV format files are generally not recommended for large volumes of data. It is recommended to use a more efficient data structure, such as parquet. </p>"}]}